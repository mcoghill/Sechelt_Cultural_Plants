---
title: "01b_BCDataLayer_Prep"
author: "Matt Coghill"
date: "02/01/2020"
output: html_document
---

The purpose of this script is to download the relevant spatial data for BEC zones, VRI, TEM, waterbodies and the road network. Data is downloaded directly from the [BC Data Catalogue](https://catalogue.data.gov.bc.ca/dataset?download_audience=Public) using the [bcdata](https://github.com/bcgov/bcdata) package.

```{r Load Packages, echo=TRUE, results='hide'}

suppressMessages(suppressWarnings({
  lapply(c("bcdata", "bcmaps", "tidyverse", "terra", "parallel"), 
         library, character.only = TRUE)[0]}))

```

### Datasets to download 

* BEC - [Biogeoclimatic Ecosystems Classification](https://catalogue.data.gov.bc.ca/dataset/bec-map) are used to define and/or select specific "subzones" within a defined study area. 

* TEM - [Terrestrial Ecosystem Mapping](https://catalogue.data.gov.bc.ca/dataset/terrestrial-ecosystem-mapping-tem-detailed-polygons-with-short-attribute-table-spatial-view) data is used later on to rasterize the study area based on deciles.

* VRI [Vegetation Resource Inventory](https://catalogue.data.gov.bc.ca/dataset/vri-2019-forest-vegetation-composite-rank-1-layer-r1-) - This layer includes a variety of vegetation measure, including cutblock age, TEM data, etc. Detailed data standards can be found [here](https://www2.gov.bc.ca/gov/content/industry/forestry/managing-our-forest-resources/forest-inventory/data-management-and-access/vri-data-standards).

* [Cutblocks](https://catalogue.data.gov.bc.ca/dataset/harvested-areas-of-bc-consolidated-cutblocks-) - This augments the VRI layer to highlight cutblock polygons in the area which may be useful for determining where/where not to sample

* Freshwater Atlas -  The atlas is separated into the different types of waterbodies ([lakes](https://catalogue.data.gov.bc.ca/dataset/freshwater-atlas-lakes), [rivers](https://catalogue.data.gov.bc.ca/dataset/freshwater-atlas-rivers), [wetlands](https://catalogue.data.gov.bc.ca/dataset/freshwater-atlas-wetlands), streams, man made, etc.), which requires a separate download per type. Lakes, rivers, and wetlands have polygon attributes which are downloaded here and merged together at the end.

* Road network - In previous works, the [raw road network](https://catalogue.data.gov.bc.ca/dataset/digital-road-atlas-dra-master-partially-attributed-roads) was found to be too detailed. The raw road network is filtered to only keep named roads, and then the up to date [Forest Service Road (FSR) layer](https://catalogue.data.gov.bc.ca/dataset/forest-tenure-road-segment-lines) is downloaded and merged with the filtered road network to produce a really good representation of where roads are actually located on the landscape.

* Fire - Not currently used for anything but may provide some information as to where we should not sample since data may be skewed due to fire. Combines both [current](https://catalogue.data.gov.bc.ca/dataset/fire-perimeters-current) and [historical](https://catalogue.data.gov.bc.ca/dataset/fire-perimeters-historical) polygons.

## Work-flow

This script relies on having an sf or sfc object, or list of either of those as inputs (ie: polygons representing the areas of interest). This is created from a DEM file. Afterwards, the function processes the requested downloads of each layer listed above and clips them to the AOI shape.

```{r Parameters}

AOI <- "Sechelt"
AOI_dir <- file.path(".", paste0(AOI, "_AOI"))
shp_dir <- file.path(AOI_dir, "0_raw_inputs", "base_layers")
cov_dir <- file.path(AOI_dir, "1_map_inputs", "covariates")

```

As of January 2021, the terra package doesn't play well with the sf package. The first small function is a quick helper from the terra package author to convert SpatVect objects to sf objects. The second one creates bounding boxes for each row of polygonal data in an sf dataframe.

```{r functions}

st_as_sf.SpatVector <- function(x, ...) {
	sf::st_as_sf(as.data.frame(x, geom = TRUE), wkt = "geometry", crs = crs(x))}

st_bbox_by_feature = function(x) {
  x <- sf::st_geometry(x)
  f <- function(y) sf::st_as_sfc(sf::st_bbox(y))
  do.call("c", lapply(x, f)) %>% sf::st_set_crs(sf::st_crs(x))
}

```

In order to get accurate polygon edges for each raster resolution, they need to be individually created and saved. The polygonization of a raster will take place using the terra package. It first replaces values in a raster with 0's, and then polygonizes the "grid classes" (of which there would only be a single class, 0). Care is taken to make sure that the geometries are valid and that the resulting object is a POLYGON class instead of a MULTIPOLYGON.

```{r Raster to polygon}

# Load DEM files
dem_files <- as.list(list.files(cov_dir, pattern = "dem.tif$", 
                        full.names = TRUE, recursive = TRUE)) %>% 
  magrittr::set_names(sapply(., function(x) basename(dirname(x))))

aoi_shapes <- lapply(dem_files, function(i) {
  cat("\rCreating the", basename(dirname(i)), "AOI polygon\n")
  # Create directory for saving DEM shapes
  shp_subdir <- file.path(shp_dir, basename(dirname(i)))
  dir.create(shp_subdir, recursive = TRUE, showWarnings = FALSE)
  
  # Load DEM and change all values to be the same
  dem <- rast(i) * 0
  
  # Convert DEM raster to polygon, remove holes but keep the largest hole
  dem_shp_all <- terra::as.polygons(dem, trunc = TRUE, dissolve = TRUE, values = TRUE) 
  
  # Small disjointed polygons have 5 points. Filter those, create an sf polygon,
  # buffer them slightly, and extract each of the bounding boxes
  dem_small <- terra::geom(dem_shp_all, df = TRUE) %>% 
    group_by(geom, part, hole) %>% 
    dplyr::filter(n() <= 5, hole < 1) %>% 
    {if(nrow(.) > 0) {
      sf::st_as_sf(., coords = c("x", "y"), crs = crs(dem)) %>% 
        summarise(do_union = TRUE, .groups = "drop") %>% 
        sf::st_cast("POLYGON") %>% 
        {if(!sf::st_is_valid(.)) {
          sf::st_make_valid(.)
        } else .} %>% 
        sf::st_buffer(res(dem)[1] / 10, nQuadSegs = 0) %>% 
        dplyr::select(geometry) %>% 
        dplyr::mutate(dem = 0, geometry = st_bbox_by_feature(.))
    } else NULL}
  
  # Union disjointed polygons to the main polygon - this allows for a singular
  # POLYGON object to be created, rather than a MULTIPOLYGON
  dem_shp <- sf::st_as_sf(dem_shp_all) %>% 
    rbind(dem_small) %>% 
    summarise(do_union = TRUE) %>% 
    {if(!sf::st_is_valid(.)) {
      sf::st_make_valid(.)
    } else .} %>% 
    {if(nrow(.) > 0) {
      dplyr::mutate(., poly_id = basename(dirname(i)))
    } else NULL}
  
  return(dem_shp)
}) %>% do.call(rbind, .) %>% tibble::remove_rownames()

```

A single function is used to download all layers listed above. If multiple AOI's are provided, the layer is downloaded once and then clipped to the respective AOI. The function is also able to perform the following checks:

* Checks that an sf or sfc object is loaded
* Checks that the objects are POLYGON or MULTIPOLYGON class
* Checks the CRS arguments and performs datum transformations where necessary

Pretty much as long as there is an sf or sfc object within the province of BC, the function should operate properly. Newly added is parallelism to download and perform spatial intersections for each downloaded layer on separate cores, hopefully increasing the efficiency (not explicitly tested yet).

```{r Function}

get_bc_shapes <- function(in_aoi, id_col) {
  
  #### Input checks
  ## Restrict in_aoi to being an sf or sfc object only. The function will convert
  ## sfc objects to sf objects where appropriate
  
  # First, check for object presence
  if(missing(in_aoi)) stop("'in_aoi' is missing with no default")
  
  # Second, detect object type and convert where necessary
  if(!inherits(in_aoi, c("sf", "sfc")))
    stop("'in_aoi' is not an sf or sfc object.")
  
  # Check properties of the id_col variable, used to uniquely identify each polygon
  if(!missing(id_col) && class(id_col) != "character")
    stop("'id_col' is not a character object")
  
  if(!missing(id_col) && length(id_col) > 1)
    stop("'id_col' needs to have a length of 1")
  
  # If it's an sfc object, convert to sf dataframe
  if(inherits(in_aoi, "sfc")) {
    id_col <- "poly_id"
    in_aoi <- st_as_sf(in_aoi) %>% 
      dplyr::rename(geometry = x) %>% 
      dplyr::mutate("{id_col}" := row_number()) %>% 
      dplyr::relocate(geometry, .after = last_col())
  } else {
    # If it's a sf dataframe, use the supplied id column for ID'ing polygons,
    # or generate a column with a warning
    if(missing(id_col)) {
      id_col <- names(which(apply(
        st_drop_geometry(in_aoi), 2, function(x) 
          length(unique(x))) == nrow(in_aoi)))[1]
      
      # If above evaluates to NULL or NA, use row numbers instead
      if(is.na(id_col) || is.null(id_col)) {
        id_col <- "poly_id"
        in_aoi <- dplyr::mutate(in_aoi, "{id_col}" := row_number())
      }
    }
  }
  
  # Check for polygon/multipolygon geometry type as input only:
  if(!st_geometry_type(in_aoi, by_geometry = FALSE) %in% c("POLYGON", "MULTIPOLYGON"))
    stop("Only POLYGON or MULTIPOLYGON geometries allowed")
  
  # Check for uniqueness of id_col variable against the nrow of the in_aoi
  if(nrow(in_aoi) != length(unique(dplyr::pull(in_aoi, id_col))))
    stop("Something is wrong with the 'id_col' variable. If error persists, don't use.")
  
  # Detect the CRS of the sf object
  if(is.na(sf::st_crs(in_aoi)))
    stop("CRS is not assigned. Use sf::st_crs() to assign a valid CRS to in_aoi")
  
  if(sf::st_is_longlat(in_aoi)) {
    cat("Input CRS is Lat/Long format. Transforming to EPSG 3005 (BC Albers) for processing\n")
    epsg <- 3005L
    in_crs <- sf::st_crs(in_aoi)
    in_aoi <- sf::st_transform(in_aoi, epsg) %>% sf::st_set_agr("constant")
  } else {
    in_crs <- sf::st_crs(in_aoi)
    epsg <- in_crs$epsg
    in_aoi <- sf::st_set_agr(in_aoi, "constant")
    if(!is.numeric(epsg)) 
      stop("There was a problem retrieving the EPSG code from the in_aoi. Is it assigned properly?")
  }
  
  # Detect the location of your polygons. If outside of BC, throw a warning.
  bc_boundary <- bcmaps::bc_bound(class = "sf", ask = FALSE) %>% 
    sf::st_bbox() %>% 
    sf::st_as_sfc() %>% 
    {if(sf::st_crs(.) != sf::st_crs(in_aoi)) {
      sf::st_transform(., sf::st_crs(in_aoi))
    } else .}
  
  if(!all(st_intersects(st_centroid(in_aoi), bc_boundary, sparse = FALSE))) {
    warning("One or some of the input polygons was found outside of the BC provincial boundary. 
            \rNo data will be downloaded. Returning NULL.\n")
    return(NULL)
  } else {
    
    #### Function script
    # Get largest bbox for downloading layers
    box <- sf::st_bbox(in_aoi)
    
    suppressMessages({
      # Download BEC and clip to AOI. If transformation is necessary, it is much
      # faster to transform the bounding box to the CRS of the BEC layer, clip
      # the BEC layer, then transform the resulting BEC layer back to the CRS of 
      # the bounding box
      bec_sf <- bcmaps::bec(class = "sf", force = TRUE, ask = FALSE) %>% 
        sf::st_set_agr("constant")
      if(sf::st_crs(bec_sf) != sf::st_crs(in_aoi)) {
        box_trans <- sf::st_as_sfc(box) %>% 
          sf::st_buffer(10, nQuadSegs = 0) %>% 
          sf::st_transform(sf::st_crs(bec_sf)) %>% 
          sf::st_bbox()
        bec_sf <- sf::st_crop(bec_sf, box_trans) %>% 
          sf::st_transform(sf::st_crs(in_aoi)) %>% 
          sf::st_set_agr("constant")
      } 
      bec_sf_clip <- sf::st_crop(bec_sf, box) %>% 
        sf::st_set_agr("constant")
      
      ## Parallel processing/downloading of BC Data layers
      # Want to use sockets for parallel processing - carries across all machines,
      # whereas forking is only used on Macs and Linux.
      
      # Provide list of links to BC Data Catalog polygonal spatial datasets
      links <- list(
        TEM = "0a83163b-a62f-4ce6-a9a1-21c228b0c0a3", 
        VRI = "2ebb35d8-c82f-4a17-9c96-612ac3532d55", 
        Cutblocks = "b1b647a6-f271-42e0-9cd0-89ec24bce9f7", 
        Lakes = "cb1e3aba-d3fe-4de1-a2d4-b8b6650fb1f6", 
        Rivers = "f7dac054-efbf-402f-ab62-6fc4b32a619e", 
        Wetlands = "93b413d8-1840-4770-9629-641d74bd1cc6", 
        Roads = "bb060417-b6e6-4548-b837-f9060d94743e", 
        FSRs = "9e5bfa62-2339-445e-bf67-81657180c682",
        FireCur = "cdfc2d7b-c046-4bf0-90ac-4897232619e1", 
        FireHist = "22c7cb44-1463-48f7-8e47-88857f207702")
      
      # Create the parallel cluster and export variables to each socket
      env <- new.env()
      cl <- makeCluster(detectCores())
      clusterExport(cl, c("box", "links", "epsg", "in_aoi"), envir = env)
      
      # Load the required libraries on each socket
      clusterEvalQ(cl, suppressMessages(suppressWarnings({
        sapply(c("bcdata", "tidyverse", "sf"), 
               library, character.only = TRUE)[0]})))[0]
      
      # Download all datasets using a parallelized lapply call
      cat("Downloading all layers...")
      dls <- parLapply(cl, links, function(x) {
        bcdata::bcdc_query_geodata(x, crs = epsg) %>%
          bcdata::filter(bcdata::BBOX(box)) %>%
          bcdata::collect() %>% 
          {if(sf::st_crs(.) != sf::st_crs(epsg)) {
            sf::st_transform(., epsg)
          } else .} %>% 
          sf::st_set_agr("constant")
      })
      cat("Done!\n")
      
      # Update BGC boundaries based on TEM lines
      tem <- dls[["TEM"]]
      if(nrow(tem) > 0 && 
         all(unique(bec_sf_clip$MAP_LABEL) %in% 
             unique(tem$BIOGEOCLIMATIC_LBL))) {
        bec_fix <- tem %>% dplyr::select(BIOGEOCLIMATIC_LBL) %>% 
          dplyr::group_by(BIOGEOCLIMATIC_LBL) %>% 
          dplyr::summarise(do_union = TRUE, .groups = "drop")
        if(any(is.na(bec_fix$BIOGEOCLIMATIC_LBL))) {
          bec_sf_clip <- sf::st_set_agr(bec_fix, "constant") %>% 
            sf::st_intersection(
              sf::st_set_agr(bec_sf_clip[, "MAP_LABEL"], "constant")) %>% 
            dplyr::mutate(BIOGEOCLIMATIC_LBL = ifelse(
              is.na(BIOGEOCLIMATIC_LBL), MAP_LABEL, BIOGEOCLIMATIC_LBL)) %>% 
            dplyr::group_by(BIOGEOCLIMATIC_LBL) %>% 
            dplyr::summarise(do_union = TRUE, .groups = "drop") %>% 
            dplyr::rename(MAP_LABEL = BIOGEOCLIMATIC_LBL)
        } else bec_sf_clip <- dplyr::rename(bec_fix, MAP_LABEL = BIOGEOCLIMATIC_LBL)
      }
      bec_sf_clip <- list(BEC = sf::st_cast(bec_sf_clip, "MULTIPOLYGON") %>% 
                            sf::st_set_agr("constant"))
      
      # Filter cutblocks to be less than 20 years old
      cutblocks <- list(
        Cutblocks = dls[["Cutblocks"]] %>% 
          dplyr::mutate(DISTURBANCE_START_DATE = as.Date(DISTURBANCE_START_DATE),
                        DISTURBANCE_END_DATE = as.Date(DISTURBANCE_END_DATE)) %>% 
          dplyr::filter(
            as.numeric(format(Sys.time(), "%Y")) - HARVEST_YEAR <= 20))
      
      # Combine lakes, rivers, and wetlands to a single Waterbodies layer
      waterbodies <- list(
        Waterbodies = do.call(rbind, dls[c("Lakes", "Rivers", "Wetlands")] %>% 
                                Filter(function(x) nrow(x) > 0, .)))
      
      # Combine digital road atlas and FSR layer to a single Roads layer
      roads <- list(
        Roads = do.call(rbind, list(
          dls[["Roads"]] %>% 
            dplyr::filter(ROAD_NAME_ID > 0) %>% 
            dplyr::select(id, ROAD_NAME_FULL, FEATURE_LENGTH_M) %>% 
            dplyr::rename(NAME = ROAD_NAME_FULL),
          dls[["FSRs"]] %>% 
            dplyr::filter(LIFE_CYCLE_STATUS_CODE == "ACTIVE") %>% 
            dplyr::select(id, MAP_LABEL, FEATURE_LENGTH_M) %>% 
            dplyr::rename(NAME = MAP_LABEL)) %>% Filter(function(x) nrow(x) > 0, .)))
      
      # Combine fire area polygons and filter out fires older than 20 years
      fires <- list(
        Fires = do.call(rbind, dls[c("FireCur", "FireHist")] %>% 
                          Filter(function(x) nrow(x) > 0, .)) %>% 
          dplyr::select(id, FIRE_NUMBER, VERSION_NUMBER, FIRE_YEAR, 
                        FIRE_SIZE_HECTARES, LOAD_DATE) %>% 
          dplyr::filter(as.numeric(format(Sys.time(), "%Y")) - FIRE_YEAR <= 20))
      
      # Recombine
      dls_comb <- lapply(c(bec_sf_clip, dls[c("TEM", "VRI")], cutblocks, 
                           waterbodies, roads, fires), sf::st_set_agr, "constant")
      
      # Export the newly combined list to the sockets
      clusterExport(cl, "dls_comb", envir = env)
      
      # Perform the spatial intersection with the study area polygons in parallel
      cat("Performing spatial intersections in parallel...")
      dls_full <- parLapply(cl, dls_comb, sf::st_intersection, in_aoi)
      if(sf::st_is_longlat(in_crs)) {
        clusterExport(cl, c("dls_full", "in_crs"), envir = env)
        dls_full <- parLapply(cl, dls_full, sf::st_transform, in_crs)
      }
      cat("Done!\n")
      
      # Finish parallel processing
      stopCluster(cl)
      
      # Reformat output
      dls_clips <- sapply(dplyr::pull(in_aoi, id_col), function(x) 
        lapply(dls_full, dplyr::filter, !!sym(id_col) == x), simplify = FALSE, USE.NAMES = TRUE) %>% 
        lapply(., function(x) {
          xx <- c(x, list(forested_sites = dplyr::select(x[["VRI"]], BCLCS_LEVEL_2)))
          xx[["Roads"]] <- sf::st_cast(xx[["Roads"]], "MULTILINESTRING")
          y <- lapply(xx[c("BEC", "TEM", "VRI", "Cutblocks", "forested_sites",
                           "Waterbodies", "Fires")],
                      sf::st_cast, "MULTIPOLYGON")
          yy <- c(y, list(Roads = xx[["Roads"]])) %>% 
            lapply(., dplyr::select, -any_of(id_col))
        })
    })
    return(dls_clips)
  }
}

```

After the function runs, the outputs are saved into the respective folders that they should be in.

```{r Run and export}

# Run the function in a loop for multiple input AOI's (function will default
# out_dir designation to the folder of in_aoi if in_aoi is a file path)
bc_shapes <- get_bc_shapes(aoi_shapes, "poly_id")

# Diagnostics for proper feature types and geometry validity, takes a short moment
sapply(bc_shapes, function(x) sapply(x, st_geometry_type, FALSE))
sapply(bc_shapes, function(x) sapply(x, function(y) all(st_is_valid(y))))

# Save files as geopackages
for(i in aoi_shapes$poly_id) {
  st_write(aoi_shapes[aoi_shapes$poly_id == i, ], file.path(shp_dir, i, "aoi.gpkg"),
           delete_layer = TRUE, quiet = TRUE)
}

sapply(names(bc_shapes), function(x) {
  sapply(names(bc_shapes[[x]]), function(y) {
     st_write(bc_shapes[[x]][[y]], file.path(shp_dir, x, paste0(y, ".gpkg")),
              delete_layer = TRUE, quiet = TRUE)})[0]})[0]

```
