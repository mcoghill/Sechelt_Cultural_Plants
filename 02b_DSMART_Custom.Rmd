---
title: "02b_DSMART"
author: "Matthew Coghill"
date: "2/19/2020"
output: html_document
---
This document details how to run the DSMART algorithm: Disaggregation and harmonisation of Soil MAp units through Resampled classification Trees. Originally, the algorithm was intented to create a model predicting soil classes; however, it's being used here to predict site series across a landscape because the process has many parallels. Here, it will be used to convert a TEM to a raster based on the proportions of each polygon.

Additionally, this script assumes that all data manipulation and pre-processing has occurred prior to running (script 02a_PointData_Prep has been ran). The intention of this script was to be a standalone documentation of DSMART since it takes quite a while to run and produce a map.

First, libraries are loaded and data directories are defined. The rdsmart package comes from BitBucket (similar to GitHub) and it is not on CRAN, so it must be downloaded separately and an installation of the devtools package is required for that. The rdsmart package has also been recently updated to streamline the process of making these spatial predictions; by combining the caret package and rdsmart, it is now much quicker to produce a map than it was previously. The whole process takes ~12 hours now on a machine running with 4 cores which is much quicker than previous trials which took upwards of 4 days.

```{r Load Packages}
ls <- c("devtools", "tidyverse", "sf", "raster", "fasterize", "rgdal", "caret", "randomForest")
#bitbucket.packages <- c("rdsmart")
new.packages <- ls[!(ls %in% installed.packages()[, "Package"])]
#new.bitbucket.packages <- bitbucket.packages[!bitbucket.packages %in% installed.packages()[, "Package"]]
if(length(new.packages))
  install.packages(new.packages)
#if(length(new.bitbucket.packages))
#  devtools::install_bitbucket("brendo1001/dsmart")
#ls <- c(ls, bitbucket.packages)
lapply(ls, library, character.only = TRUE)
rm(ls, bitbucket.packages, new.packages, new.bitbucket.packages)

source("../DSMART/R/disaggregate.R")
```

Next, file paths are defined.

```{r Set directories}
AOI <- "Sechelt"
AOI_dir <- file.path(".", paste0(AOI, "_AOI"))
map_res <- 4

shapes_path <- file.path(AOI_dir, "0_raw_inputs", "base_layers")
covariate_dir <- file.path(AOI_dir, "1_map_inputs", "covariates")
dsmart_dir <- file.path(AOI_dir, "1_map_inputs", "dsmart")

stratified = FALSE
```

Next, important data is loaded. There are a lot of covariate layers created from DEM derivatives, satellite imagery, and climate data layers; however, in order to speed up processing, the amount of covariates are limited to only DEM derivatives and satellite imagery indices.

```{r Load data}
# Load covariates
res_folder <- paste0(map_res, "m")
all_covariates <- list.files(file.path(covariate_dir, res_folder), full.names = TRUE)
covariate_index <- starts_with("normal", vars = basename(all_covariates)) # Limit the inputs
covariates <- all_covariates[-covariate_index]
dsmart_covariates <- raster::stack(covariates)

# Load the polygons and associated observations
dsmart_polygons <- readOGR(file.path(dsmart_dir, "inputs", "dsmart_polygons.gpkg"), layer = "dsmart_polygons")

if(stratified == TRUE) {
  dsmart_strata_sf <- st_read(file.path(dsmart_dir, "inputs", "dsmart_strata.gpkg"))
  level_table <- data.frame(ID = 1:nlevels(dsmart_strata_sf$Subzone), 
                            Subzone = levels(dsmart_strata_sf$Subzone))
  dsmart_strata_raster <- asFactor(fasterize(dsmart_strata_sf, dsmart_covariates$dem, field = "Subzone"))
  levels(dsmart_strata_raster) <- level_table
  dsmart_composition <- read.csv(file.path(dsmart_dir, "inputs", "dsmart_composition.csv")) %>% 
    mutate(Subzone = sub("\\..*", "", MapUnit)) %>% 
    merge(level_table) %>% 
    dplyr::select(POLY_NO, MAP_CODE, ID, MapUnit, proportion)
  
} else {
  dsmart_strata_raster <- NULL
  dsmart_composition <- read.csv(file.path(dsmart_dir, "inputs", "dsmart_composition.csv"))
}

# Load additional observations
dsmart_observations <- read.csv(file.path(dsmart_dir, "inputs", "dsmart_observations.csv"))

```

Finally, we need to run the algorithm. This version of DSMART has implementation allowing the use of the caret package to make models, and that functionality is leveraged here. We will use a parallelized implementation of Random Forest modelling in order to predict site series fore each pixel.

It's very important to note that this is a very slow program, thus it's important that all of the datasets created above are in fact correct prior to running the algorithm less you get a few hours in and it fails because the names of the MapUnit columns are not valid in R.

That said, I'll try to explain the process here. First, we need to build a covariate dataset. For the PEM project, we used DEM and satellite derived layers to build our site series maps, and I'll stick to that here as well in order to speed up some of the processing. I'm getting rid of covariate layers whose name starts with "normal" because all of the climate layers (of which there are about 150) begin with the word "normal", so it's a simple way to omit those layers from analysis. The next part (fit_control) specifies some of the modelling intricacies. Modelling will be passed on to the caret package where it will run a parallelized version of random forest analysis to generate class probabilities.

Finally, we can run the dsmart algorithm to predict site series for a given pixel. At the time of running this algorithm, for a 4m resolution, the first part of the algorithm (sampling) took approximately 6.5 hours to run. After that, it starts to produce models and maps from the data, and it takes about 1 hour per model to produce a "realization" (for 5 models that's 5 hours). The final part of the algorithm takes those realizations and summarizes them into n most probable maps (nprob), and for 1 probability map it took about 30 minutes. The first is the most probable map, and the last defined would be the least probable map of that series. The verbose output seems to throw a lot of errors, but when the models are loaded back into a given R session they seem unphased, so I think that there is some issue with the verbose nature of the output seen here so it can be ignored as long as the algorithm is running.

Some ways in which this could be sped up:
1. Reduce the map resolution. Con: the map is a lower resolution
2. Decrease the "rate" parameter in the dsmart function. Con: fewer samples are drawn per polygon so the accuracy may be skewed
3. Change the "method.sample" parameter to "by_area" and draw "rate" number of samples per square km. Con: Doing this could potentially miss small polygons and their associated site series calls.
4. Decrease the "reals" parameter in the dsmart function. Con: fewer models are produced for comparison and final map generation
5. Find a way to parallelize the caret::train function within dsmart (a cluster may need to be created prior to running the algorithm)
6. Decrease the nprob parameter to generate only the most probable map instead of the top 3 most probable maps.

```{r Run dsmart algorithm}
fit_control <- trainControl(method = "repeatedcv", 
                            number = 10, 
                            repeats = 5, 
                            p = 0.9, 
                            returnData = TRUE,
                            returnResamp = "all",
                            savePredictions = "final",
                            classProbs = TRUE,
                            summaryFunction = multiClassSummary, # Adds a suite of performance metrics to the output
                            preProcOptions = list(cutoff = 0.9), # Defines the correlation cutoff for the covariate layers
                            search = "grid",
                            allowParallel = TRUE)

# Create tuning grid
tune_grid <- expand.grid(mtry = round(seq(2, nlayers(dsmart_covariates), length.out = sqrt(nlayers(dsmart_covariates)))), 
                        splitrule = c("gini", "extratrees"))


site_series <- dsmart(covariates = dsmart_covariates, 
                      polygons = dsmart_polygons, 
                      composition = dsmart_composition,
                      rate = 10, # How many samples to draw per polygon
                      reals = 5, # How many models to produce from the modelling run
                      observations = dsmart_observations, 
                      method.sample = "by_polygon", 
                      method.allocate = "weighted", 
                      method.model = "ranger", 
                      strata = dsmart_strata_raster, 
                      nprob = 1, 
                      outputdir = file.path(AOI_dir, "1_map_inputs", "dsmart"), 
                      cpus = detectCores() - 1, 
                      args.model = list(preProcess = c("center", "scale", "nzv", "corr"), 
                                        num.trees = 101, 
                                        trControl = fit_control, 
                                        tuneGrid = tune_grid, 
                                        metric = "logLoss"))

# Last run took about 9 hours to complete
# If the above process finishes the disaggregation process and hangs on the summarise process, use the following part to get going again.
#dsmart_disagg_path <- list.files(file.path(AOI_dir, "1_map_inputs", "dsmart", "output", "realisations"), full.names = TRUE)
#dsmart_reals <- raster::stack(dsmart_disagg_path)
#dsmart_lookup <- read.table(file.path(AOI_dir, "1_map_inputs", "dsmart", "output", "lookup.txt"), sep = ",", header = TRUE)
#site_series_summ <- rdsmart::summarise(realisations = dsmart_reals, 
#                                       lookup = dsmart_lookup, 
#                                       nprob = 1, 
#                                       cpus = detectCores() - 1, 
#                                       outputdir = file.path(AOI_dir, "1_map_inputs", "dsmart"))

# Save the file to a better location
file.copy(from = file.path(AOI_dir, "1_map_inputs", "dsmart", "output", "mostprobable", "mostprob_01_class.tif"), 
            to = file.path(AOI_dir, "1_map_inputs", "covariates", "4m", "site_series.tif"))
```