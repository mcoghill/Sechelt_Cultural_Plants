---
title: "04c_Model_Enhancement"
author: "Matt"
date: "9/9/2020"
output: html_document
---

The purpose of this document is to once again run modelling using similar machine learning techniques from before. At this point, there is more data available so that hopefully modelling plant presence/absence should be more accurate. Again, this script uses the `mlr3` package, so some of the scripting language looks strange. First, load the required packages.

```{r Load Packages, echo=TRUE, results='hide'}

invisible(suppressPackageStartupMessages(
  lapply(c("tidyverse", "tools", "terra", "sf", "mlr3verse"), 
         library, character.only = TRUE)))

# Load custom scripts
source("./_functions/model_gen_mlr3.R")
source("./_functions/predict_landscape_mlr3.R")

```

Next, define the locations and options for where files are located and how to continue processing.

```{r Define directories}

AOI <- "Sechelt"
AOI_dir <- file.path(".", paste0(AOI, "_AOI"))
map_res <- 4
res_dir <- ifelse(is.numeric(map_res), paste0(map_res, "m"), map_res)

input_dir <- file.path(
  "./Sechelt_AOI/1_map_inputs/field_data/processed_2020", res_dir)
covariate_dir <- file.path(AOI_dir, "1_map_inputs", "covariates", res_dir)
shapes_dir <- file.path(AOI_dir, "0_raw_inputs", "base_layers", res_dir)
dsmart_site_ser_dir <- file.path(AOI_dir, "1_map_inputs", "dsmart_2020", 
                                 res_dir, "site_ser")
dsmart_struct_stg_dir <- file.path(AOI_dir, "1_map_inputs", "dsmart_2020", 
                                   res_dir, "struct_stg")
output_dir <- file.path(AOI_dir, "4_map_predictions_enhanced", res_dir)
tile_dir <- file.path(output_dir, "tiles")

dir.create(tile_dir, showWarnings = FALSE, recursive = TRUE)
dir.create(file.path(output_dir, "pres_abs_predictions"), showWarnings = FALSE)
dir.create(file.path(output_dir, "cover_predictions"), showWarnings = FALSE)

```

Next, load the data and define processing options. This is copied and pasted from the 02c script.

```{r Load data}

# List of processing options...
temporal <- c("annual") # either or both of annual and seasonal
ss <- c("simple", "dsmart", NA) # any of simple, dsmart, and/or NA
probabilities <- c(TRUE, FALSE) # Use probabilites or classification
bgc <- c(TRUE, FALSE) # Whether or not to use classified BGC subzone layer
climate_vars <- c(TRUE, FALSE) # TRUE, FALSE
perms <- expand.grid(
  temporal = c(temporal, paste0(temporal, collapse = ", ")), 
  ss = ss, 
  probabilities = c(probabilities, paste0(probabilities, collapse = ", ")),
  bgc = bgc, 
  climate_vars = climate_vars, 
  stringsAsFactors = FALSE) %>% 
  mutate(probabilities = ifelse(is.na(ss), NA, probabilities)) %>% 
  distinct()

# Raster covariates
terrain_covariates <- grep(pattern = "normal|sentinel2", list.files(
  covariate_dir, full.names = TRUE, pattern = ".tif$"), 
  invert = TRUE, value = TRUE)

if("annual" %in% temporal && "seasonal" %in% temporal) {
  s_pattern <- "sentinel2.*..tif$"
  c_pattern <- "normal.*..tif$"
} else if("annual" %in% temporal && !"seasonal" %in% temporal) {
  s_pattern <- "sentinel2.*.2019.tif$"
  c_pattern <- "normal(?!.*(_at.tif|_sm.tif|_sp.tif|_wt.tif))"
} else if(!"annual" %in% temporal && "seasonal" %in% temporal) {
  s_pattern <- "sentinel2.*.(fall.tif|spring.tif|summer.tif|winter.tif)"
  c_pattern <- "normal.*.(_at.tif|_sm.tif|_sp.tif|_wt.tif)"
}

sentinel_covariates <- grep(file.path(covariate_dir, s_pattern), 
                            list.files(covariate_dir, full.names = TRUE), 
                            value = TRUE, perl = TRUE)

climate_covariates <- if(any(climate_vars)) {
  grep(file.path(covariate_dir, c_pattern),
       list.files(covariate_dir, full.names = TRUE), 
       value = TRUE, perl = TRUE)
} else NULL

bgc_layer <- if(any(bgc)) {
  bgc_vector <- vect(
    file.path(AOI_dir, "0_raw_inputs", "base_layers", res_dir, "bec.gpkg"))
  bgc_raster <- terra::rasterize(bgc_vector, rast(terrain_covariates[1])) %>% 
    magrittr::set_names("bgc") %>% 
    writeRaster(
      filename = grep("site_ser", file.path(dsmart_site_ser_dir, "bgc.tif"), value = TRUE), 
      overwrite = TRUE, wopt = list(datatype = "INT2S"))
  
  sources(bgc_raster)[, "source"]
} else NULL

# There are 2 sources of site series calls: simple and DSMART. under both of those,
# there are class outputs and probabilities. We need to create model permutations
# for each scenario
if(!all(is.na(ss))) {
  site_series <- c(
    rast(file.path(dsmart_site_ser_dir, "simple", "site_series_simple.tif")) %>% 
      magrittr::set_names("simple_site_ser_class"),
    rast(grep(
      "site_series_simple.tif", 
      list.files(file.path(dsmart_site_ser_dir, "simple"), full.names = TRUE),
      invert = TRUE, value = TRUE)) %>% 
      magrittr::set_names(paste0("simple_site_ser_", names(.))),
    rast(file.path(dsmart_site_ser_dir, "site_ser_dsmart.tif")) %>% 
      magrittr::set_names("dsmart_site_ser_class"),
    rast(list.files(
      file.path(dsmart_site_ser_dir, "output", "probabilities"), full.names = TRUE)) %>% 
      magrittr::set_names(paste0("dsmart_site_ser_probability_", names(.))))
  
  # Create filter based on ss and bgc variables
  ss_pattern <- grep(ifelse(length(probabilities) > 1, "probability|class", 
                            ifelse(probabilities, "probability", "class")),
                     grep(paste0(ss, collapse = "|"), names(site_series), value = TRUE), 
                     value = TRUE)
  site_series <- subset(site_series, ss_pattern)
} else site_series <- NULL

# We also need to include the structural stage rasters in this analysis as well.
# I debated on whether I should include the associated probabilities. In the end,
# I decided no.
struct_stg <- rast(file.path(dsmart_struct_stg_dir, "struct_stg_dsmart.tif")) %>% 
  magrittr::set_names("dsmart_struct_stg_class")

# Combine all covariates
covariates <- c(rast(c(terrain_covariates, sentinel_covariates, 
                       climate_covariates, bgc_layer)),
                site_series, struct_stg)

# Find the best masking layer to use in the predict_landscape algorithm
mask_layers <- sapply(names(covariates), function(x) {
  cat(paste0("\rCounting NA values in ", x, " [", 
             which(x == names(covariates)), 
             " of ", nlyr(covariates), "]\n"))
  unname(freq(subset(covariates, x) * 0)[, "count"])}) %>% 
  data.frame(data_cells = .) %>% 
  rownames_to_column("layer") %>% 
  arrange(desc(data_cells))

covariates <- subset(covariates, dplyr::pull(
  dplyr::filter(mask_layers, data_cells >= quantile(data_cells, prob = 0.05)), 
  var = layer))

# mask_layers <- data.frame(layer = "chm", data_cells = Inf)

# Attributed point data - limit to only CORNCAN and RUBUSPE
pres_abs_points <- sapply(c("Corn can", "Rubu spe"), function(x) {
  st_read(file.path(input_dir, "pres_abs.gpkg"), layer = x, quiet = TRUE) %>% 
    cbind(st_coordinates(.)) %>% 
    {if(!is.null(site_series)) {
      cbind(., terra::extract(c(site_series, struct_stg), st_coordinates(.)))
    } else .} %>% 
    {if(!is.null(bgc_layer)) {
      cbind(., terra::extract(rast(bgc_layer), st_coordinates(.)))
    } else .} %>% 
    st_drop_geometry() %>%
    mutate(across(c("Pres", "simple_site_ser_class", "dsmart_site_ser_class", 
                    "dsmart_struct_stg_class", "bgc"), as.factor)) %>%
    dplyr::select(Pres, names(covariates), X, Y)
  }, simplify = FALSE, USE.NAMES = TRUE)

# Create list of variable names to be selected 
var_list <- do.call(c, lapply(1:nrow(perms), function(i) {
  
  # Generate output ID
  id <- paste0(
    
    if(perms[i, "bgc"]) {
      "_bec_"
    } else "_",
    
    if(perms[i, "temporal"] == "annual, seasonal") {
      "ann_seas_"
    } else if(perms[i, "temporal"] == "annual") {
      "annual_"
    } else if(perms[i, "temporal"] == "seasonal") "seasonal_",
    
    if(perms[i, "climate_vars"]) {
      "climate_"
    } else NULL,
    
    if(!is.na(perms[i, "ss"])) {
      paste0(perms[i, "ss"], "_", ifelse(
        length(unlist(strsplit(perms[i, "probabilities"], split = "[, ]"))) > 1,
        "probs_class", ifelse(perms[i, "probabilities"], "probs", "class")))
    } else "no_ss")
  
  # Use character matching to select layers used in each model permutation
  filtered_data <- lapply(pres_abs_points[1], function(x) {
    variables <- c(
      basename(tools::file_path_sans_ext(terrain_covariates)), 
      names(struct_stg),
      if("annual" %in% unlist(strsplit(perms[i, "temporal"], split = "[, ]"))) {
        c(grep("sentinel2.*.2019", names(x), value = TRUE),
          grep("normal(?!.*(_at|_sm|_sp|_wt))", names(x), value = TRUE, perl = TRUE))
      },
      if("seasonal" %in% unlist(strsplit(perms[i, "temporal"], split = "[, ]"))) {
        c(grep("sentinel2.*.(fall|spring|summer|winter)", names(x), value = TRUE), 
          grep("normal.*.(_at|_sm|_sp|_wt)", names(x), value = TRUE, perl = TRUE))
      },
      
      if(!is.na(perms[i, "ss"])) {
        grep(ifelse(
          length(unlist(strsplit(perms[i, "probabilities"], split = "[, ]"))) > 1, 
          "probability|class", ifelse(perms[i, "probabilities"], "probability", "class")), 
          grep(perms[i, "ss"], names(x), value = TRUE), 
          value = TRUE)
      },
      
      if(perms[i, "bgc"]) "bgc"
    )
    if(!perms[i, "climate_vars"]) {
      variables <- variables[!startsWith(variables, "normal")]
    }
    return(variables)
  }) %>% setNames(id)
}))

```



```{r mlr3 Presence probability modelling}

# Create the list of input datasets
pres_abs <- lapply(names(var_list), function(i) {
  lapply(pres_abs_points, function(x) {
    x %>% dplyr::select(any_of(c("Pres", var_list[[i]], "x", "y", "X", "Y"))) %>% 
      drop_na() %>% 
      dplyr::rename_with(toupper, c("X", "x", "Y", "y")
                         [c("X", "x", "Y", "y") %in% names(.)])
  }) %>% setNames(paste0(names(.), i))
}) %>% unlist(recursive = FALSE)

# Write input data
dir_out <- file.path(output_dir, "pres_abs_predictions")
dir.create(dir_out, showWarnings = FALSE, recursive = TRUE)
lapply(names(pres_abs), function(x) {
  y <- st_as_sf(pres_abs[[x]], coords = c("X", "Y"), crs = crs(covariates))
  st_write(y, file.path(dir_out, paste0("pres_abs_input_data.gpkg")), 
           layer = x, delete_layer = TRUE, quiet = TRUE)
})[0]

# To save a bit of memory and in order to see results as they are generated, 
# iterate through the list of input data to model and predict
for(i in 1:length(pres_abs)) {
  
  dir_out <- file.path(output_dir, "pres_abs_predictions", names(pres_abs[i]))
  dir.create(dir_out, showWarnings = FALSE)
  
  # Create models for each dataset. Each model is a 10-fold spatial cross validation
  # repeated 5 times.
  pres_models <- model_gen_mlr3(traindat = pres_abs[i], target = "Pres", 
                                     feature_selection = "filter", type = "prob")
  
  # Extract and write performance metrics
  pres_performance <- sapply(names(pres_models$learners), function(x) {
    
    folds <- pres_models$learners[[x]]$instance_args$resampling$param_set$values$folds
    reps <- pres_models$learners[[x]]$instance_args$resampling$param_set$values$repeats
    n_iters <- pres_models$learners[[x]]$archive$benchmark_result$n_resample_results
    
    tune_results <- data.frame(
      filter_fraction = pres_models$learners[[x]]$archive$data()$importance.filter.frac, 
      classif.ce = pres_models$learners[[x]]$archive$data()$classif.ce
    ) %>% dplyr::arrange(classif.ce)
    
    confusion <- as.data.frame(
      pres_models$learners[[x]]$model$tuning_instance$archive$benchmark_result$
        resample_results$resample_result[[
          pres_models$learners[[x]]$model$tuning_instance$archive$best()$batch_nr]]$
        prediction()$confusion)
    
    model_data <- lapply(1:n_iters, function(j) {
      test_data <- pres_models$learners[[x]]$archive$benchmark_result$resample_result(j)$
        resampling$instance %>% 
        cbind(batch_nr = j)
      train_data <- bind_rows(lapply(1:reps, function(k) {
        bind_rows(lapply(1:folds, function(l) {
          test_rows <- dplyr::filter(test_data, rep == k, fold == l)
          missing <- as.numeric(rownames(pres_abs[[i]])[
            !as.numeric(rownames(pres_abs[[i]])) %in% test_rows$row_id])
          data.frame(row_id = missing, rep = k, fold = l, batch_nr = j)
        }))
      }))
      return(list(test_data = test_data, train_data = train_data))
    }) %>% setNames(paste0(
      "filter_frac = ", 
      pres_models$learners[[x]]$model$tuning_instance$archive$data()$
        importance.filter.frac))
    
    train_data <- do.call(rbind, lapply(model_data, "[[", "train_data")) %>% 
      magrittr::set_rownames(NULL)
    test_data <- do.call(rbind, lapply(model_data, "[[", "test_data")) %>% 
      magrittr::set_rownames(NULL)
    
    imp <- data.frame(importance = pres_models$learners[[x]]$learner$model$
                        importance$scores) %>% 
      rownames_to_column("layer") %>% 
      dplyr::filter(layer %in% pres_models$learners[[x]]$learner$model$
                      importance$features) %>% 
      arrange(desc(importance))
    
    out <- utils::capture.output(pres_models$learners[[x]]$learner$model$classif.ranger$model)
    best_model <- pres_models$learners[[x]]$learner$model$classif.ranger$model
    
    write.csv(tune_results, file = file.path(dir_out, paste0(x, "_pres_abs_tune_results.csv")),
              row.names = FALSE)
    cat(out, file = file.path(dir_out, paste0(x, "_pres_abs_model.txt")), sep = "\n")
    save(best_model, file = file.path(dir_out, paste0(x, "_pres_abs_model.RData")))
    write.csv(test_data, file = file.path(
      dir_out, paste0(x, "_pres_abs_model_resampling_test_data.csv")), 
      row.names = FALSE)
    write.csv(train_data, file = file.path(
      dir_out, paste0(x, "_pres_abs_model_resampling_train_data.csv")), 
      row.names = FALSE)
    write.csv(confusion, file = file.path(dir_out, paste0(x, "_pres_abs_confusion.csv")), 
              row.names = FALSE)
    write.csv(imp, file.path(dir_out, paste0(x, "_pres_abs_var_importance.csv")), 
              row.names = FALSE)
    write.csv(pres_abs[i][[x]], file.path(dir_out, paste0(x, "_pres_abs_input_data.csv")), 
              row.names = FALSE)
    
    return(list(
      tune_results = tune_results,
      best_model = best_model, 
      resampling = test_data,
      confusion = confusion,
      importance = imp
    ))
    
  }, simplify = FALSE, USE.NAMES = TRUE)
  
  # Run the map predictions
  pres_abs_predictions <- sapply(names(pres_models$learners), function(x) {
    
    cov_sub <- subset(covariates, pres_models$tasks[[x]]$col_roles$feature)
    
    for(j in 1:nrow(mask_layers)) {
      if(mask_layers[j, "layer"] %in% names(cov_sub)) {
        pred_mask <- mask_layers[j, "layer"]
        break
      }
    }
    # source("./_functions/predict_landscape_mlr3.R")
    out <- predict_landscape_mlr3(
      learner = pres_models$learners[[x]],
      task = pres_models$tasks[[x]], 
      type = "prob",
      covariates = cov_sub,
      tilesize = 500, 
      outDir = tile_dir, 
      mask_layer = pred_mask
    )
    
    # with predict_landscape_mlr3 type set to "prob", the output is a list of files
    for(j in out) {
      file.copy(
        from = j,
        to = file.path(dir_out, paste0(x, "_", gsub("_.*", "", basename(j)), ".tif")),
        overwrite = TRUE)
    }
    return(out)
  }, simplify = FALSE, USE.NAMES = TRUE)
}

# Using the available cover model results, find the best models for each species
# First, collect all of that data together and then use an upper percentile or top
# number of models from the entire collection for a single species
best_model_sets <- bind_rows(lapply(c("Corn can", "Rubu spe"), function(x) {
  
  sp_dirs <- dir(file.path(output_dir, "pres_abs_predictions"), pattern = x, 
                 full.names = TRUE)
  
  result_list <- bind_rows(lapply(sp_dirs, function(i) {
    
    read.csv(file.path(i, list.files(i, pattern = "_tune_results.csv$"))) %>% 
      dplyr::slice(which.min(classif.ce)) %>% 
      mutate(model = basename(i))
  })) %>% arrange(classif.ce)
}))

write.csv(best_model_sets, file.path(
  output_dir, "pres_abs_predictions", "model_performance_index.csv"), 
  row.names = FALSE)

```



```{r Zonal statistics}

# Using the dsmart TEM as the zonal layer, run zonal statistics for each map model
site_series_z <- rast(file.path(dsmart_site_ser_dir, "site_ser_dsmart.tif")) %>% 
      magrittr::set_names("dsmart_ss_class")

# Get the numeric codes representing the site series classes
dsmart_lookup <- read.csv(file.path(dsmart_site_ser_dir, "output", "lookup.txt"))

# Collect all model output folder names. Should be the same for both pres/abs
# and cover
model_outs <- list.dirs(
  file.path(output_dir, "pres_abs_predictions"), 
  full.names = FALSE)[-1]

# Load the predicted output rasters from each folder and rename them
pres_abs_outs <- rast(file.path(output_dir, "pres_abs_predictions", model_outs, 
                    paste0(model_outs, "_TRUE.tif"))) %>% 
  magrittr::set_names(model_outs)

# Perform zonal statistics for each of the predicted outputs
pres_stats <- sapply(c("mean", "min", "max", "sd"), function(x) 
  zonal(pres_abs_outs, site_series_z, fun = x, na.rm = TRUE), 
  simplify = FALSE, USE.NAMES = TRUE)

# Reorder the dataframes and merge with the lookup table to define actual
# site series instead of just numeric codes. Save the outputs to the defined
# folders.
pres_reorder <- sapply(model_outs, function(x) {
  z <- lapply(pres_stats, function(y) y[, c("zone", make.names(x))]) %>% Reduce(
    function(x, y, ...) merge(x, y, all = TRUE, by = "zone", ...), .) %>% 
    magrittr::set_names(c("zone", "mean", "min", "max", "sd")) %>% 
    merge(dsmart_lookup, ., by.x = "code", by.y = "zone")
  write.csv(z, file.path(output_dir, "pres_abs_predictions", x, 
                         paste0(x, "_pres_abs_zonal_stats.csv")), 
            row.names = FALSE)
  return(z)
}, simplify = FALSE, USE.NAMES = TRUE)

```
