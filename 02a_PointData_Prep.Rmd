---
title: "02a_Data_Consolidation"
author: "Matthew Coghill"
date: "2/6/2020"
output: html_document
---

This document performs various tasks, including:
1. Import/download of useful datasets;
2. Combining those datasets to produce master datasets for predicting:
  + A site series raster
  + Predictive maps for cultural plants of interest (pres/abs and cover)

A lot of the work stems from previously ran scripts which have:
1. Created a 4m resolution DEM;
2. Produced DEM derivatives;
3. Downloaded and produced satellite layers and indices;
4. Downloaded BCData layers

Using the RMarkdown format, I'll walk through the scripting process used to create the final maps. First, we must load/install the required packages.

```{r Load Packages}

suppressMessages(suppressWarnings({
  ls <- c("plyr", "tidyverse", "readxl", "svSocket", "RODBC", "sf", "terra")
  new.packages <- ls[!(ls %in% installed.packages()[, "Package"])]
  if(length(new.packages))
    install.packages(new.packages)
  sapply(ls, library, character.only = TRUE, quietly = TRUE)[0]
  rm(ls, new.packages)
}))

```

Next, we should set some directories for this work. A lot of the scripting used here is adapted from PEM mapping, so the file folder structure used will be much the same. The idea is that if we want to apply a similar shcematic to another study area, we should only have to adjust the directories for that given study area.

Additionally, we need to set the directory for where the 32-bit version of R is installed on your machine. This will be used later on in MS Access Database processing. (will probably move function to separate script file)

```{r Set directories}

AOI <- "Sechelt"
AOI_dir <- file.path(".", paste0(AOI, "_AOI"))
map_res <- 4
poi <- c("RUBUSPE", "CORNCAN", "VERAVIR", "LEDUGRO") # plants of interest

shapes_path <- file.path(AOI_dir, "0_raw_inputs", "base_layers")
field_data_dir <- file.path(AOI_dir, "1_map_inputs", "field_data")
covariate_dir <- file.path(AOI_dir, "1_map_inputs", "covariates")
dsmart_dir <- file.path(AOI_dir, "1_map_inputs", "dsmart")
out_path <- file.path(AOI_dir, "2_map_predictions")

```

The next thing that is typical is to place any repeated functions at the top so that they may be called in a given parameter below. access_query_32 will open a separate 32-bit version of R in order to read in the tables of a Microsoft Access database, while the droplevels function adds the sf class to be able to drop factor levels when necessary without throwing an error.

```{r Functions}

access_query_32 <- function(db_path = NULL) {
  # variables to make values uniform
  # Shouldn't need to change socket port number, but if so just change variable
  # here and the rest should run fine.
  sock_port <- 8642L
  
  if(file.exists(db_path)) {
    # start socket server to transfer data to 32 bit session
    svSocket::startSocketServer(port = sock_port, server.name = "access_query_32", local = TRUE)
    
    # Build expression to pass to 32 bit R session. The bquote() function allows
    # for variables to be passed as strings if preceded by a period
    expr <- bquote({
      ls <- c("svSocket", "RODBC", "tidyverse")
      new.packages <- ls[!(ls %in% installed.packages()[, "Package"])]
      if(length(new.packages)) install.packages(new.packages, repos = "https://cloud.r-project.org")
      lapply(ls, library, character.only = TRUE)[0]
      a32_con <- RODBC::odbcConnectAccess(.(db_path))
      tbl_list <- RODBC::sqlTables(a32_con) %>% 
        dplyr::filter(TABLE_TYPE != "SYSTEM TABLE")
      for(j in tbl_list$TABLE_NAME) {
        tryCatch({
          do.call("<-", list(paste0("MDB_", j), RODBC::sqlFetch(a32_con, j)))
        }, error = function(c) {
          message("Could not process table from MDB")
          print(c)
        })
      }
      RODBC::odbcCloseAll()
      mdb_out <- objects(pattern = "MDB_")
      tbl_out <- list()
      for(k in 1:length(mdb_out)) {
        tryCatch({
          message(paste("Processing table", mdb_out[k]))
          m <- mdb_out[k]
          tbl_out[[m]] <- get(mdb_out[k])
        }, error = function(c) {
          message("Could not push table to environment")
          print(c)
        })
      }
      sv_con <- socketConnection(port = .(sock_port))
      svSocket::evalServer(sv_con, tbl_out, tbl_out)
    })
    expr <- paste(paste(expr)[-1], collapse = ";")
    
    # launch 32 bit R session and run expressions
    prog <- file.path(R.home(), "bin/i386/Rscript.exe")
    system2(prog, args = c("-e", shQuote(expr)), stdout = NULL, wait = TRUE, 
            timeout = 30, invisible = TRUE)
    
    # stop socket server
    svSocket::stopSocketServer(port = sock_port)
  } else {
    warning("database not found: ", db_path)
  }
  return(tbl_out)
}

droplevels.sf = function(x, except, exclude, ...) x # Allows sf dataframes to have unused factor
droplevels.sfc = function(x, except, exclude, ...) x # Allows sf dataframes to have unused factor

```

In the next chunk, we are processing the 2019 field data excel file. The file contains 5 separate worksheets:
1. Site information for each visted site;
2. Plant cover information for Rubus spectabilis (salmonberry) at each site;
3. Plant cover information for Veratrum viride (hellebore) at each site; 
4. Plant cover information for Cornus canadensis (dogwood) at each site; and
5. Miscellaneous/extra plants that were recorded but not part of the sampling set

It should be noted that this script deals with the raw datasheets as they were initially sent. No manipulation in Excel was performed, all manipulations are carried out here in code for transparencies sake.

Each worksheet is read in separately but processed in a similar manner. First, the site information sheet is dealt with. This contains relevant BEC info for each site. The recorded site series was a bit of a jumble; however, a search and replace for the first two numerical values only aided significantly in defining site series consistently (i.e.: if there was a site series call of 10(11), the site series call was changed to 10. Depending on the modelling outcomes of dsmart, I might change this to look only for pure calls in the list). This sheet also contained lat/long for points, which were used to create an sf dataframe

Each of the species specific sheets had the site ID but no lat/long for points, so the sheets had to be joined together using the site ID in order to associate species data with a location on a map. For presence/absence data, the query was essentially "if there was a recording of plant cover for this site, return a value of TRUE, and if not, return FALSE". Plant cover data was the sum of each of the plant cover types in each structural stage column.

There was discussion on whether or not to include Ledum groenlandicum (Labrador tea) in any analyses. There was a single recording of it in the final sheet of the excel file, so I decided to see what would come of throwing it into a model. To boost the presence/absence ratio for Ledum, I only used the points that were included on the last datasheet rather than all of the points.

```{r Field data 2019 - Site}

spreadsheet <- file.path(field_data_dir, "raw", "shishalhBECandCulturalPlants_FieldData_2019.xlsx")
sheets <- excel_sheets(spreadsheet)

# Site data processing:
# 1) Remove brackets and everything within them
# 2) Shorten strings to 5 characters max
# 3) Remove strings starting with lowercase
# 4) In strings with commas, remove white space
# 5) In strings with white space, remove the text after the white space
# 6) If there is a comma, keep only text after it if it does not begin with lowercase
# 7) Removes all special characters except for commas
# 8) Removes trailing commas
# 9) Apply fix to incorrectly labelled BGC subzones
# 10) Create map unit column
# 11) Make an sf dataframe using lat/long and transform to EPSG 3005
sechelt_bec_full <- read_excel(spreadsheet, sheet = "Site data") %>% 
  mutate(`Site Disturbance` = gsub("\\s*\\([^\\)]+\\)", "", `Site Disturbance`)) %>% # 1
  mutate(`Site Disturbance` = substr(`Site Disturbance`, start = 1, stop = 5)) %>% # 2
  mutate(`Site Disturbance` = gsub("^[[:lower:]]+", NA, `Site Disturbance`)) %>% # 3
  mutate(`Site Disturbance` = gsub(", +", ",", `Site Disturbance`)) %>% # 4
  mutate(`Site Disturbance` = sub(" .*", "", `Site Disturbance`)) %>% # 5
  mutate(`Site Disturbance` = gsub(",[a-z]+", "", `Site Disturbance`)) %>% # 6
  mutate(`Site Disturbance` = str_replace_all(`Site Disturbance`, "[^[A-Za-z,]]", " ") %>% 
           str_squish(.)  %>% str_replace_all(.," , ", ", ")) %>% # 7
  mutate(`Site Disturbance` = gsub(",$", "", `Site Disturbance`)) %>% # 8
  mutate(`Structural Stage` = substr(`Structural Stage`, start = 1, stop = 2)) %>% 
  mutate(`Structural Stage` = str_replace_all(`Structural Stage`, "[^[:alnum:]]", " ")) %>% 
  dplyr::rename(subzone = `BGC Subzone/variant`) %>% 
  mutate(subzone = str_replace(subzone, "CHWxm", "CWHxm"), 
         subzone = str_replace(subzone, "CWHxm", "CWHxm1")) %>% # 9
  st_as_sf(coords = c("Long", "Lat"), crs = 4326) %>% # 11
  st_transform(3005)

# Make site series calls pure calls only (for simplicity)
# sechelt_bec <- dplyr::filter(sechelt_bec_full, nchar(`Forested Site Series`) == 2) %>% 
#   dplyr::mutate(MapUnit = as.factor(make.names(paste0(subzone, "/", 
#                                                `Forested Site Series`))))

# Make site series calls majority (or first) calls only
sechelt_bec <- dplyr::mutate(
  sechelt_bec_full, `Forested Site Series` = stringi::stri_extract_first_regex(
    `Forested Site Series`, "[0-9]+"
  )) %>% 
  dplyr::mutate(MapUnit = as.factor(make.names(paste0(subzone, "/", 
                                               `Forested Site Series`))))

```


```{r Field Data 2019 - Plants}
# The species tables don't have coordinates associated with them, but they can 
# be merged with the BEC table which has that data

# It's unclear how Ledugro was sampled in the field. There is an extra spreadsheet in the 
# excel file with more plant cover data, however I'm uncertain whether this was checked 
# at each site, or if it was recorded as they were seen. Since Ledugro already occurs far
# less in frequency than other plants, I'll err on the conservative side and assume that 
# it was only recorded as it was seen, and just use the 20 points from that spreadsheet.

sechelt_plants <- sapply(sheets[!sheets %in% c("Site data")], function(x) {
  dplyr::select(sechelt_bec_full, `Point ID`, `Structural Stage`, 
                `Site Disturbance`, `% Canopy Cover`) %>% 
    merge(read_excel(spreadsheet, x)) %>% 
    dplyr::filter(Comments != "*not recorded" | is.na(Comments)) %>%
    mutate(B2 = as.numeric(unlist(str_extract_all(B2,"\\(?[0-9,.]+\\)?")))) %>% 
    mutate(Cover = ifelse(
      Species %in% c("Rubuspe", "Corncan", "Ledgro", "Veravir"),
      rowSums(st_drop_geometry(.)[, c("A1", "A2", "A3", "B1", "B2", "C", "D")], 
              na.rm = TRUE), 0)) %>% 
    mutate(Pres = Cover > 0.1, 
           `Structural Stage` = as.factor(`Structural Stage`), 
           `Site Disturbance` = as.factor(`Site Disturbance`)) %>% 
    dplyr::rename(point_id = `Point ID`) %>% 
    dplyr::select(point_id, Pres, Cover, `Structural Stage`, 
                  `Site Disturbance`, `% Canopy Cover`) %>% 
    drop_na()
}, simplify = FALSE, USE.NAMES = TRUE)

# Rename list names
names(sechelt_plants) <- ifelse(names(sechelt_plants) == "Other species", 
                                "LEDUGRO", toupper(names(sechelt_plants)))

sechelt_pres_cov <- lapply(sechelt_plants, function(x) {
  dplyr::select(x, point_id, Pres, Cover) %>% 
    {if(!any(.$Pres)) return(.[0, ]) else return(.)}
  })

```


In 2019, a total of 84 sites were sampled in the Sechelt study area. Unfortunately, that doesn't exactly provide us with a lot of data; however, there are some external bits of data that we can call upon. The TEM project that went on for the Sechelt area is available freely on EcoCat, and there are two important datasets to garnish from there:
1. An excel file containing point data of site series calls and notes with plant names;
2. A Microsoft Access Database file with site series calls as well as all of the plants recorded at each site and their respective cover data

Opening the excel file is no problem; columns were added to that dataset to show presence/absence data of the culturally important plants.

Opening the Microsoft Access Database (mdb) requires a much more intricate process. mdb files are created using 32-bit versions of Microsof Office and thus can only be opened in R when ran in a 32-bit session. At this point, one might give up on opening the database altogether, but fear not! You can open a separate R session within your current R session in order to process various functions. In this case, we want to be able to process the mdb file from the TEM project in a separate 32-bit R session and then pass the data on to our current R session (that's the access_query_32 function below). Note an issue that may arise when trying to run this function: It might fail because the packages required are not installed. SOLUTION: unscripted (yet), but go to the directory of your 32-bit R session, open R_gui.exe, and install the missing packages that way.

When the mdb is processed and passed on to your current R session, it passes on all of the tables that it found within the database. In this case there were a lot of useless data (leftover queries that have no meaning to this project, tables with 0 records, small vector data), and these are immediately removed. From that leftover list of tables, we only actually need three of them: Site data, vegetation layer data, and species data. All of these tables are linked together by common fields in their tables.

For reference: https://a100.gov.bc.ca/pub/acat/public/viewReport.do?reportId=35895

```{r Get external datasets}

# Download TEM Spreadsheet
download.file(
  "http://a100.gov.bc.ca/appsdata/acat/documents/r35895/tem_4678_eci_1363698000276_6de51d6c988801e3369406fe9a22713aa4dca27f63de77d54bf44b70a246813b.xls", 
  destfile = file.path(field_data_dir, "raw", "tem_data.xls"), mode = "wb", quiet = TRUE)

# Download TEM .mdb database. 
# This has issues when trying to code it in, and you'll likely get a successful
# download but the file will be empty. This has to do with the link being an 
# http link rather than an https link (I think?), and can carry potential securtiy
# issues due to that. This prevents a full download of the file and I have tried
# multiple packages to try to get around this to no avail. The best way to do this
# is to copy the link below, paste it into your own web browser, and download the
# file accepting the security risks through clicking the appropriate buttons.
# When the download starts, make sure you save the .mdb file into the same folder
# as the TEM spreadsheet above.
# https://a100.gov.bc.ca/pub/acat/public/viewReport.do?reportId=35895
# download.file(
#   "http://a100.gov.bc.ca/appsdata/acat/documents/r35895/tem_4678_eci_1363697969131_6de51d6c988801e3369406fe9a22713aa4dca27f63de77d54bf44b70a246813b.mdb", 
#   destfile = file.path(field_data_dir, "raw", "tem_data.mdb"), mode = "wb")

# Using RSelenium is tricky but it may work:
# library(RSelenium)
# eCaps <- list(
#   chromeOptions = 
#     list(prefs = list(
#       "profile.default_content_settings.popups" = 0L,
#       "download.prompt_for_download" = FALSE,
#       "download.default_directory" = "C:/Users/matth/Downloads"
#     )
#     )
# )
# rsel <- rsDriver(chromever = "84.0.4147.30", extraCapabilities = eCaps)
# remDr <- rsel[["client"]]
# remDr$navigate("https://a100.gov.bc.ca/pub/acat/public/viewReport.do?reportId=35895")
# webElem <- remDr$findElement(using = "xpath", "//a[@href='http://a100.gov.bc.ca/appsdata/acat/documents/r35895/tem_4678_eci_1363697969131_6de51d6c988801e3369406fe9a22713aa4dca27f63de77d54bf44b70a246813b.mdb']")
# webElem$clickElement()
# remDr$close()
# rsel[["server"]]$stop()

# Process the TEM spreadsheet by finding matching string in the comments portion
# of the spreadsheet
tem_xls <- sapply(poi, function(x) {
  read_xls(file.path(field_data_dir, "raw", "tem_data.xls"), skip = 1) %>% 
    mutate(Pres = grepl(x, Comments)) %>% 
    dplyr::mutate(subzone = paste0(`BGC Zone`, tolower(Subzone), ifelse(is.na(Variant), "", Variant))) %>% 
    as.data.frame() %>% 
    st_as_sf(coords = c("UTM_East", "UTM_North"), crs = 26910) %>% 
    st_transform(3005)
}, simplify = FALSE, USE.NAMES = TRUE)

# Process the .mdb file in a 32 bit R session, function passes variables 
# to this R session in a single list entry
tem_mdb <- access_query_32(db_path = file.path(field_data_dir, "raw", "tem_data.mdb")) %>% 
  lapply(function(x) {
  if(class(x) == "data.frame") {
    if(nrow(x) == 0) {
      x <- NULL
    } else x
  } else x <- NULL
}) %>% compact() %>% 
  .[grep("PlotData|VegLayer|VegSpecies", names(.))] %>% 
  lapply(function(x) Filter(function(y)!all(is.na(y)), x))

```

After downloading the TEM datasets, we need to prepare them. The next chunk does some data wrangling of the TEM site data get more directed outputs.

```{r TEM data - Site}

# TEM excel spreadsheet - BEC info (just uses one of the species since all species has same data)
# This data will be used in generating a site series raster. so the "best" calls
# should be used. Initially, this was pure calls only, however it might be better
# to do majority calls instead
tem_xls_bec <- dplyr::select(tem_xls[[1]], Poly_Nbr, ECI_Tag, subzone,
                             Sser1, Sser2, Sser3, SDec1, Sdec2, Sdec3, `Str. Stg.`) %>% 
  dplyr::rename(StructuralStage = `Str. Stg.`) %>% 
  dplyr::filter(ifelse(is.na(Sdec3), SDec1 >= 6, SDec1 >= 5)) %>% 
  drop_na(SDec1) %>% 
  mutate(Sser1 = substr(Sser1, start = 1, stop = 2)) %>% 
  mutate(MapUnit = as.factor(make.names(paste0(subzone, "/", Sser1)))) %>% 
  st_sf()

# MDB_tblPlotData has way too many columns, keep only important ones for BEC
tem_mdb_bec <- dplyr::select(
  tem_mdb$MDB_tblPlotData, PlotID, PlotNumber, BECUnit, SiteSeriesSymbol, 
  UTMNorthing, UTMEasting, StructuralStage, SiteNotes) %>% 
  separate(BECUnit, into = c("subzone", "site_series")) %>% 
  dplyr::mutate(MapUnit = ifelse(site_series == "00", as.character(SiteSeriesSymbol), 
                          paste0(subzone, "/", site_series))) %>% 
  mutate(MapUnit = as.factor(make.names(MapUnit))) %>% 
  st_as_sf(coords = c("UTMEasting", "UTMNorthing"), crs = 26910) %>% 
  st_transform(3005)

```

Further data wrangling is applied to plant data from the TEM datasets.

```{r TEM Data - Plants}

tem_xls_pres <- lapply(tem_xls, function(x) {
  dplyr::select(x, Poly_Nbr, ECI_Tag, Pres) %>% 
    unite(point_id, c(Poly_Nbr, ECI_Tag), sep = "_", remove = TRUE) %>% 
    {if(!any(.$Pres)) return(.[0, ]) else return(.)}
  })

# Merge all 3 .mdb tables and filter plants of interest
tem_mdb_merge <- expand.grid(PlotID = unique(tem_mdb$MDB_tblPlotData$PlotID), 
                             Species = unique(tem_mdb$MDB_tblVegSpecies$Species)) %>% 
  merge(
    merge(tem_mdb$MDB_tblVegLayer[, c("PlotID", "VegLayerID")], 
          tem_mdb$MDB_tblVegSpecies[, c("VegLayerID", "Species", "Cover")]), all = TRUE) %>% 
  merge(tem_mdb$MDB_tblPlotData[, c("PlotID", "PlotNumber", "PolygonNumber", 
                                    "UTMEasting", "UTMNorthing")]) %>% 
  unite(point_id, c(PolygonNumber, PlotNumber), sep = "_", remove = TRUE) %>% 
  group_by(point_id, Species, UTMEasting, UTMNorthing) %>% 
  dplyr::summarise(across(Cover, sum), .groups = "drop") %>% 
  dplyr::mutate(Pres = Cover > 0.1) %>% 
  replace_na(list(Pres = FALSE, Cover = 0)) %>% 
  st_as_sf(coords = c("UTMEasting", "UTMNorthing"), crs = 26910) %>% 
  st_transform(3005)

tem_mdb_pres_cov <- sapply(poi, function(x) {
  dplyr::filter(tem_mdb_merge, Species == x) %>% 
    dplyr::select(point_id, Pres, Cover)
}, simplify = FALSE, USE.NAMES = TRUE)

```

##TEM data extraction
Next, we want to read in the TEM polygon layer and apply some filtering/data manipulation of it. The TEM feeds into the dsmart program to predict site series for the study area, and in the end it needs columns with an integer polygon number, a unique site ID associated with the polygon number, map unit ID's, and proportions of those map units. I'll walk through the processing steps:

1. Select only the important columns needed for processing. The original TEM file comes with approximately 80 columns of data which is too cumbersome to work with, so we should only select the ones important for our work: Polygon ID, BEC subzone, site series labels, site series deciles, and site series map codes.
2. Rename the columns to something more useful
3. Remove any NA values in the MapUnit1 column. If there are NA values in the first column, then there will be NA values in all of the other columns, and NA values can cause issues downstream.
4. Rename Map Units in each MapUnit column. If it is a forested site unit, it is concatenated from the BGC label and the site series call; if it's nonforested (i.e.: site series call is 00), then the site series map label is used instead
5. Replace NA values in the MapUnit columns to 0's (this was necessary moving forward so that certain rows did not get removed);
6, 7, 8. Sum values in columns if the site series calls were exactly the same;
9. Replace 0's in the map unit columns with NA values
10. Make valid R names to the site series calls and remove site series with NA in the columns, and classify those site series calls as factors
11. Select the final dataset columns
12. Replace remaining NA's with 0's (only applied to numerical columns, doesn't affect factor levels)

The dsmart program requires a separate compositional data frame to be supplied. The data frame will have 5 columns in the end:
1. Polygon number
2. Map labels
3. Strata
4. Map Units
5. proportions of map units in each polygon

It's pretty much a melted data frame from the polygon data, though I've adjusted it slightly so that it's shorter to remove rows with NA values. This compositional data frame is included in the dsmart algorithm.

```{r DSMART TEM processing}

tem_sf <- st_read(file.path(shapes_path, paste0(map_res, "m"), "tem.gpkg"), quiet = TRUE) %>% 
  select_if(function(x) !all(is.na(x)))
aoi_sf <- st_read(file.path(shapes_path, paste0(map_res, "m"), "aoi.gpkg"), quiet = TRUE) %>% 
  st_geometry() %>% st_make_valid()

# TEM polygon processing as a DSMART input
dsmart_site_ser_polys <- dplyr::select(
    tem_sf, PROJECT_POLYGON_IDENTIFIER, BIOGEOCLIMATIC_LBL, 
    ECOSYSTEM_DECILE_CPNT_1, 
    ECOSYSTEM_DECILE_CPNT_2,
    ECOSYSTEM_DECILE_CPNT_3, 
    SITE_SERIES_LBL_CPNT_1, 
    SITE_SERIES_LBL_CPNT_2, 
    SITE_SERIES_LBL_CPNT_3,
    SITE_SERIES_MAP_CDE_LBL_CPNT_1, 
    SITE_SERIES_MAP_CDE_LBL_CPNT_2,
    SITE_SERIES_MAP_CDE_LBL_CPNT_3) %>% 
  dplyr::rename(
    Subzone = BIOGEOCLIMATIC_LBL, 
    MapUnit1 = SITE_SERIES_LBL_CPNT_1, 
    MapUnit2 = SITE_SERIES_LBL_CPNT_2, 
    MapUnit3 = SITE_SERIES_LBL_CPNT_3,
    Sdec1 = ECOSYSTEM_DECILE_CPNT_1, 
    Sdec2 = ECOSYSTEM_DECILE_CPNT_2, 
    Sdec3 = ECOSYSTEM_DECILE_CPNT_3, 
    NFMapUnit1 = SITE_SERIES_MAP_CDE_LBL_CPNT_1, 
    NFMapUnit2 = SITE_SERIES_MAP_CDE_LBL_CPNT_2, 
    NFMapUnit3 = SITE_SERIES_MAP_CDE_LBL_CPNT_3, 
    MAP_CODE = PROJECT_POLYGON_IDENTIFIER) %>% 
  drop_na(MapUnit1) %>% 
  mutate(
    Sdec1 = Sdec1 * 10, Sdec2 = Sdec2 * 10, Sdec3 = Sdec3 * 10, 
    MapUnit1 = ifelse(MapUnit1 == "00", paste0(Subzone, "/", as.character(NFMapUnit1)), 
                      paste0(Subzone, "/", MapUnit1)),
    MapUnit2 = ifelse(MapUnit2 == "00", paste0(Subzone, "/", as.character(NFMapUnit2)), 
                      paste0(Subzone, "/", MapUnit2)),
    MapUnit3 = ifelse(MapUnit3 == "00", paste0(Subzone, "/", as.character(NFMapUnit3)), 
                      paste0(Subzone, "/", MapUnit3)),
    POLY_NO = as.integer(sub(".*_", "", MAP_CODE))) %>% 
  replace_na(list(MapUnit1 = 0, MapUnit2 = 0, MapUnit3 = 0)) %>% 
  mutate(
    Sdec2 = ifelse(MapUnit2 == MapUnit3, Sdec2 + Sdec3, Sdec2), 
    Sdec3 = ifelse(MapUnit2 == MapUnit3, NA, Sdec3), 
    MapUnit3 = ifelse(MapUnit2 == MapUnit3, 0, MapUnit3)) %>% 
  mutate(
    Sdec1 = ifelse(MapUnit1 == MapUnit3, Sdec1 + Sdec3, Sdec1), 
    Sdec3 = ifelse(MapUnit1 == MapUnit3, NA, Sdec3), 
    MapUnit3 = ifelse(MapUnit1 == MapUnit3, 0, MapUnit3)) %>% 
  mutate(
    Sdec1 = ifelse(MapUnit1 == MapUnit2, Sdec1 + Sdec2, Sdec1), 
    Sdec2 = ifelse(MapUnit1 == MapUnit2, NA, Sdec2),
    MapUnit2 = ifelse(MapUnit2 == MapUnit1, 0, MapUnit2)) %>% 
  mutate(
    MapUnit2 = na_if(MapUnit2, "0"), 
    MapUnit3 = na_if(MapUnit3, "0")) %>% 
  mutate(
    MapUnit2 = ifelse((is.na(MapUnit2) & !is.na(MapUnit3)), MapUnit3, MapUnit2), 
    MapUnit3 = ifelse(MapUnit2 == MapUnit3, NA, MapUnit3), 
    Sdec2 = ifelse((is.na(Sdec2) & !is.na(Sdec3)), Sdec3, Sdec2),
    Sdec3 = ifelse(MapUnit2 == MapUnit3, NA, Sdec3)) %>% 
  mutate(
    MapUnit1 = as.factor(sub(".*NA\\.", NA, make.names(MapUnit1))), 
    MapUnit2 = as.factor(sub(".*NA\\.", NA, make.names(MapUnit2))), 
    MapUnit3 = as.factor(sub(".*NA\\.", NA, make.names(MapUnit3))),
    Subzone = as.factor(Subzone)) %>% 
  dplyr::select(
    POLY_NO, Subzone, MAP_CODE, MapUnit1, Sdec1, MapUnit2, Sdec2, MapUnit3, Sdec3) %>% 
  replace_na(list(Sdec1 = 0, Sdec2 = 0, Sdec3 = 0)) %>% 
  droplevels()

# Inspect geometries and remove incorrect data types:
if(!st_geometry_type(dsmart_site_ser_polys, FALSE) %in% c("POLYGON", "MULTIPOLYGON")) {
  tem_geom <- st_geometry(dsmart_site_ser_polys)
  for(i in 1:length(tem_geom)) {
    if(!st_geometry_type(tem_geom[[i]]) %in% c("POLYGON", "MULTIPOLYGON")) {
      tem_geom[[i]] <- st_collection_extract(tem_geom[[i]], "POLYGON")
    }
  }
  st_geometry(dsmart_site_ser_polys) <- st_cast(tem_geom, "MULTIPOLYGON")
}

# Convert polygon composition to table with 5 columns
dsmart_site_ser_composition <- st_drop_geometry(dsmart_site_ser_polys) %>% 
  pivot_longer(cols = c("Sdec1", "Sdec2", "Sdec3")) %>% 
  mutate(MapUnit = ifelse(name == "Sdec1", as.character(MapUnit1), 
                                    ifelse(name == "Sdec2", as.character(MapUnit2), 
                                           ifelse(name == "Sdec3", as.character(MapUnit3), NA))),
         POLY_NO = as.integer(POLY_NO), 
         proportion = as.integer(value)) %>% 
  drop_na(MapUnit) %>% 
  mutate(MapUnit = as.factor(make.names(MapUnit)), 
         Subzone = as.integer(Subzone)) %>% 
  dplyr::select(POLY_NO, MAP_CODE, Subzone, MapUnit, proportion) %>% 
  group_by(POLY_NO, MAP_CODE, Subzone, MapUnit) %>% 
  dplyr::summarise(across(proportion, sum), .groups = "drop")

# Create table of additional observations
# Subzone renaming occurs at this step as I suspect a certain amount of human error is
# incurred in some of the datasets, so subzones change to match subzones in the TEM
dsmart_site_ser_observations <- lapply(list(sechelt_bec, tem_xls_bec, tem_mdb_bec), function(x) 
  dplyr::select(x, MapUnit, subzone) %>% 
    sf::st_set_agr("constant") %>% 
    sf::st_transform(crs = st_crs(tem_sf)) %>% 
    sf::st_intersection(aoi_sf) %>% 
    droplevels()) %>% 
  do.call(what = rbind, args = .) %>% 
  sf::st_join(dsmart_site_ser_polys) %>% 
  dplyr::mutate(MapUnit = as.character(MapUnit), Subzone = as.character(Subzone)) %>% 
  dplyr::mutate(MapUnit = ifelse((subzone != Subzone & !is.na(Subzone)), 
                                 str_replace(MapUnit, ".*\\.", paste0(Subzone, ".")), 
                                 MapUnit)) %>%
  cbind(st_coordinates(.)) %>% 
  sf::st_drop_geometry() %>% 
  dplyr::select(X, Y, MapUnit) %>% 
  dplyr::mutate(MapUnit = as.factor(make.names(MapUnit)))

# From the TEM data, create a subzone layer which will be used to stratify sampling
dsmart_strata <- group_by(dsmart_site_ser_polys, Subzone) %>% 
  dplyr::summarise(.groups = "drop_last") %>% 
  mutate(strata = as.integer(Subzone)) %>% 
  st_cast("MULTIPOLYGON")

# Write DSMART outputs
dso <- file.path(paste0(dsmart_dir, "_site_ser"), "inputs")
dir.create(dso, recursive = TRUE, showWarnings = FALSE)

sapply(c("dsmart_site_ser_polys", "dsmart_strata"), function(x) st_write(
  eval(parse(text = x)), file.path(dso, paste0(x, ".gpkg")), 
  delete_layer = TRUE, quiet = TRUE))[0]

sapply(c("dsmart_site_ser_composition", "dsmart_site_ser_observations"), function(x) write.csv(
  eval(parse(text = x)), file.path(dso, paste0(x, ".csv")), row.names = FALSE))[0]

# Write output as processed points in separate folder
dsmart_site_ser_observations %>% 
  st_as_sf(coords = c("X", "Y")) %>% 
  st_set_crs(st_crs(tem_sf)) %>% 
  st_write(file.path(field_data_dir, "processed", "site_ser_pts.gpkg"), quiet = TRUE, 
           delete_layer = TRUE)

```

With the essential data configured for the dsmart run, some extra data can be added as "observations". These will be point data of Map Unit calls, which is compiled from the TEM excel spreadsheet, the TEM mdb, and the 2019 sampling data. Depending on how the data is organized, various processing needed to occur for each dataset.

For the TEM excel spreadsheet, columns for each site series are selected, and then rows are filtered to include only pure calls from the first site series call. Certain column values are adjusted to remove NA values, or to include only the relevant/cosistent site series numbers and subzone names. The MapUnit column is then built as a concatenation of zone, subzone, variant, and site series, unless the site series call is made up purely of character values and not numbers, in which case it gives it the character values (this would be for non-forested site units so that they are named "RO" instead of "CWHdm1/RO".)

The mdb only had pure call values, so the site series column and site series symbols columns were selected. A similar approach is ensued where non forested site units are standalone site units while forested ones are prefixed with the subzone name. 

The three data sources (TEM excel spreadsheet, TEM mdb, and 2019 point data) are finally bound together, but only data that intersects with the original TEM map is included. We now have our 3 datasets needed to properly run the dsmart algorithm: TEM polygons with class proportion information; compositions of the TEM polygons in a separate data frame; and extra observations that will help with the prediction of each map.

```{r Pres/Abs and Cover Merge}

pto <- file.path(field_data_dir, "processed")
dir.create(pto, recursive = TRUE, showWarnings = FALSE)

# Create the raster stack (terra package) which will be used for data extraction
covars <- terra::rast(list.files(file.path(covariate_dir, paste0(map_res, "m")), 
                                   full.names = TRUE))

# Combine them all
pres_abs <- sapply(poi, function(x) {
  
  # Create unattributed points and write those
  out <- dplyr::bind_rows(sechelt_pres_cov[[x]], tem_xls_pres[[x]], tem_mdb_pres_cov[[x]]) %>% 
    dplyr::select(point_id, Pres) %>% 
    st_set_agr("constant") %>% 
    st_transform(crs = st_crs(aoi_sf)) %>% 
    st_intersection(aoi_sf)
  st_write(out, file.path(pto, "pres_abs_no_atts.gpkg"), layer = x, 
           delete_layer = TRUE, quiet = TRUE)
  
  # Attribute the points and write those separately
  out_att <- cbind(out, terra::extract(covars, st_coordinates(out))) %>% 
    Filter(function(y) !all(is.na(y)), .) %>% 
    dplyr::select(Pres, names(covars)) %>% 
    drop_na()
  st_write(out_att, file.path(pto, "pres_abs.gpkg"), layer = x, 
           delete_layer = TRUE, quiet = TRUE)
  return(out_att)
}, simplify = FALSE, USE.NAMES = TRUE)

cover <- sapply(poi, function(x) {
  out <- dplyr::bind_rows(sechelt_pres_cov[[x]], tem_mdb_pres_cov[[x]]) %>% 
    dplyr::select(point_id, Cover) %>% 
    st_set_agr("constant") %>% 
    st_transform(crs = st_crs(aoi_sf)) %>% 
    st_intersection(aoi_sf) 
  st_write(out, file.path(pto, "cover_no_atts.gpkg"), layer = x, 
           delete_layer = TRUE, quiet = TRUE)
  
  out_att <- cbind(out, terra::extract(covars, st_coordinates(out))) %>% 
    Filter(function(y) !all(is.na(y)), .) %>% 
    dplyr::select(Cover, names(covars)) %>% 
    drop_na()
  st_write(out_att, file.path(pto, "cover.gpkg"), layer = x, 
           delete_layer = TRUE, quiet = TRUE)
  return(out_att)
}, simplify = FALSE, USE.NAMES = TRUE)

```

Using the land cover classification manual, create decisions for attributing each VRI polygon with a structural stage. This is complex. 

The land cover classifications use the BCLCS codes in 5 different layers to detail what a polygon might look like. Additionally, there are 3 "LAND_COVER_CLASS_CD" columns which detail how much of a given polygon is of a particular land classification; however, this is largely incomplete. Where it is, the best guess from the BCLCS layers will be used to determine the land class. Because there are three different calls sometimes, we can plug the final result into DSMART to spit out a raster of structural stage.

***Currently, I'm not sure how to implement water bodies. I'm currently putting all water classified polygons (freshwater and marine) into the 2c classification, but that may change.

```{r Structural stage prep}

# Read in VRI data
vri <- st_read(file.path(shapes_path, paste0(map_res, "m"), "vri_full.gpkg"), quiet = TRUE)
vri_bgc <- dplyr::select(vri, BEC_ZONE_CODE, BEC_SUBZONE, BEC_VARIANT) %>% 
  dplyr::mutate(bgc = ifelse(!is.na(BEC_VARIANT), 
                             paste0(BEC_ZONE_CODE, BEC_SUBZONE, BEC_VARIANT), 
                             paste0(BEC_ZONE_CODE, BEC_SUBZONE))) %>% 
  group_by(bgc) %>% dplyr::summarise(.groups = "drop")

st_write(vri_bgc, file.path(shapes_path, paste0(map_res, "m"), "bgc_vri.gpkg"), 
         delete_layer = TRUE, quiet = TRUE)

# Use VRI to interpret structural stage
# First splits: Vegetated and non-vegetated, then trees vs. no trees in vegetated table
vri_veg_only <- dplyr::select(vri, starts_with(c(
  "POLYGON_ID", "MAP_ID", "BCLCS", "LAND_COVER_CLASS", "EST_COVERAGE_PCT", 
  "SPECIES_PCT", "PROJ_AGE", "BEC"))) %>% 
  dplyr::filter(BCLCS_LEVEL_1 == "V")
vri_no_veg <- dplyr::select(vri, starts_with(c(
  "POLYGON_ID", "MAP_ID", "BCLCS", "LAND_COVER_CLASS", "EST_COVERAGE_PCT", 
  "SPECIES_PCT", "PROJ_AGE", "BEC"))) %>% 
  dplyr::filter(BCLCS_LEVEL_1 != "V")
vri_trees <- dplyr::filter(vri_veg_only, BCLCS_LEVEL_2 == "T")
vri_no_trees <- dplyr::filter(vri_veg_only, BCLCS_LEVEL_2 != "T")

# Focus on ss 1 - 3
# In the Sechelt dataset, I treat ES (exposed soil) as 1a, 
# BM (bryoid moss) gets 1b, and BL (bryoid lichens) gets 1c
ss_1a <- dplyr::filter(vri_no_veg, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "EL", 
  LAND_COVER_CLASS_CD_1 == "ES")) %>% 
  dplyr::mutate(ss1 = "1a", ss2 = NA, ss3 = NA)
ss_1b <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "BM",
  LAND_COVER_CLASS_CD_1 == "BM")) %>% 
  dplyr::mutate(ss1 = "1b", ss2 = NA, ss3 = NA)
ss_1c <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "BL",
  LAND_COVER_CLASS_CD_1 == "BL")) %>% 
  dplyr::mutate(ss1 = "1c", ss2 = NA, ss3 = NA)

# Additional entries classified as BY get randomly assigned to either 1b or 1c:
ss_1 <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "BY",
  LAND_COVER_CLASS_CD_1 == "BY")) %>% 
  dplyr::mutate(LAND_COVER_CLASS_CD_1 = ifelse(round(runif(nrow(.))) == 1, "BM", "BL")) %>% 
  dplyr::mutate(ss1 = ifelse(LAND_COVER_CLASS_CD_1 == "BM", "1b", "1c"), 
                ss2 = NA, ss3 = NA)
ss_1b <- if(any(ss_1$LAND_COVER_CLASS_CD_1 == "BM")) {
  rbind(ss_1b, ss_1[ss_2$LAND_COVER_CLASS_CD_1 == "BM", ])
} else ss_1b
ss_1c <- if(any(ss_1$LAND_COVER_CLASS_CD_1 == "BL")) {
  rbind(ss_1c, ss_1[ss_2$LAND_COVER_CLASS_CD_1 == "BL", ])
} else ss_1c

# The herb dominated polygons will be assigned structural stage 2
# Forb dominants (HF) get 2a, graminoid dominants (HG) get 2b
# All polygons that are designated lakes or ocean get assigned 2c, though this may change
ss_2a <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "HF", 
  LAND_COVER_CLASS_CD_1 == "HF")) %>% 
  dplyr::mutate(ss1 = "2a", ss2 = NA, ss3 = NA)
ss_2b <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "HG", 
  LAND_COVER_CLASS_CD_1 == "HG")) %>% 
  dplyr::mutate(ss1 = "2b", ss2 = NA, ss3 = NA)
ss_2c <- rbind(
  dplyr::filter(vri_no_veg, ifelse(
    is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_5 %in% c("LA", "RE", "RI", "OC"), 
    LAND_COVER_CLASS_CD_1 %in% c("LA", "RE", "RI", "OC"))),
  dplyr::filter(vri_no_trees, 
    LAND_COVER_CLASS_CD_1 %in% c("LA", "RE", "RI", "OC"))) %>% 
  dplyr::mutate(ss1 = "2c", ss2 = NA, ss3 = NA)

# No foreseeable way to distinguish structural stage 2d from VRI data
# Additional entries that may go into 2a, 2b, or 2d, or be randomly assigned to
# 2a or 2b:
ss_2 <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "HE",
  LAND_COVER_CLASS_CD_1 == "HE")) %>% 
  dplyr::mutate(LAND_COVER_CLASS_CD_1 = ifelse(round(runif(nrow(.))) == 1, "HF", "HG")) %>% 
  dplyr::mutate(ss1 = ifelse(LAND_COVER_CLASS_CD_1 == "HF", "2a", "2b"), 
                ss2 = NA, ss3 = NA)
ss_2a <- if(any(ss_2$LAND_COVER_CLASS_CD_1 == "HF")) {
  rbind(ss_2a, ss_2[ss_2$LAND_COVER_CLASS_CD_1 == "HF", ])
} else ss_2a
ss_2b <- if(any(ss_2$LAND_COVER_CLASS_CD_1 == "HG")) {
  rbind(ss_2b, ss_2[ss_2$LAND_COVER_CLASS_CD_1 == "HG", ])
} else ss_2b

# SL stands for shrub low, ST shrub tall. Those easily get assigned to 3a and 3b,
# respectively
ss_3a <- rbind(
  dplyr::filter(vri_no_trees, ifelse(
    is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "SL", 
    LAND_COVER_CLASS_CD_1 == "SL")),
  dplyr::filter(vri_no_veg, ifelse(
    is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "SL", 
    LAND_COVER_CLASS_CD_1 == "SL"
  ))) %>% 
  dplyr::mutate(
    ss1 = "3a", 
    ss2 = ifelse(LAND_COVER_CLASS_CD_2 == "HE", ifelse(round(runif(nrow(.))) == 1, "2a", "2b"), 
                 ifelse(LAND_COVER_CLASS_CD_2 == "TC", "5", LAND_COVER_CLASS_CD_2)),
    ss3 = ifelse(LAND_COVER_CLASS_CD_3 == "LA", "2c", LAND_COVER_CLASS_CD_3))
ss_3b <- rbind(
  dplyr::filter(vri_no_trees, ifelse(
    is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "ST", 
    LAND_COVER_CLASS_CD_1 == "ST")),
  dplyr::filter(vri_no_veg, ifelse(
    is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "ST", 
    LAND_COVER_CLASS_CD_1 == "ST"
  ))) %>% 
  dplyr::mutate(ss1 = "3b", ss2 = NA, ss3 = NA)

### Moving on to treed polygons
# First, create columns of the weighted average stand age of the two most
# dominant tree species
vri_avg <- dplyr::mutate(vri_trees,
  age_avg = ifelse(
    !is.na(PROJ_AGE_2), 
    ((SPECIES_PCT_1 * PROJ_AGE_1) + (SPECIES_PCT_2 * PROJ_AGE_2)) / (SPECIES_PCT_1 + SPECIES_PCT_2), 
    PROJ_AGE_1))

# Basing all decisions on average stand age...might not be overly accurate, but
# closest approximation given the data currently at hand. Structural stage 4
# gets < 20 years, 5 gets 20-80 years, 6 gets 80-250 years (based on NDT from
# subzone), and 7 gets greater than 250 years. I cannot find an accurate way to 
# decipher between structural stages 7a and 7b from VRI data, so they are lumped 
# together here.
ss_4 <- dplyr::filter(vri_avg, age_avg < 20) %>% 
  dplyr::mutate(ss1 = "4", ss2 = NA, ss3 = NA)
ss_5 <- dplyr::filter(vri_avg, age_avg >= 20, age_avg < 80) %>% 
  dplyr::mutate(
    ss1 = "5", 
    ss2 = ifelse(LAND_COVER_CLASS_CD_2 == "SL", "3a", LAND_COVER_CLASS_CD_2),
    ss3 = ifelse(LAND_COVER_CLASS_CD_3 == "BR", "1a", LAND_COVER_CLASS_CD_3)
  )
ss_6 <- dplyr::filter(vri_avg, age_avg >= 80, age_avg < 250) %>% 
  dplyr::mutate(ss1 = "6", ss2 = NA, ss3 = NA)
ss_7 <- dplyr::filter(vri_avg, age_avg >= 250) %>% 
  dplyr::mutate(
    ss1 = "7", 
    ss2 = ifelse(LAND_COVER_CLASS_CD_2 == "BR", "1a", LAND_COVER_CLASS_CD_2),
    ss3 = NA
  )

# Combine each of the structural stages together and create columns used in 
# DSMART 
dsmart_struct_stg_polys <- rbind(ss_1a, ss_1b, ss_1c, ss_2a, ss_2b, ss_2c, ss_3a, ss_3b, 
                rbind(ss_4, ss_5, ss_6, ss_7) %>% dplyr::select(names(ss_1a))) %>% 
  dplyr::mutate(EST_COVERAGE_PCT_1 = ifelse(is.na(EST_COVERAGE_PCT_1), 100, EST_COVERAGE_PCT_1), 
                EST_COVERAGE_PCT_2 = ifelse(is.na(EST_COVERAGE_PCT_2), 0, EST_COVERAGE_PCT_2), 
                EST_COVERAGE_PCT_3 = ifelse(is.na(EST_COVERAGE_PCT_3), 0, EST_COVERAGE_PCT_3), 
                MAP_CODE = paste0(MAP_ID, "_", POLYGON_ID), 
                ss1 = as.factor(make.names(ss1)), 
                ss2 = as.factor(sub(".*NA\\.", NA, make.names(ss2))), 
                ss3 = as.factor(sub(".*NA\\.", NA, make.names(ss3)))) %>% 
  unite(Subzone, c(BEC_ZONE_CODE, BEC_SUBZONE, BEC_VARIANT), sep = "", remove = TRUE, na.rm = TRUE) %>% 
  dplyr::select(POLYGON_ID, Subzone, MAP_CODE, ss1, EST_COVERAGE_PCT_1, ss2, EST_COVERAGE_PCT_2, 
                ss3, EST_COVERAGE_PCT_3) %>% 
  mutate(across(Subzone,as.factor))

# Inspect geometries and remove incorrect data types:
if(!st_geometry_type(dsmart_struct_stg_polys, FALSE) %in% c("POLYGON", "MULTIPOLYGON")) {
  ss_geom <- st_geometry(dsmart_struct_stg_polys)
  for(i in 1:length(ss_geom)) {
    if(!st_geometry_type(ss_geom[[i]]) %in% c("POLYGON", "MULTIPOLYGON")) {
      ss_geom[[i]] <- st_collection_extract(ss_geom[[i]], "POLYGON")
    }
  }
  st_geometry(dsmart_struct_stg_polys) <- st_cast(ss_geom, "MULTIPOLYGON")
}

# Convert polygon composition to table with 5 columns
dsmart_struct_stg_composition <- st_drop_geometry(dsmart_struct_stg_polys) %>% 
  pivot_longer(cols = c("EST_COVERAGE_PCT_1", "EST_COVERAGE_PCT_2", "EST_COVERAGE_PCT_3")) %>% 
  mutate(ss = ifelse(name == "EST_COVERAGE_PCT_1", as.character(ss1), 
                     ifelse(name == "EST_COVERAGE_PCT_2", as.character(ss2), 
                            ifelse(name == "EST_COVERAGE_PCT_3", as.character(ss3), NA))),
         POLYGON_ID = as.integer(POLYGON_ID), 
         proportion = as.integer(value)) %>% 
  drop_na(ss) %>% 
  mutate(ss = as.factor(ss), 
         Subzone = as.integer(Subzone)) %>% 
  dplyr::select(POLYGON_ID, MAP_CODE, Subzone, ss, proportion) %>% 
  group_by(POLYGON_ID, MAP_CODE, Subzone, ss) %>% 
  dplyr::summarise(across(proportion, sum), .groups = "drop")

# Create table of additional observations
# Fix incorrectly attributed structural stage calls
tem_mdb_struct_stg <- dplyr::select(tem_mdb_bec, StructuralStage, SiteNotes) %>% 
  dplyr::filter(StructuralStage != "") %>% 
  dplyr::mutate(StructuralStage = ifelse(
    StructuralStage == "3b/c", "3b", ifelse(
      StructuralStage == 2, "2a", ifelse(
        StructuralStage == 3 & grepl(
          "logged in 1997|open grown with lots of salal|in lower portion of cutblock", 
          .$SiteNotes), "3a", ifelse(
            StructuralStage == 3 & grepl(
              "young second growth|old cut block|Fd plantation", .$SiteNotes), 
            "3b", StructuralStage)))))

# Put all observations together in a single table
dsmart_struct_stg_observations <- dplyr::select(sechelt_bec_full, `Structural Stage`) %>% 
  dplyr::rename(StructuralStage = `Structural Stage`) %>% 
  dplyr::mutate(StructuralStage = ifelse(StructuralStage %in% c(2, 3), "3a", StructuralStage)) %>% 
  bind_rows(dplyr::select(tem_mdb_struct_stg, StructuralStage),
            dplyr::select(tem_xls_bec, StructuralStage)) %>% 
  drop_na() %>% 
  mutate(StructuralStage = trimws(StructuralStage)) %>% 
  st_set_agr("constant") %>% 
  st_transform(crs = st_crs(aoi_sf)) %>% 
  st_intersection(aoi_sf) %>% 
  cbind(st_coordinates(.)) %>% 
  st_drop_geometry() %>% 
  dplyr::select(X, Y, StructuralStage) %>% 
  dplyr::mutate(StructuralStage = as.factor(make.names(StructuralStage)))

# Create extracted sf dataframe of observations
struct_stg <- dsmart_struct_stg_observations %>% 
  sf::st_as_sf(coords = c("X", "Y")) %>% 
  sf::st_set_crs(sf::st_crs(dsmart_struct_stg_polys)) %>% 
  cbind(terra::extract(covars, st_coordinates(.))) %>% 
  Filter(function(y) !all(is.na(y)), .) %>% 
  dplyr::select(StructuralStage, names(covars)) %>% 
  drop_na()

# Write data outputs
dso <- file.path(paste0(dsmart_dir, "_struct_stg"), "inputs")
dir.create(dso, recursive = TRUE, showWarnings = FALSE)

st_write(dsmart_struct_stg_polys, file.path(dso, paste0("dsmart_struct_stg_polys.gpkg")), 
  delete_layer = TRUE, quiet = TRUE)
sapply(c("dsmart_struct_stg_composition", "dsmart_struct_stg_observations"), function(x) write.csv(
  eval(parse(text = x)), file.path(dso, paste0(x, ".csv")), row.names = FALSE))[0]

st_write(struct_stg, file.path(pto, "struct_stg_pts.gpkg"), 
         delete_layer = TRUE, quiet = TRUE)

```

After an initial DSMART run on the structural stage, I was not very satisfied with the results. I believe that there were too few samples from certain categories, so I will instead lump groups together (e.g.: 3a and 3b will simply become 3)

```{r Simple structural stage}

dsmart_struct_stg_polys <- dsmart_struct_stg_polys %>% 
  dplyr::mutate(across(starts_with("ss"), as.character)) %>% 
  dplyr::mutate(across(starts_with("ss"), function(x) {
    ifelse(
    x %in% c("X1a", "X1b", "X1c"), "X1", ifelse(
      x %in% c("X2a", "X2b", "X2c", "X2d"), "X2", ifelse(
        x %in% c("X3a", "X3b"), "X3", x)))}))

dsmart_struct_stg_composition <- dsmart_struct_stg_composition %>% 
  dplyr::mutate(ss = as.character(ss)) %>% 
  dplyr::mutate(ss = ifelse(
    ss %in% c("X1a", "X1b", "X1c"), "X1", ifelse(
      ss %in% c("X2a", "X2b", "X2c", "X2d"), "X2", ifelse(
        ss %in% c("X3a", "X3b"), "X3", ss)))) %>% 
  dplyr::mutate(ss = as.factor(ss))

dsmart_struct_stg_observations <- dsmart_struct_stg_observations %>% 
  dplyr::mutate(StructuralStage = as.character(StructuralStage)) %>% 
  dplyr::mutate(StructuralStage = ifelse(
    StructuralStage %in% c("X1a", "X1b", "X1c"), "X1", ifelse(
      StructuralStage %in% c("X2a", "X2b", "X2c", "X2d"), "X2", ifelse(
        StructuralStage %in% c("X3a", "X3b"), "X3", StructuralStage)))) %>% 
  dplyr::mutate(StructuralStage = as.factor(StructuralStage))

# Write data outputs
dso <- file.path(paste0(dsmart_dir, "_struct_stg_simple"), "inputs")
dir.create(dso, recursive = TRUE, showWarnings = FALSE)

st_write(dsmart_struct_stg_polys, file.path(dso, paste0("dsmart_struct_stg_simple_polys.gpkg")), 
  delete_layer = TRUE, quiet = TRUE)
sapply(c("dsmart_struct_stg_simple_composition", "dsmart_struct_stg_simple_observations"), function(x) write.csv(
  eval(parse(text = x)), file.path(dso, paste0(x, ".csv")), row.names = FALSE))[0]

```

