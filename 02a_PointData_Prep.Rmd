---
title: "02a_PointData_Prep"
author: "Matthew Coghill"
date: "2/6/2020"
output: html_document
---

This document performs various tasks, including:
1. Import/download of useful datasets;
2. Combining those datasets to produce master datasets for predicting:
  + A site series raster
  + Predictive maps for cultural plants of interest (pres/abs and cover)

A lot of the work stems from previously ran scripts which have:
1. Created a 4m resolution DEM;
2. Produced DEM derivatives;
3. Downloaded and produced satellite layers and indices;
4. Downloaded BCData layers

Using the RMarkdown format, I'll walk through the scripting process used to create the final maps. First, we must load/install the required packages.

```{r Load Packages, echo=TRUE, results='hide'}

invisible(suppressPackageStartupMessages(
  lapply(c("tidyverse", "readxl", "svSocket", "RODBC", "sf", "terra"), 
         library, character.only = TRUE, quietly = TRUE)))

```

Next, we should set some directories for this work. A lot of the scripting used here is adapted from PEM mapping, so the file folder structure used will be much the same. The idea is that if we want to apply a similar schematic to another study area, we should only have to adjust the directories for that given study area.

Additionally, we need to set the directory for where the 32-bit version of R is installed on your machine. This will be used later on in MS Access Database processing. (will probably move function to separate script file)

```{r Set directories}

AOI <- "Sechelt"
AOI_dir <- file.path(".", paste0(AOI, "_AOI"))
map_res <- 10
res_dir <- ifelse(is.numeric(map_res), paste0(map_res, "m"), map_res)
poi <- c("RUBUSPE", "CORNCAN", "VERAVIR", "LEDUGRO") # plants of interest

shapes_dir <- file.path(AOI_dir, "0_raw_inputs", "base_layers", res_dir)
field_data_dir <- file.path(AOI_dir, "1_map_inputs", "field_data")
covariate_dir <- file.path(AOI_dir, "1_map_inputs", "covariates", res_dir)
dsmart_dir <- file.path(AOI_dir, "1_map_inputs", "dsmart_2019", res_dir)

```

The next thing that is typical is to place any repeated functions at the top so that they may be called in a given parameter below. access_query_32 will open a separate 32-bit version of R in order to read in the tables of a Microsoft Access database, while the `droplevels()` function adds the `sf` class to be able to drop factor levels when necessary without throwing an error.

```{r Functions}

access_query_32 <- function(db_dir = NULL) {
  # Variables to make values uniform
  # Shouldn't need to change socket port number, but if so just change variable
  # here and the rest should run fine.
  sock_port <- 8642L
  
  if(file.exists(db_dir)) {
    # Start socket server to transfer data to 32 bit session
    svSocket::startSocketServer(port = sock_port, 
                                server.name = "access_query_32", local = TRUE)
    
    # Build expression to pass to 32 bit R session. The bquote() function allows
    # for variables to be passed as strings if preceded by a period
    expr <- bquote({
      ls <- c("svSocket", "RODBC", "tidyverse")
      new.packages <- ls[!(ls %in% installed.packages()[, "Package"])]
      if(length(new.packages)) 
        install.packages(new.packages, repos = "https://cloud.r-project.org")
      invisible(suppressPackageStartupMessages(
        lapply(ls, library, character.only = TRUE)))
      a32_con <- RODBC::odbcConnectAccess(.(db_dir))
      tbl_list <- RODBC::sqlTables(a32_con) %>% 
        dplyr::filter(TABLE_TYPE != "SYSTEM TABLE")
      for(j in tbl_list$TABLE_NAME) {
        tryCatch({
          do.call("<-", list(paste0("MDB_", j), RODBC::sqlFetch(a32_con, j)))
        }, error = function(c) {
          message("Could not process table from MDB")
          print(c)
        })
      }
      RODBC::odbcCloseAll()
      mdb_out <- objects(pattern = "MDB_")
      tbl_out <- list()
      for(k in 1:length(mdb_out)) {
        tryCatch({
          message(paste("Processing table", mdb_out[k]))
          m <- mdb_out[k]
          tbl_out[[m]] <- get(mdb_out[k])
        }, error = function(c) {
          message("Could not push table to environment")
          print(c)
        })
      }
      sv_con <- socketConnection(port = .(sock_port))
      svSocket::evalServer(sv_con, tbl_out, tbl_out)
    })
    expr <- paste(paste(expr)[-1], collapse = ";")
    
    # Launch 32 bit R session and run expressions
    prog <- file.path(R.home(), "bin/i386/Rscript.exe")
    system2(prog, args = c("-e", shQuote(expr)), stdout = NULL, wait = TRUE, 
            timeout = 30, invisible = TRUE)
    
    # Stop socket server
    svSocket::stopSocketServer(port = sock_port)
  } else {
    warning("database not found: ", db_dir)
  }
  return(tbl_out)
}

droplevels.sf = function(x, except, exclude, ...) x
droplevels.sfc = function(x, except, exclude, ...) x

```

In the next chunk, we are processing the 2019 field data excel file. The original file required some manual alterations, thus an edited version was created. Rather than having separate sheets for each plant species, there are now a total of 2 sheets in the excel spreadsheet:

1. Site information for each visited site;
2. Plant presence/absence and cover data for each species at each site

First, the site information page is read in and processed to become an sf dataframe.

```{r Field data 2019 - Site}

spreadsheet <- file.path(
  field_data_dir, "raw", 
  "shishalhBECandCulturalPlants_FieldData_2019_EDITS.xlsx")
sheets <- excel_sheets(spreadsheet)

# Load BEC information
sechelt_bec <- read_excel(spreadsheet, sheet = "sechelt_site") %>% 
  dplyr::rename(subzone = `BGC Subzone / Variant`, point_id = `Point ID`) %>% 
  stats::setNames(make.names(names(.))) %>% 
  dplyr::mutate(MapUnit = ifelse(is.na(Forested.Site.Series), Nonforested.Class,
                                 paste0(subzone, ".", Forested.Site.Series))) %>% 
  st_as_sf(coords = c("Long", "Lat"), crs = 4326, remove = FALSE) %>% 
  st_transform(3005)

```

The plant datasheet is related to the site datasheet through the point ID columns. The plant datasheet is read in and an sf dataframe is created for each plant through a left join. The resulting sechelt_pres_cov object is a list object where each entry is a separate plant species. Entries are filtered out if at that point, there was no data recorded on that particular plant (marked in the Excel spreadsheet in the comments column as "*not recorded").

```{r Field Data 2019 - Plants}

# The species tables don't have coordinates associated with them, but they can 
# be merged with the BEC table which has that data

sechelt_plants <- read_excel(spreadsheet, sheet = "sechelt_plants") %>% 
  dplyr::mutate(
    Cover = rowSums(.[, c("A1", "A2", "A3", "B1", "B2", "C", "D")], na.rm = TRUE),
    Species = ifelse(nchar(Species) > 2, toupper(Species), Species)) %>% 
  dplyr::mutate(Pres = Cover > 0) %>% 
  dplyr::filter(Comments != "*not recorded" | is.na(Comments)) %>% 
  dplyr::rename(point_id = `Point ID`) %>% 
  left_join(sechelt_bec, by = "point_id", suffix = c("", ".y")) %>% 
  dplyr::select(point_id, Species, MapUnit, Lat, Long, A1, A2, A3, B1, B2, C, D, 
                Flowers, Fruit, Pres, Cover, Comments) %>% 
  st_as_sf(coords = c("Long", "Lat"), crs = 4326) %>% 
  st_transform(3005)

# Create separate sf dataframes for each species in the spreadsheet
sechelt_pres_cov <- sapply(unique(sechelt_plants$Species), function(x) 
  dplyr::filter(sechelt_plants, Species == x) %>% 
    dplyr::select(-Species), simplify = FALSE, USE.NAMES = TRUE)

```

In 2019, a total of 84 sites were sampled in the Sechelt study area. Unfortunately, that doesn't exactly provide us with a lot of data; however, there are some external bits of data that we can call upon. The TEM project that went on for the Sechelt area is available freely on EcoCat, and there are two important datasets to garnish from there:

1. An excel file containing point data of site series calls and notes with plant names;
2. A Microsoft Access Database file with site series calls as well as all of the plants recorded at each site and their respective cover data

Opening the excel file is no problem; columns were added to that dataset to show presence/absence data of the culturally important plants.

Opening the Microsoft Access Database (mdb) requires a much more intricate process. mdb files are created using 32-bit versions of Microsoft Office and thus can only be opened in R when ran in a 32-bit session. At this point, one might give up on opening the database altogether, but fear not! You can open a separate R session within your current R session in order to process various functions. In this case, we want to be able to process the mdb file from the TEM project in a separate 32-bit R session and then pass the data on to our current R session (that's the access_query_32 function below). Note an issue that may arise when trying to run this function: It might fail because the packages required are not installed. SOLUTION: unscripted (yet), but go to the directory of your 32-bit R session, open R_gui.exe, and install the missing packages that way.

When the mdb is processed and passed on to your current R session, it passes on all of the tables that it found within the database. In this case there were a lot of useless data (leftover queries that have no meaning to this project, tables with 0 records, small vector data), and these are immediately removed. From that leftover list of tables, we only actually need three of them: Site data, vegetation layer data, and species data. All of these tables are linked together by common fields in their tables.

For reference: https://a100.gov.bc.ca/pub/acat/public/viewReport.do?reportId=35895

```{r Get external datasets}

# Download TEM Spreadsheet
download.file(
  "http://a100.gov.bc.ca/pub/acat/documents/r35895/tem_4678_eci_1363698000276_6de51d6c988801e3369406fe9a22713aa4dca27f63de77d54bf44b70a246813b.xls", 
  destfile = file.path(field_data_dir, "raw", "tem_data.xls"), 
  mode = "wb", quiet = TRUE)

# Download TEM .mdb database. 
# This has issues when trying to code it in, and you'll likely get a successful
# download but the file will be empty. This has to do with the link being an 
# http link rather than an https link (I think?), and can carry potential
# security issues due to that. This prevents a full download of the file and I
# have tried multiple packages to try to get around this to no avail. The best
# way to do this is to copy the link below, paste it into your own web browser,
# and download the file accepting the security risks through clicking the
# appropriate buttons. When the download starts, make sure you save the .mdb
# file into the same folder as the TEM spreadsheet above. Additionally, cloning
# the repository for this project should contain the proper files.
# https://a100.gov.bc.ca/pub/acat/public/viewReport.do?reportId=35895
# download.file(
#   "http://a100.gov.bc.ca/appsdata/acat/documents/r35895/tem_4678_eci_1363697969131_6de51d6c988801e3369406fe9a22713aa4dca27f63de77d54bf44b70a246813b.mdb", 
#   destfile = file.path(field_data_dir, "raw", "tem_data.mdb"), mode = "wb")

# Using RSelenium is tricky but it may work:
# library(RSelenium)
# eCaps <- list(
#   chromeOptions = 
#     list(prefs = list(
#       "profile.default_content_settings.popups" = 0L,
#       "download.prompt_for_download" = FALSE,
#       "download.default_directory" = "C:/Users/matth/Downloads"
#     )
#     )
# )
# rsel <- rsDriver(chromever = "84.0.4147.30", extraCapabilities = eCaps)
# remDr <- rsel[["client"]]
# remDr$navigate("https://a100.gov.bc.ca/pub/acat/public/viewReport.do?reportId=35895")
# webElem <- remDr$findElement(using = "xpath", "//a[@href='http://a100.gov.bc.ca/appsdata/acat/documents/r35895/tem_4678_eci_1363697969131_6de51d6c988801e3369406fe9a22713aa4dca27f63de77d54bf44b70a246813b.mdb']")
# webElem$clickElement()
# remDr$close()
# rsel[["server"]]$stop()

# Process the TEM spreadsheet by finding matching string in the comments portion
# of the spreadsheet
tem_xls <- sapply(poi, function(x) {
  read_xls(file.path(field_data_dir, "raw", "tem_data.xls"), skip = 1) %>% 
    dplyr::mutate(Pres = grepl(x, Comments)) %>% 
    dplyr::mutate(subzone = paste0(`BGC Zone`, tolower(Subzone), 
                                   ifelse(is.na(Variant), "", Variant))) %>% 
    as.data.frame() %>% 
    st_as_sf(coords = c("UTM_East", "UTM_North"), crs = 26910) %>% 
    st_transform(3005)
}, simplify = FALSE, USE.NAMES = TRUE)

# Process the .mdb file in a 32 bit R session, function passes variables 
# to this R session in a single list entry
tem_mdb <- access_query_32(
  db_dir = file.path(field_data_dir, "raw", "tem_data.mdb")) %>% 
  lapply(function(x) {
  if(class(x) == "data.frame") {
    if(nrow(x) == 0) {
      x <- NULL
    } else x
  } else x <- NULL
}) %>% compact() %>% 
  .[grep("PlotData|VegLayer|VegSpecies", names(.))] %>% 
  lapply(function(x) Filter(function(y) !all(is.na(y)), x))

```

After downloading the TEM datasets, we need to prepare them. The next chunk does some data wrangling of the TEM site data get more directed outputs.

```{r TEM data - Site}

# In this version of the TEM, there is no class for "nonforested" class; instead,
# it is non-vegetated class and then other site series marked as 00 were new
# additions that were not classified in the red handbook. Here, I'll build the
# table of non-vegetated classes
non_veg <- expand.grid(
  site_series = "00", 
  map_code = c("GC", "GP", "LA", "OW", "PD", "RO", "RZ", "TA", "UR"))

# Other 00 site series are considered forested here on.

# TEM excel spreadsheet - BEC info (just uses one of the species since all
# species have same data). This data will be used in generating a site series
# raster. so the "best" calls should be used. Initially, this was pure calls
# only, however it might be better to do majority calls instead
tem_xls_bec <- dplyr::select(
  tem_xls[[1]], Poly_Nbr, ECI_Tag, subzone,
  Sser1, Sser2, Sser3, SDec1, Sdec2, Sdec3, `Str. Stg.`) %>% 
  dplyr::rename(StructuralStage = `Str. Stg.`) %>% 
  # dplyr::filter(ifelse(is.na(Sdec3), SDec1 >= 6, SDec1 >= 5)) %>% 
  drop_na(SDec1) %>% 
  dplyr::mutate(Sser1 = substr(Sser1, start = 1, stop = 2)) %>% 
  dplyr::mutate(across(c(Sser1, Sser2, Sser3), ~ sub("MA", "SW", .x))) %>% 
  dplyr::mutate(MapUnit = as.factor(
    ifelse(Sser1 %in% non_veg$map_code, Sser1, paste0(subzone, ".", Sser1)))) %>% 
  unite(point_id, c(Poly_Nbr, ECI_Tag), sep = "_", remove = TRUE) %>% 
  st_sf()

# MDB_tblPlotData has way too many columns, keep only important ones for BEC
tem_mdb_bec <- dplyr::select(
  tem_mdb$MDB_tblPlotData, PlotID, PlotNumber, PolygonNumber, BECUnit, SiteSeriesSymbol, 
  UTMNorthing, UTMEasting, StructuralStage, SiteNotes) %>% 
  tidyr::separate(BECUnit, into = c("subzone", "site_series")) %>% 
  unite(point_id, c(PolygonNumber, PlotNumber), sep = "_", remove = TRUE) %>% 
  dplyr::mutate(MapUnit = as.factor(
    ifelse(site_series == "00" & SiteSeriesSymbol %in% non_veg$map_code, 
           as.character(SiteSeriesSymbol), ifelse(
             site_series == "00", paste0(subzone, ".", SiteSeriesSymbol),
             paste0(subzone, ".", site_series))))) %>% 
  st_as_sf(coords = c("UTMEasting", "UTMNorthing"), crs = 26910) %>% 
  st_transform(3005)

```

Further data wrangling is applied to plant data from the TEM datasets.

```{r TEM Data - Plants}

tem_xls_pres <- lapply(tem_xls, function(x) {
  dplyr::select(x, Poly_Nbr, ECI_Tag, Pres, Comments) %>% 
    unite(point_id, c(Poly_Nbr, ECI_Tag), sep = "_", remove = TRUE) %>% 
    dplyr::mutate(Cover = NA_real_, A1 = NA_real_, A2 = NA_real_, A3 = NA_real_, 
                  B1 = NA_real_, B2 = NA_real_, C = NA_real_, D = NA_real_, 
                  Flowers = NA_character_, Fruit = NA_character_) %>% 
    merge(dplyr::select(st_drop_geometry(tem_xls_bec), point_id, MapUnit), all.x = TRUE) %>% 
    {if(!any(.$Pres)) .[0, ] else .}
  })

# Merge all 3 .mdb tables and filter plants of interest
tem_mdb_merge <- expand.grid(
  PlotID = unique(tem_mdb$MDB_tblPlotData$PlotID), 
  Species = unique(tem_mdb$MDB_tblVegSpecies$Species)) %>% 
  merge(
    merge(tem_mdb$MDB_tblVegLayer[, c("PlotID", "VegLayerID")], 
          tem_mdb$MDB_tblVegSpecies[, c("VegLayerID", "Species", "Cover")]), 
    all = TRUE) %>% 
  merge(tem_mdb$MDB_tblPlotData[, c("PlotID", "PlotNumber", "PolygonNumber", 
                                    "SiteNotes", "UTMEasting", "UTMNorthing")]) %>% 
  unite(point_id, c(PolygonNumber, PlotNumber), sep = "_", remove = TRUE) %>% 
  merge(dplyr::select(st_drop_geometry(tem_mdb_bec), point_id, MapUnit), all = TRUE) %>% 
  group_by(point_id, Species, MapUnit, SiteNotes, UTMEasting, UTMNorthing) %>% 
  dplyr::summarise(across(Cover, sum), .groups = "drop") %>% 
  dplyr::mutate(Pres = Cover > 0, A1 = NA_real_, A2 = NA_real_, A3 = NA_real_, 
                  B1 = NA_real_, B2 = NA_real_, C = NA_real_, D = NA_real_, 
                  Flowers = NA_character_, Fruit = NA_character_) %>% 
  replace_na(list(Pres = FALSE, Cover = 0)) %>% 
  st_as_sf(coords = c("UTMEasting", "UTMNorthing"), crs = 26910) %>% 
  st_transform(3005)

tem_mdb_pres_cov <- sapply(poi, function(x)
  dplyr::filter(tem_mdb_merge, Species == x) %>% 
    dplyr::select(point_id, MapUnit, A1, A2, A3, B1, B2, C, D, Flowers, Fruit, 
                  Pres, Cover, SiteNotes) %>% 
    dplyr::rename(Comments = SiteNotes), simplify = FALSE, USE.NAMES = TRUE)

```

##TEM data extraction
Next, we want to read in the TEM polygon layer and apply some filtering/data manipulation of it. The TEM feeds into the DSMART program to predict site series for the study area, and in the end it needs columns with an integer polygon number, a unique site ID associated with the polygon number, map unit ID's, and proportions of those map units. I'll walk through the processing steps:

1. Rename and select only the important columns needed for processing. The original TEM file comes with approximately 80 columns of data which is much too cumbersome to work with, so we should only select the ones important for our work: Polygon ID, BEC subzone, site series labels, site series deciles, and site series map codes.
3. Remove any NA values in the MapUnit1 column. If there are NA values in the first column, then there will be NA values in all of the other columns, and NA values can cause issues downstream.
4. Provide a fix to rename "WE" units to "OW" ("wetland" to "open water"), to be consistent with the TEM Excel spreadhseet and database.
5. Rename Map Units in each MapUnit column. If it is a forested site unit, it is concatenated from the BGC label and the site series call (e.g.: CWHdm/01); if it's nonforested (i.e.: site series call is 00), then the site series map label is used instead (e.g.: CWHdm/HL). If it is non-vegetated, only the site series map label is used (e.g.: RO)
6. Replace NA values in the MapUnit columns to 0's (this was necessary moving forward so that certain rows did not get removed);
7, 8, 9. Sum values in columns if the site series calls were exactly the same;
10. Replace 0's in the map unit columns with NA values
11. Make valid R names to the site series calls and remove site series with NA in the columns, and classify those site series calls as factors
12. Select the final dataset columns
13. Replace remaining NA's with 0's (only applied to numerical columns, doesn't affect factor levels)

The DSMART program requires a separate data frame of polygon compositions to be supplied. The data frame will have 5 columns in the end:
1. Polygon number
2. Map labels
3. Strata
4. Map Units
5. Proportions of map units in each polygon

It's pretty much a melted data frame from the polygon data, though I've adjusted it slightly so that it's shorter to remove rows with NA values. This data frame of polygon compositions is included in the DSMART algorithm.

```{r DSMART TEM processing}

# Load and remove columns where all values are NA
tem_sf <- st_read(file.path(shapes_dir, "tem.gpkg"), quiet = TRUE) %>% 
  Filter(function(x) !all(is.na(x)), .)

aoi_sf <- st_read(file.path(shapes_dir, "aoi.gpkg"), quiet = TRUE) %>% 
  st_geometry() %>% st_make_valid()

# TEM polygon processing as a DSMART input
dsmart_site_ser_polys <- dplyr::rename(
  tem_sf,
  Subzone = BIOGEOCLIMATIC_LBL, 
  MapUnit1 = SITE_SERIES_LBL_CPNT_1, 
  MapUnit2 = SITE_SERIES_LBL_CPNT_2, 
  MapUnit3 = SITE_SERIES_LBL_CPNT_3,
  Sdec1 = ECOSYSTEM_DECILE_CPNT_1, 
  Sdec2 = ECOSYSTEM_DECILE_CPNT_2, 
  Sdec3 = ECOSYSTEM_DECILE_CPNT_3, 
  NFMapUnit1 = SITE_SERIES_MAP_CDE_LBL_CPNT_1, 
  NFMapUnit2 = SITE_SERIES_MAP_CDE_LBL_CPNT_2, 
  NFMapUnit3 = SITE_SERIES_MAP_CDE_LBL_CPNT_3, 
  MAP_CODE = PROJECT_POLYGON_IDENTIFIER) %>% 
  dplyr::select(
    MAP_CODE, Subzone, starts_with(c("Sdec", "MapUnit", "NFMApUnit"))) %>% 
  drop_na(MapUnit1) %>% 
  dplyr::mutate(
    across(c(NFMapUnit1, NFMapUnit2, NFMapUnit3), ~ sub("WE", "OW", .x)), 
    across(c(Sdec1, Sdec2, Sdec3), ~ .x * 10)) %>% 
  dplyr::mutate(
    MapUnit1 = ifelse(MapUnit1 == "00" & NFMapUnit1 %in% non_veg$map_code, 
                      as.character(NFMapUnit1), ifelse(
                        MapUnit1 == "00", 
                        paste0(Subzone, "/", as.character(NFMapUnit1)), 
                        paste0(Subzone, "/", MapUnit1))),
    MapUnit2 = ifelse(MapUnit2 == "00" & NFMapUnit2 %in% non_veg$map_code, 
                      as.character(NFMapUnit2), ifelse(
                        MapUnit2 == "00",
                        paste0(Subzone, "/", as.character(NFMapUnit2)), 
                        paste0(Subzone, "/", MapUnit2))),
    MapUnit3 = ifelse(MapUnit3 == "00" & NFMapUnit3 %in% non_veg$map_code, 
                      as.character(NFMapUnit3), ifelse(
                        MapUnit3 == "00",
                        paste0(Subzone, "/", as.character(NFMapUnit3)), 
                        paste0(Subzone, "/", MapUnit3))),
    POLY_NO = as.integer(sub(".*_", "", MAP_CODE))) %>% 
  replace_na(list(MapUnit1 = 0, MapUnit2 = 0, MapUnit3 = 0)) %>% 
  dplyr::mutate(
    Sdec2 = ifelse(MapUnit2 == MapUnit3, Sdec2 + Sdec3, Sdec2), 
    Sdec3 = ifelse(MapUnit2 == MapUnit3, NA, Sdec3), 
    MapUnit3 = ifelse(MapUnit2 == MapUnit3, 0, MapUnit3)) %>% 
  dplyr::mutate(
    Sdec1 = ifelse(MapUnit1 == MapUnit3, Sdec1 + Sdec3, Sdec1), 
    Sdec3 = ifelse(MapUnit1 == MapUnit3, NA, Sdec3), 
    MapUnit3 = ifelse(MapUnit1 == MapUnit3, 0, MapUnit3)) %>% 
  dplyr::mutate(
    Sdec1 = ifelse(MapUnit1 == MapUnit2, Sdec1 + Sdec2, Sdec1), 
    Sdec2 = ifelse(MapUnit1 == MapUnit2, NA, Sdec2),
    MapUnit2 = ifelse(MapUnit2 == MapUnit1, 0, MapUnit2)) %>% 
  dplyr::mutate(
    MapUnit2 = na_if(MapUnit2, "0"), 
    MapUnit3 = na_if(MapUnit3, "0")) %>% 
  dplyr::mutate(
    MapUnit2 = ifelse((is.na(MapUnit2) & !is.na(MapUnit3)), MapUnit3, MapUnit2), 
    MapUnit3 = ifelse(MapUnit2 == MapUnit3, NA, MapUnit3), 
    Sdec2 = ifelse((is.na(Sdec2) & !is.na(Sdec3)), Sdec3, Sdec2),
    Sdec3 = ifelse(MapUnit2 == MapUnit3, NA, Sdec3)) %>% 
  dplyr::mutate(across(c(MapUnit1, MapUnit2, MapUnit3), 
                       ~as.factor(sub(".*NA\\.", NA, make.names(.x)))),
    Subzone = as.factor(Subzone)) %>% 
  dplyr::select(
    POLY_NO, Subzone, MAP_CODE, MapUnit1, Sdec1, 
    MapUnit2, Sdec2, MapUnit3, Sdec3) %>% 
  replace_na(list(Sdec1 = 0, Sdec2 = 0, Sdec3 = 0)) %>% 
  droplevels()

# Inspect geometries and remove incorrect data types:
if(!st_geometry_type(dsmart_site_ser_polys, FALSE) %in% 
   c("POLYGON", "MULTIPOLYGON")) {
  tem_geom <- st_geometry(dsmart_site_ser_polys)
  for(i in 1:length(tem_geom)) {
    if(!st_geometry_type(tem_geom[[i]]) %in% c("POLYGON", "MULTIPOLYGON")) {
      tem_geom[[i]] <- st_collection_extract(tem_geom[[i]], "POLYGON")
    }
  }
  st_geometry(dsmart_site_ser_polys) <- st_cast(tem_geom, "MULTIPOLYGON")
}

# Convert polygon composition to table with 5 columns
dsmart_site_ser_composition <- st_drop_geometry(dsmart_site_ser_polys) %>% 
  pivot_longer(cols = c("Sdec1", "Sdec2", "Sdec3")) %>% 
  mutate(MapUnit = ifelse(name == "Sdec1", as.character(MapUnit1), 
                          ifelse(name == "Sdec2", as.character(MapUnit2), 
                                 ifelse(name == "Sdec3", as.character(MapUnit3), 
                                        NA))),
         POLY_NO = as.integer(POLY_NO), 
         proportion = as.integer(value)) %>% 
  drop_na(MapUnit) %>% 
  mutate(MapUnit = as.factor(make.names(MapUnit)), 
         Subzone = as.integer(Subzone)) %>% 
  dplyr::select(POLY_NO, MAP_CODE, Subzone, MapUnit, proportion) %>% 
  group_by(POLY_NO, MAP_CODE, Subzone, MapUnit) %>% 
  dplyr::summarise(across(proportion, sum), .groups = "drop")

# From the TEM data, create a subzone layer which will be used to stratify sampling
dsmart_strata <- group_by(dsmart_site_ser_polys, Subzone) %>% 
  dplyr::summarise(.groups = "drop_last") %>% 
  dplyr::mutate(strata = as.integer(Subzone)) %>% 
  sf::st_cast("MULTIPOLYGON")

# Create table of additional observations
# Subzone filtering occurs at this step as I suspect a certain amount of human
# error is incurred in some of the datasets, so non-matching points (i.e.: points
# that are clearly in the wrong subzone) are removed.
dsmart_site_ser_observations <- bind_rows(lapply(
  list(sechelt_bec, tem_xls_bec, tem_mdb_bec), 
  dplyr::select, point_id, MapUnit, subzone)) %>% 
  sf::st_set_agr("constant") %>% 
  sf::st_transform(crs = st_crs(tem_sf)) %>% 
  sf::st_intersection(aoi_sf) %>% 
  droplevels() %>% 
  sf::st_join(dsmart_site_ser_polys) %>% 
  dplyr::mutate(across(c(MapUnit, Subzone), as.character)) %>% 
  cbind(
    bind_rows(lapply(1:nrow(.), function(i) {
      o <- .[i, ]
      s <- dplyr::filter(dsmart_strata, Subzone == o$subzone)
      data.frame(dist = st_distance(o, s))}))) %>% 
  dplyr::filter(dist <= units::set_units(100, m)) %>%
  cbind(st_coordinates(.)) %>% 
  sf::st_drop_geometry() %>% 
  dplyr::select(X, Y, point_id, MapUnit) %>% 
  dplyr::mutate(MapUnit = as.factor(make.names(MapUnit)))

# Write DSMART outputs
dso <- file.path(dsmart_dir, "site_ser", "inputs")
dir.create(dso, recursive = TRUE, showWarnings = FALSE)

invisible(sapply(c("dsmart_site_ser_polys", "dsmart_strata"), function(x) st_write(
  eval(parse(text = x)), file.path(dso, paste0(x, ".gpkg")), 
  delete_layer = TRUE, quiet = TRUE)))

invisible(sapply(
  c("dsmart_site_ser_composition", "dsmart_site_ser_observations"), function(x) 
    write.csv(
      eval(parse(text = x)), 
      file.path(dso, paste0(x, ".csv")), row.names = FALSE)))

# Write output as processed points in separate folder
proc_dir <- file.path(field_data_dir, "processed_2019", res_dir)
dir.create(proc_dir, showWarnings = FALSE, recursive = TRUE)
dsmart_site_ser_observations %>% 
  st_as_sf(coords = c("X", "Y")) %>% 
  st_set_crs(st_crs(tem_sf)) %>% 
  st_write(file.path(proc_dir, "site_ser_pts.gpkg"), quiet = TRUE, 
           delete_layer = TRUE)

```

With the essential data configured for the DSMART run, some extra data can be added as "observations". These will be point data of Map Unit calls, which is compiled from the TEM excel spreadsheet, the TEM mdb, and the 2019 sampling data. Depending on how the data is organized, various processing needed to occur for each dataset.

For the TEM excel spreadsheet, columns for each site series are selected, and then rows are filtered to include only pure calls from the first site series call. Certain column values are adjusted to remove NA values, or to include only the relevant/cosistent site series numbers and subzone names. The MapUnit column is then built as a concatenation of zone, subzone, variant, and site series, unless the site series call is made up purely of character values and not numbers, in which case it gives it the character values (this would be for non-forested site units so that they are named "RO" instead of "CWHdm1/RO".)

The mdb only had pure call values, so the site series column and site series symbols columns were selected. A similar approach is ensued where non forested site units are standalone site units while forested ones are prefixed with the subzone name. 

The three data sources (TEM excel spreadsheet, TEM mdb, and 2019 point data) are finally bound together, but only data that intersects with the original TEM map is included. We now have our 3 datasets needed to properly run the dsmart algorithm: TEM polygons with class proportion information; compositions of the TEM polygons in a separate data frame; and extra observations that will help with the prediction of each map.

```{r Pres/Abs and Cover Merge}

pto <- file.path(field_data_dir, "processed_2019", res_dir)
dir.create(pto, recursive = TRUE, showWarnings = FALSE)

# Create the raster stack (terra package) which will be used for data extraction
covariates <- terra::rast(list.files(covariate_dir, full.names = TRUE))

# Preliminary check of covariate data to eliminate any outlying rasters
# containing less data than normal
cov_check <- sapply(names(covariates), function(x) {
  cat(paste0("\rCounting NA values in ", x, " [", 
             which(x == names(covariates)), 
             " of ", nlyr(covariates), "]\n"))
  unname(freq(subset(covariates, x) * 0)[, "count"])}) %>% 
  data.frame(data_cells = .) %>% 
  rownames_to_column("layer") %>% 
  dplyr::filter(data_cells >= 0.95 * median(.$data_cells))
if(nrow(cov_check) != nlyr(covariates)) 
  covariates <- subset(covariates, cov_check$layer)

# Combine them all
pres_abs <- sapply(poi, function(x) {
  
  # Create unattributed points and write those
  out <- dplyr::bind_rows(sechelt_pres_cov[[x]], tem_xls_pres[[x]], 
                          tem_mdb_pres_cov[[x]]) %>% 
    dplyr::select(point_id, MapUnit, Pres, A1, A2, A3, B1, B2, C, D, Flowers, Fruit, Cover, Comments) %>% 
    st_set_agr("constant") %>% 
    st_transform(crs = st_crs(aoi_sf)) %>% 
    st_intersection(aoi_sf)
  st_write(out, file.path(pto, "pres_abs_no_atts.gpkg"), layer = x, 
           delete_layer = TRUE, quiet = TRUE)
  
  # Attribute the points and write those separately
  out_att <- cbind(out, terra::extract(covariates, st_coordinates(out))) %>%
    Filter(function(y) !all(is.na(y)), .) %>%
    dplyr::select(Pres, any_of(names(covariates))) %>%
    drop_na()
  st_write(out_att, file.path(pto, "pres_abs.gpkg"), layer = x,
           delete_layer = TRUE, quiet = TRUE)
  return(out_att)
}, simplify = FALSE, USE.NAMES = TRUE)

cover <- sapply(poi, function(x) {
  out <- dplyr::bind_rows(sechelt_pres_cov[[x]], tem_mdb_pres_cov[[x]]) %>% 
    dplyr::select(point_id, Cover) %>% 
    st_set_agr("constant") %>% 
    st_transform(crs = st_crs(aoi_sf)) %>% 
    st_intersection(aoi_sf) 
  st_write(out, file.path(pto, "cover_no_atts.gpkg"), layer = x, 
           delete_layer = TRUE, quiet = TRUE)
  
  out_att <- cbind(out, terra::extract(covariates, st_coordinates(out))) %>% 
    Filter(function(y) !all(is.na(y)), .) %>% 
    dplyr::select(Cover, any_of(names(covariates))) %>% 
    drop_na()
  st_write(out_att, file.path(pto, "cover.gpkg"), layer = x, 
           delete_layer = TRUE, quiet = TRUE)
  return(out_att)
}, simplify = FALSE, USE.NAMES = TRUE)

```

Using the land cover classification manual, create decisions for attributing each VRI polygon with a structural stage. This is complex. 

The land cover classifications use the BCLCS codes in 5 different layers to detail what a polygon might look like. Additionally, there are 3 "LAND_COVER_CLASS_CD" columns which detail how much of a given polygon is of a particular land classification; however, this is largely incomplete. Where it is, the best guess from the BCLCS layers will be used to determine the land class. Because there are three different calls sometimes, we can plug the final result into DSMART to spit out a raster of structural stage.

***Currently, I'm not sure how to implement water bodies. I'm currently putting all water classified polygons (freshwater and marine) into the 2c classification, but that may change.

```{r Structural stage prep}

# Read in VRI data
vri <- st_read(file.path(shapes_dir, "vri_full.gpkg"), quiet = TRUE)
vri_bgc <- dplyr::select(vri, BEC_ZONE_CODE, BEC_SUBZONE, BEC_VARIANT) %>% 
  dplyr::mutate(bgc = ifelse(!is.na(BEC_VARIANT), 
                             paste0(BEC_ZONE_CODE, BEC_SUBZONE, BEC_VARIANT), 
                             paste0(BEC_ZONE_CODE, BEC_SUBZONE))) %>% 
  group_by(bgc) %>% dplyr::summarise(.groups = "drop")

st_write(vri_bgc, file.path(shapes_dir, "bgc_vri.gpkg"), 
         delete_layer = TRUE, quiet = TRUE)

# Use VRI to interpret structural stage
# First splits: Vegetated and non-vegetated, then trees vs. no trees in
# vegetated table
vri_veg_only <- dplyr::select(vri, starts_with(c(
  "POLYGON_ID", "MAP_ID", "BCLCS", "LAND_COVER_CLASS", "EST_COVERAGE_PCT", 
  "SPECIES_PCT", "PROJ_AGE", "BEC"))) %>% 
  dplyr::filter(BCLCS_LEVEL_1 == "V")
vri_no_veg <- dplyr::select(vri, starts_with(c(
  "POLYGON_ID", "MAP_ID", "BCLCS", "LAND_COVER_CLASS", "EST_COVERAGE_PCT", 
  "SPECIES_PCT", "PROJ_AGE", "BEC"))) %>% 
  dplyr::filter(BCLCS_LEVEL_1 != "V")
vri_trees <- dplyr::filter(vri_veg_only, BCLCS_LEVEL_2 == "T")
vri_no_trees <- dplyr::filter(vri_veg_only, BCLCS_LEVEL_2 != "T")

# Focus on ss 1 - 3
# In the Sechelt dataset, I treat ES (exposed soil) as 1a, 
# BM (bryoid moss) gets 1b, and BL (bryoid lichens) gets 1c
ss_1a <- dplyr::filter(vri_no_veg, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "EL", 
  LAND_COVER_CLASS_CD_1 == "ES")) %>% 
  dplyr::mutate(ss1 = "1a", ss2 = NA, ss3 = NA)
ss_1b <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "BM",
  LAND_COVER_CLASS_CD_1 == "BM")) %>% 
  dplyr::mutate(ss1 = "1b", ss2 = NA, ss3 = NA)
ss_1c <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "BL",
  LAND_COVER_CLASS_CD_1 == "BL")) %>% 
  dplyr::mutate(ss1 = "1c", ss2 = NA, ss3 = NA)

# Additional entries classified as BY get randomly assigned to either 1b or 1c:
ss_1 <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "BY",
  LAND_COVER_CLASS_CD_1 == "BY")) %>% 
  dplyr::mutate(LAND_COVER_CLASS_CD_1 = ifelse(round(runif(nrow(.))) == 1, 
                                               "BM", "BL")) %>% 
  dplyr::mutate(ss1 = ifelse(LAND_COVER_CLASS_CD_1 == "BM", "1b", "1c"), 
                ss2 = NA, ss3 = NA)
ss_1b <- if(any(ss_1$LAND_COVER_CLASS_CD_1 == "BM")) {
  rbind(ss_1b, ss_1[ss_2$LAND_COVER_CLASS_CD_1 == "BM", ])
} else ss_1b
ss_1c <- if(any(ss_1$LAND_COVER_CLASS_CD_1 == "BL")) {
  rbind(ss_1c, ss_1[ss_2$LAND_COVER_CLASS_CD_1 == "BL", ])
} else ss_1c

# The herb dominated polygons will be assigned structural stage 2
# Forb dominants (HF) get 2a, graminoid dominants (HG) get 2b
# All polygons that are designated lakes or ocean get assigned 2c, though this
# may change
ss_2a <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "HF", 
  LAND_COVER_CLASS_CD_1 == "HF")) %>% 
  dplyr::mutate(ss1 = "2a", ss2 = NA, ss3 = NA)
ss_2b <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "HG", 
  LAND_COVER_CLASS_CD_1 == "HG")) %>% 
  dplyr::mutate(ss1 = "2b", ss2 = NA, ss3 = NA)
ss_2c <- rbind(
  dplyr::filter(vri_no_veg, ifelse(
    is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_5 %in% c("LA", "RE", "RI", "OC"), 
    LAND_COVER_CLASS_CD_1 %in% c("LA", "RE", "RI", "OC"))),
  dplyr::filter(vri_no_trees, 
    LAND_COVER_CLASS_CD_1 %in% c("LA", "RE", "RI", "OC"))) %>% 
  dplyr::mutate(ss1 = "2c", ss2 = NA, ss3 = NA)

# No foreseeable way to distinguish structural stage 2d from VRI data
# Additional entries that may go into 2a, 2b, or 2d, or be randomly assigned to
# 2a or 2b:
ss_2 <- dplyr::filter(vri_no_trees, ifelse(
  is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "HE",
  LAND_COVER_CLASS_CD_1 == "HE")) %>% 
  dplyr::mutate(LAND_COVER_CLASS_CD_1 = ifelse(round(runif(nrow(.))) == 1, 
                                               "HF", "HG")) %>% 
  dplyr::mutate(ss1 = ifelse(LAND_COVER_CLASS_CD_1 == "HF", "2a", "2b"), 
                ss2 = NA, ss3 = NA)
ss_2a <- if(any(ss_2$LAND_COVER_CLASS_CD_1 == "HF")) {
  rbind(ss_2a, ss_2[ss_2$LAND_COVER_CLASS_CD_1 == "HF", ])
} else ss_2a
ss_2b <- if(any(ss_2$LAND_COVER_CLASS_CD_1 == "HG")) {
  rbind(ss_2b, ss_2[ss_2$LAND_COVER_CLASS_CD_1 == "HG", ])
} else ss_2b

# SL stands for shrub low, ST shrub tall. Those easily get assigned to 3a and 3b,
# respectively
ss_3a <- rbind(
  dplyr::filter(vri_no_trees, ifelse(
    is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "SL", 
    LAND_COVER_CLASS_CD_1 == "SL")),
  dplyr::filter(vri_no_veg, ifelse(
    is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "SL", 
    LAND_COVER_CLASS_CD_1 == "SL"))) %>% 
  dplyr::mutate(
    ss1 = "3a", 
    ss2 = ifelse(
      LAND_COVER_CLASS_CD_2 == "HE", ifelse(
        round(runif(nrow(.))) == 1, "2a", "2b"), 
      ifelse(LAND_COVER_CLASS_CD_2 == "TC", "5", LAND_COVER_CLASS_CD_2)),
    ss3 = ifelse(LAND_COVER_CLASS_CD_3 == "LA", "2c", LAND_COVER_CLASS_CD_3))
ss_3b <- rbind(
  dplyr::filter(vri_no_trees, ifelse(
    is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "ST", 
    LAND_COVER_CLASS_CD_1 == "ST")),
  dplyr::filter(vri_no_veg, ifelse(
    is.na(LAND_COVER_CLASS_CD_1), BCLCS_LEVEL_4 == "ST", 
    LAND_COVER_CLASS_CD_1 == "ST"))) %>% 
  dplyr::mutate(ss1 = "3b", ss2 = NA, ss3 = NA)

### Moving on to treed polygons
# First, create columns of the weighted average stand age of the two most
# dominant tree species
vri_avg <- dplyr::mutate(vri_trees,
  age_avg = ifelse(
    !is.na(PROJ_AGE_2), 
    ((SPECIES_PCT_1 * PROJ_AGE_1) + (SPECIES_PCT_2 * PROJ_AGE_2)) / 
      (SPECIES_PCT_1 + SPECIES_PCT_2), 
    PROJ_AGE_1))

# Basing all decisions on average stand age...might not be overly accurate, but
# closest approximation given the data currently at hand. Structural stage 4
# gets < 20 years, 5 gets 20-80 years, 6 gets 80-250 years (based on NDT from
# subzone), and 7 gets greater than 250 years. I cannot find an accurate way to 
# decipher between structural stages 7a and 7b from VRI data, so they are lumped 
# together here.
ss_4 <- dplyr::filter(vri_avg, age_avg < 20) %>% 
  dplyr::mutate(ss1 = "4", ss2 = NA, ss3 = NA)
ss_5 <- dplyr::filter(vri_avg, age_avg >= 20, age_avg < 80) %>% 
  dplyr::mutate(
    ss1 = "5", 
    ss2 = ifelse(LAND_COVER_CLASS_CD_2 == "SL", "3a", LAND_COVER_CLASS_CD_2),
    ss3 = ifelse(LAND_COVER_CLASS_CD_3 == "BR", "1a", LAND_COVER_CLASS_CD_3))
ss_6 <- dplyr::filter(vri_avg, age_avg >= 80, age_avg < 250) %>% 
  dplyr::mutate(ss1 = "6", ss2 = NA, ss3 = NA)
ss_7 <- dplyr::filter(vri_avg, age_avg >= 250) %>% 
  dplyr::mutate(
    ss1 = "7", 
    ss2 = ifelse(LAND_COVER_CLASS_CD_2 == "BR", "1a", LAND_COVER_CLASS_CD_2),
    ss3 = NA)

# Combine each of the structural stages together and create columns used in 
# DSMART 
dsmart_struct_stg_polys <- rbind(
  ss_1a, ss_1b, ss_1c, ss_2a, ss_2b, ss_2c, ss_3a, ss_3b, 
  rbind(ss_4, ss_5, ss_6, ss_7) %>% dplyr::select(names(ss_1a))) %>% 
  dplyr::mutate(
    across(EST_COVERAGE_PCT_1, ~ replace_na(., 100)),
    across(c(EST_COVERAGE_PCT_2, EST_COVERAGE_PCT_3), ~ replace_na(., 0)), 
    across(c(ss1, ss2, ss3), ~ as.factor(sub(".*NA\\.", NA, make.names(.)))),
    MAP_CODE = paste0(MAP_ID, "_", POLYGON_ID)) %>% 
  unite(Subzone, c(BEC_ZONE_CODE, BEC_SUBZONE, BEC_VARIANT), 
        sep = "", remove = TRUE, na.rm = TRUE) %>% 
  dplyr::select(POLYGON_ID, Subzone, MAP_CODE, ss1, EST_COVERAGE_PCT_1, 
                ss2, EST_COVERAGE_PCT_2, ss3, EST_COVERAGE_PCT_3) %>% 
  mutate(across(Subzone,as.factor))

# Inspect geometries and remove incorrect data types:
if(!st_geometry_type(dsmart_struct_stg_polys, FALSE) %in% 
   c("POLYGON", "MULTIPOLYGON")) {
  ss_geom <- st_geometry(dsmart_struct_stg_polys)
  for(i in 1:length(ss_geom)) {
    if(!st_geometry_type(ss_geom[[i]]) %in% c("POLYGON", "MULTIPOLYGON")) {
      ss_geom[[i]] <- st_collection_extract(ss_geom[[i]], "POLYGON")
    }
  }
  st_geometry(dsmart_struct_stg_polys) <- st_cast(ss_geom, "MULTIPOLYGON")
}

# Convert polygon composition to table with 5 columns
dsmart_struct_stg_composition <- st_drop_geometry(dsmart_struct_stg_polys) %>% 
  pivot_longer(cols = c(
    "EST_COVERAGE_PCT_1", "EST_COVERAGE_PCT_2", "EST_COVERAGE_PCT_3"), 
    values_to = "proportion") %>% 
  dplyr::mutate(
    ss = as.factor(ifelse(
      name == "EST_COVERAGE_PCT_1", as.character(ss1), ifelse(
        name == "EST_COVERAGE_PCT_2", as.character(ss2), ifelse(
          name == "EST_COVERAGE_PCT_3", as.character(ss3), NA)))),
    across(c(POLYGON_ID, proportion, Subzone), as.integer)) %>% 
  drop_na(ss) %>% 
  dplyr::select(POLYGON_ID, MAP_CODE, Subzone, ss, proportion) %>% 
  group_by(POLYGON_ID, MAP_CODE, Subzone, ss) %>% 
  dplyr::summarise(across(proportion, sum), .groups = "drop")

# Create table of additional observations
# Fix incorrectly attributed structural stage calls
tem_mdb_struct_stg <- dplyr::select(tem_mdb_bec, StructuralStage, SiteNotes) %>% 
  dplyr::filter(StructuralStage != "") %>% 
  dplyr::mutate(StructuralStage = ifelse(
    StructuralStage == "3b/c", "3b", ifelse(
      StructuralStage == 2, "2a", ifelse(
        StructuralStage == 3 & grepl(
          "logged in 1997|open grown with lots of salal|in lower portion of cutblock", 
          .$SiteNotes), "3a", ifelse(
            StructuralStage == 3 & grepl(
              "young second growth|old cut block|Fd plantation", .$SiteNotes), 
            "3b", StructuralStage)))))

# Put all observations together in a single table
dsmart_struct_stg_observations <- dplyr::select(sechelt_bec, Structural.Stage) %>% 
  dplyr::rename(StructuralStage = Structural.Stage) %>% 
  dplyr::mutate(StructuralStage = ifelse(StructuralStage %in% 
                                           c(2, 3), "3a", StructuralStage)) %>% 
  bind_rows(dplyr::select(tem_mdb_struct_stg, StructuralStage),
            dplyr::select(tem_xls_bec, StructuralStage)) %>% 
  dplyr::filter(complete.cases(StructuralStage)) %>% 
  mutate(StructuralStage = trimws(StructuralStage)) %>% 
  st_set_agr("constant") %>% 
  st_transform(crs = st_crs(aoi_sf)) %>% 
  st_intersection(aoi_sf) %>% 
  cbind(st_coordinates(.)) %>% 
  st_drop_geometry() %>% 
  dplyr::select(X, Y, StructuralStage) %>% 
  dplyr::mutate(StructuralStage = as.factor(make.names(StructuralStage)))

# Create extracted sf dataframe of observations
struct_stg <- dsmart_struct_stg_observations %>% 
  sf::st_as_sf(coords = c("X", "Y"), crs = st_crs(dsmart_struct_stg_polys)) %>% 
  cbind(terra::extract(covariates, st_coordinates(.))) %>% 
  Filter(function(y) !all(is.na(y)), .) %>% 
  dplyr::select(StructuralStage, names(covariates)) %>% 
  drop_na()

# Write data outputs
dso <- file.path(dsmart_dir, "struct_stg", "inputs")
dir.create(dso, recursive = TRUE, showWarnings = FALSE)

st_write(
  dsmart_struct_stg_polys, file.path(dso, paste0("dsmart_struct_stg_polys.gpkg")), 
  delete_layer = TRUE, quiet = TRUE)

invisible(sapply(
  c("dsmart_struct_stg_composition", "dsmart_struct_stg_observations"), function(x) 
    write.csv(
      eval(parse(text = x)), file.path(dso, paste0(x, ".csv")), 
      row.names = FALSE)))

st_write(struct_stg, file.path(pto, "struct_stg_pts.gpkg"), 
         delete_layer = TRUE, quiet = TRUE)

```

