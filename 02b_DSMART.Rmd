---
title: "02b_DSMART"
author: "Matthew Coghill"
date: "2/19/2020"
output: html_document
---
This document details how to run the DSMART algorithm: Disaggregation and harmonisation of Soil MAp units through Resampled classification Trees. Originally, the algorithm was intented to create a model predicting soil classes; however, it's being used here to predict site series across a landscape because the process has many parallels. Here, it will be used to convert a TEM to a raster based on the proportions of each polygon.

Additionally, this script assumes that all data manipulation and pre-processing has occurred prior to running (script 02a_PointData_Prep has been ran). The intention of this script was to be a standalone documentation of DSMART since it takes quite a while to run and produce a map.

First, libraries are loaded and data directories are defined. The rdsmart package comes from BitBucket (similar to GitHub) and it is not on CRAN, so it must be downloaded separately and an installation of the devtools package is required for that. The rdsmart package has also been recently updated to streamline the process of making these spatial predictions; by combining the caret package and rdsmart, it is now much quicker to produce a map than it was previously. The whole process takes ~12 hours now on a machine running with 4 cores which is much quicker than previous trials which took upwards of 4 days.

June update: Thinking about this a bit, I have improved the DSMART algorithm internally so that it runs much quicker now. The base way of producing a map through the DSMART algorithm is to generate "n.reals" amounts of predictions using C50 tree models and then calculate the best map it could from the realilsations at hand. The other way that I was doing involved using a RandomForest approach in either the caret or mlr3 packages. If this is the way it's going to be created, THIS SHOULD ONLY NEED TO HAVE A SINGLE REALISATION since it creates predictions based on multiple resamplings etc.

```{r Load Packages}

ls <- c("sf", "raster", "fasterize", "rgdal", "caret", "ranger", 
        "tidyverse", "terra", "foreach", "GSIF", "gdalUtils", "MLmetrics")
new.packages <- ls[!(ls %in% installed.packages()[, "Package"])]
if(length(new.packages))
  install.packages(new.packages)

# Make sure terra package is up to date! This may take a moment
if(compareVersion(as.character(packageVersion("terra")), "0.7-4") < 0)
  install.packages("terra")
lapply(ls, library, character.only = TRUE)[0]
rm(ls, new.packages)

# Load custom DSMART files
for(i in list.files("./_functions/dsmart_custom", full.names = TRUE, 
                    all.files = TRUE, no.. = TRUE))
  source(i)

```

Next, file paths are defined.

```{r Set directories}

AOI <- "Sechelt"
AOI_dir <- file.path(".", paste0(AOI, "_AOI"))
map_res <- 4

shapes_path <- file.path(AOI_dir, "0_raw_inputs", "base_layers")
covariate_dir <- file.path(AOI_dir, "1_map_inputs", "covariates")
dsmart_dir <- file.path(AOI_dir, "1_map_inputs", "dsmart")

include_bgc <- FALSE
use_caret <- TRUE

```

Next, important data is loaded. There are a lot of covariate layers created from DEM derivatives, satellite imagery, and climate data layers; however, in order to speed up processing, the amount of covariates are limited to only DEM derivatives, annual satellite imagery indices, and more recent annual climate data. These variables were chosen since it makes sense to model a landscape using more recent data in my mind, and not worry about older factors that may have influenced the landscape previously. If a BGC layer is to be used to create the DSMART maps as well, then that is included as well.

```{r Load data}

# Load covariates
res_folder <- paste0(map_res, "m")
terrain_covariates <- grep(pattern = "normal|sentinel2", list.files(
  file.path(covariate_dir, res_folder), full.names = TRUE, pattern = ".tif$"), invert = TRUE, 
  value = TRUE)
sentinel_covariates <- list.files(file.path(covariate_dir, res_folder), 
                                  pattern = "^sentinel2.*.2019.tif$", full.names = TRUE)
climate_covariates <- list.files(file.path(covariate_dir, res_folder), 
                                 pattern = "^normal_1981_2010y", full.names = TRUE)
covariates <- raster::stack(c(terrain_covariates, sentinel_covariates, climate_covariates))

# Load the polygons and associated observations
polygons <- readOGR(file.path(dsmart_dir, "inputs", "dsmart_polygons.gpkg"), layer = "dsmart_polygons")

if(include_bgc) {
  bgc <- st_read(file.path(shapes_path, res_folder, "bec.gpkg"), quiet = TRUE) %>% 
    mutate(MAP_LABEL = as.factor(MAP_LABEL)) 
  tem_geom <- st_geometry(bgc)
  
  # Fix geometries that aren't polygon/multipolygon
  for(i in 1:length(tem_geom)) {
    if(!st_geometry_type(tem_geom[[i]]) %in% c("POLYGON", "MULTIPOLYGON")) 
      tem_geom[[i]] <- st_collection_extract(tem_geom[[i]], "POLYGON") %>% 
        st_multipolygon()
  }
  st_geometry(bgc) <- st_cast(tem_geom, "MULTIPOLYGON")
  
  level_table <- unique.data.frame(data.frame(
    ID = as.numeric(bgc$MAP_LABEL), 
    Subzone = as.character(bgc$MAP_LABEL)))
  bgc_raster <- asFactor(fasterize(bgc, covariates[[1]], field = "MAP_LABEL"))
  levels(bgc_raster) <- level_table
  
  # File needs to be written to disk and then reloaded back into R for proper 
  # feature assignments
  writeRaster(bgc_raster, file.path(dsmart_dir, "inputs", "bgc.tif"), overwrite = TRUE)
  covariates <- stack(covariates, file.path(dsmart_dir, "inputs", "bgc.tif"))
  factors <- "bgc"
  
} else {
  factors <- NULL
}

# Load TEM composition and additional observations
composition <- read.csv(file.path(dsmart_dir, "inputs", "dsmart_composition.csv")) %>% 
    dplyr::select(-Subzone)
observations <- read.csv(file.path(dsmart_dir, "inputs", "dsmart_observations.csv"))

```



```{r Simple TEM rasterization}

dir.create(file.path(dsmart_dir, "simple"), showWarnings = FALSE)

# Rasterize first calls only
tem_sf <- st_read(file.path(dsmart_dir, "inputs", "dsmart_polygons.gpkg"), quiet = TRUE) %>% 
  mutate(MapUnit1 = as.factor(MapUnit1))
tem_r <- fasterize(tem_sf, covariates$dem, field = "MapUnit1")
writeRaster(tem_r, file.path(dsmart_dir, "simple", "site_series_simple.tif"), 
            overwrite = TRUE)

# Generate probabilities based on composition
ss_calls <- unique(composition$MapUnit)
composition_remake <- foreach(i = unique(composition$POLY_NO), .combine = rbind) %do% {
  composition %>% dplyr::filter(POLY_NO == i) %>% 
    dplyr::select(POLY_NO, MapUnit, proportion) %>% 
    rbind(data.frame(
      POLY_NO = .$POLY_NO[1], 
      MapUnit = ss_calls[!ss_calls %in% .$MapUnit],
      proportion = 0
    ))
}

tem_remake <- tem_sf %>% 
  dplyr::select(POLY_NO) %>% 
  merge(composition_remake) %>% 
  mutate(proportion = proportion / 100)

for(i in unique(tem_remake$MapUnit)) {
  ss_filter <- tem_remake[tem_remake$MapUnit == i, ]
  ss_prop <- fasterize(ss_filter, covariates$dem, field = "proportion") %>% 
    magrittr::set_names(paste0("probability_", i))
  writeRaster(ss_prop, file.path(dsmart_dir, "simple", paste0("probability_", i, ".tif")),
              overwrite = TRUE)
}

```



```{r Editing the dsmart functions}

# Note: for randomForest model method, you only need to have the mtry defined in 
# the tuneGrid part. Also note that if class probabilities are requested, the 
# value needs to be changed to TRUE and "type" in the disaggregate function 
# needs to be changed to "prob".
if(use_caret) {
  method.model <- "ranger"
  args.model <- list(
    trControl = trainControl(
      method = "repeatedcv", 
      number = 10, 
      repeats = 5, 
      classProbs = TRUE,
      returnData = FALSE,
      returnResamp = "final",
      savePredictions = "final",
      summaryFunction = multiClassSummary, # Adds a suite of performance metrics to the output
      allowParallel = TRUE),
    tuneGrid = expand.grid(
      mtry = floor(sqrt(nlayers(covariates))), # Default random forest options
      min.node.size = 1,
      splitrule = "gini"
    )
  )
} else {
  method.model <- NULL
  args.model <- NULL
}

disag <- disaggregate(
  covariates = covariates, 
  polygons = polygons, 
  composition = composition,
  rate = 20, # How many samples to draw per polygon
  reals = 5, # How many models to produce from the modelling run? Gets overridden by number of repeats if model type is ranger
  observations = observations, 
  method.sample = "by_polygon", 
  method.allocate = "weighted",
  method.model = method.model,
  args.model = args.model,
  outputdir = file.path(AOI_dir, "1_map_inputs", "dsmart"), 
  cpus = parallel::detectCores(), 
  factors = factors,
  type = "prob"
)

if(is.null(method.model)) {
  summ <- summarise(
    realisations = raster::stack(list.files(
      file.path(AOI_dir, "1_map_inputs", "dsmart", "output", "realisations"), 
      full.names = TRUE, pattern = ".tif$")),
    lookup = read.table(
      file.path(AOI_dir, "1_map_inputs", "dsmart", "output", "lookup.txt"), 
      header = TRUE, sep = ","),
    outputdir = file.path(AOI_dir, "1_map_inputs", "dsmart"), 
    nprob = 1,
    cpus = parallel::detectCores(), 
    type = "raw"
  )
}

# Save the file to a better location
covariates_t <- rast(covariates)
site_series <- rast(file.path(dsmart_dir, "output", "realisations", "realisation_1.tif")) %>% 
  magrittr::set_names("site_series_dsmart")

writeRaster(site_series, 
            file.path(dsmart_dir, "output", "site_series_dsmart.tif"), 
            overwrite = TRUE, wopt = list(datatype = "INT2S"))

```

USING RANGER METHOD, TOOK A TOTAL OF 6 HOURS TO COMPLETE

Finally, we need to run the algorithm. This version of DSMART has implementation allowing the use of the caret package to make models, and that functionality is leveraged here. We will use a parallelized implementation of Random Forest modelling in order to predict site series fore each pixel.

It's very important to note that this is a very slow program, thus it's important that all of the datasets created above are in fact correct prior to running the algorithm less you get a few hours in and it fails because the names of the MapUnit columns are not valid in R.

That said, I'll try to explain the process here. First, we need to build a covariate dataset. For the PEM project, we used DEM and satellite derived layers to build our site series maps, and I'll stick to that here as well in order to speed up some of the processing. I'm getting rid of covariate layers whose name starts with "normal" because all of the climate layers (of which there are about 150) begin with the word "normal", so it's a simple way to omit those layers from analysis. The next part (fit_control) specifies some of the modelling intricacies. Modelling will be passed on to the caret package where it will run a parallelized version of random forest analysis to generate class probabilities.

Finally, we can run the dsmart algorithm to predict site series for a given pixel. At the time of running this algorithm, for a 4m resolution, the first part of the algorithm (sampling) took approximately 6.5 hours to run. After that, it starts to produce models and maps from the data, and it takes about 1 hour per model to produce a "realization" (for 5 models that's 5 hours). The final part of the algorithm takes those realizations and summarizes them into n most probable maps (nprob), and for 1 probability map it took about 30 minutes. The first is the most probable map, and the last defined would be the least probable map of that series. The verbose output seems to throw a lot of errors, but when the models are loaded back into a given R session they seem unphased, so I think that there is some issue with the verbose nature of the output seen here so it can be ignored as long as the algorithm is running.

Some ways in which this could be sped up:
1. Reduce the map resolution. Con: the map is a lower resolution
2. Decrease the "rate" parameter in the dsmart function. Con: fewer samples are drawn per polygon so the accuracy may be skewed
3. Change the "method.sample" parameter to "by_area" and draw "rate" number of samples per square km. Con: Doing this could potentially miss small polygons and their associated site series calls.
4. Decrease the "reals" parameter in the dsmart function. Con: fewer models are produced for comparison and final map generation
5. Find a way to parallelize the caret::train function within dsmart (a cluster may need to be created prior to running the algorithm)
6. Decrease the nprob parameter to generate only the most probable map instead of the top 3 most probable maps.


