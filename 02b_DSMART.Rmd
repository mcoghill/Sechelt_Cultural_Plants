---
title: "02b_DSMART"
author: "Matthew Coghill"
date: "2/19/2020"
output: html_document
---
This document details how to run the DSMART algorithm: Disaggregation and harmonisation of Soil MAp units through Resampled classification Trees. Originally, the algorithm was intented to create a model predicting soil classes; however, it's being used here to predict site series across a landscape because the process has many parallels. Here, it will be used to convert a TEM to a raster based on the proportions of each polygon.

Additionally, this script assumes that all data manipulation and pre-processing has occurred prior to running (script 02a_PointData_Prep has been ran). The intention of this script was to be a standalone documentation of DSMART since it takes quite a while to run and produce a map.

First, libraries are loaded and data directories are defined. The rdsmart package comes from BitBucket (similar to GitHub) and it is not on CRAN, so it must be downloaded separately and an installation of the devtools package is required for that. The rdsmart package has also been recently updated to streamline the process of making these spatial predictions; by combining the caret package and rdsmart, it is now much quicker to produce a map than it was previously. The whole process takes ~12 hours now on a machine running with 4 cores which is much quicker than previous trials which took upwards of 4 days.

June update: Thinking about this a bit, I have improved the DSMART algorithm internally so that it runs much quicker now. The base way of producing a map through the DSMART algorithm is to generate "n.reals" amounts of predictions using C50 tree models and then calculate the best map it could from the realilsations at hand. The other way that I was doing involved using a RandomForest approach in either the caret or mlr3 packages. If this is the way it's going to be created, THIS SHOULD ONLY NEED TO HAVE A SINGLE REALISATION since it creates predictions based on multiple resamplings etc.

```{r Load Packages}

suppressMessages(suppressWarnings({
  ls <- c("tidyverse", "terra", "sf", "raster", "foreach", "caret", "ranger", 
          "GSIF", "gdalUtils", "MLmetrics")
  new.packages <- ls[!(ls %in% installed.packages()[, "Package"])]
  if(length(new.packages))
    install.packages(new.packages)
  
  # Make sure terra package is up to date! This may take a moment
  if(compareVersion(as.character(packageVersion("terra")), "0.7-4") < 0)
    install.packages("terra")
  lapply(ls, library, character.only = TRUE)[0]
  rm(ls, new.packages)}))

# Load custom DSMART files
source("./_functions/dsmart_custom/dsmart.R")

```

Next, file paths are defined.

```{r Set directories}

AOI <- "Sechelt"
AOI_dir <- file.path(".", paste0(AOI, "_AOI"))
map_res <- 25
res_dir <- ifelse(is.numeric(map_res), paste0(map_res, "m"), map_res)

shapes_dir <- file.path(AOI_dir, "0_raw_inputs", "base_layers", res_dir)
covariate_dir <- file.path(AOI_dir, "1_map_inputs", "covariates", res_dir)

include_bgc <- FALSE
use_caret <- TRUE
run <- "site_ser" # option of "struct_stg" or "site_ser"
dsmart_dir <- file.path(AOI_dir, "1_map_inputs", "dsmart_2019", res_dir, run)

```

Next, important data is loaded. There are a lot of covariate layers created from DEM derivatives, satellite imagery, and climate data layers; however, in order to speed up processing, the amount of covariates are limited to only DEM derivatives, annual satellite imagery indices, and more recent annual climate data. These variables were chosen since it makes sense to model a landscape using more recent data in my mind, and not worry about older factors that may have influenced the landscape previously. Additionally, the climate variables are not much different between the 1961-1990 dataset and the 1981-2010 dataset, so autocorrelation effects may be induced if all of the climate variables were kept here. If a BGC layer is to be used to create the DSMART maps as well, then that is included as well.

```{r Load data}

# Load covariates
terrain_covariates <- grep(pattern = "normal|sentinel2", list.files(
  covariate_dir, full.names = TRUE, pattern = ".tif$"), invert = TRUE, 
  value = TRUE)
sentinel_covariates <- list.files(
  covariate_dir, pattern = "^sentinel2.*.2019.tif$", full.names = TRUE)
climate_covariates <- list.files(
  covariate_dir, pattern = "^normal_1981_2010y", full.names = TRUE)
covariates <- terra::rast(c(terrain_covariates, sentinel_covariates, climate_covariates))

# Preliminary check of covariate data to eliminate any outlying rasters containing
# less data than normal
cov_check <- foreach(i = 1:nlyr(covariates), .combine = rbind) %do% {
  new <- subset(covariates, i) * 0
  data.frame(layer = names(new), 
             data_cells = data.frame(freq(new))$count)
} %>% dplyr::filter(data_cells >= 0.95 * median(.$data_cells))
if(nrow(cov_check) != nlyr(covariates)) 
  covariates <- subset(covariates, cov_check$layer)

# Load the polygons and associated observations
polygons <- terra::vect(file.path(dsmart_dir, "inputs", paste0(
  "dsmart_", run, "_polys.gpkg")))

if(include_bgc) {
  bgc <- st_read(file.path(shapes_dir, "bec.gpkg"), quiet = TRUE) %>% 
    mutate(MAP_LABEL = as.factor(MAP_LABEL), bgc = as.numeric(as.factor(MAP_LABEL))) 
  tem_geom <- st_geometry(bgc)
  
  # Fix geometries that aren't polygon/multipolygon
  for(i in 1:length(tem_geom)) {
    if(!st_geometry_type(tem_geom[[i]]) %in% c("POLYGON", "MULTIPOLYGON")) 
      tem_geom[[i]] <- st_collection_extract(tem_geom[[i]], "POLYGON") %>% 
        st_multipolygon()
  }
  st_geometry(bgc) <- st_cast(tem_geom, "MULTIPOLYGON")
  
  level_table <- unique.data.frame(data.frame(
    label = as.character(bgc$MAP_LABEL), 
    value = as.numeric(bgc$MAP_LABEL)))
  
  bgc_rast <- as.factor(terra::rasterize(vect(bgc), covariates[[1]], field = "bgc"))
  levels(bgc_rast) <- level_table
  
  # File needs to be written to disk and then reloaded back into R for proper 
  # feature assignments
  bgc_rast <- writeRaster(bgc_rast, file.path(dsmart_dir, "inputs", "bgc.tif"), 
                          overwrite = TRUE, wopt = list(datatype = "INT2S"))
  covariates <- c(covariates, bgc_rast)
  factors <- "bgc"
  
} else {
  factors <- NULL
}

# Load TEM composition and additional observations
composition <- read.csv(file.path(dsmart_dir, "inputs", paste0(
  "dsmart_", run, "_composition.csv"))) %>% 
    dplyr::select(-Subzone)
observations <- read.csv(file.path(dsmart_dir, "inputs", paste0(
  "dsmart_", run, "_observations.csv")))

```



```{r Simple TEM rasterization}

if(run == "site_ser") {
  dir.create(file.path(dsmart_dir, "simple"), showWarnings = FALSE)
  
  # Rasterize first calls only
  tem <- terra::vect(file.path(dsmart_dir, "inputs", paste0("dsmart_", run, "_polys.gpkg")))
  tem$MapUnit1_num <- as.numeric(as.factor(tem$MapUnit1))
  tem_rast <- terra::rasterize(tem, covariates[[1]], field = "MapUnit1_num", 
                               filename = file.path(dsmart_dir, "simple", "site_series_simple.tif"), 
                               overwrite = TRUE)
  
  # Generate probabilities based on composition
  ss_calls <- unique(composition$MapUnit)
  composition_remake <- foreach(i = unique(composition$POLY_NO), .combine = rbind) %do% {
    composition %>% dplyr::filter(POLY_NO == i) %>% 
      dplyr::select(POLY_NO, MapUnit, proportion) %>% 
      rbind(data.frame(
        POLY_NO = .$POLY_NO[1], 
        MapUnit = ss_calls[!ss_calls %in% .$MapUnit],
        proportion = 0
      ))
  }
  
  tem_remake <- values(tem) %>% 
    dplyr::select(POLY_NO) %>% 
    merge(composition_remake) %>% 
    mutate(proportion = proportion / 100) %>% 
    merge(tem, .)
  
  tem_props <- foreach(i = unique(tem_remake$MapUnit), .combine = c) %do% {
    tem_remake[tem_remake$MapUnit == i] %>% 
      magrittr::set_names(c(names(.)[names(.) != "proportion"], paste0("probability_", i))) %>% 
      rasterize(covariates[[1]], field = paste0("probability_", i), overwrite = TRUE, 
                filename = file.path(dsmart_dir, "simple", paste0("probability_", i, ".tif")))
  }
}

```

The following section sets up some additional parameters for the DSAMRT algorithm. The args.model object specifies some of the modelling intricacies. Modelling will be passed on to the caret package where it will run a parallelized version of random forest analysis to generate class probabilities. 

There was a point in time that I tried to do the modelling via the mlr3 package since it has spatial capabilities; however, this proved to be a very difficult challenge and did not incur any speed benefit in terms of processing since the ranger package (used for the core modelling aspects) is written in C++ as it is. For those reasons, I did not implement the mlr3 modelling techniques into the DSMART algorithm.

```{r DSMART parameters}

# Note: for randomForest model method, you only need to have the mtry defined in 
# the tuneGrid part. Also note that if class probabilities are requested, the 
# value needs to be changed to TRUE and "type" in the disaggregate function 
# needs to be changed to "prob".
if(use_caret) {
  method.model <- "ranger"
  args.model <- list(
    trControl = trainControl(
      method = "repeatedcv", 
      number = 10, 
      repeats = 5, 
      classProbs = TRUE,
      returnData = FALSE,
      returnResamp = "final",
      savePredictions = "final",
      summaryFunction = multiClassSummary, # Adds a suite of performance metrics to the output
      allowParallel = TRUE),
    tuneGrid = expand.grid(
      mtry = floor(sqrt(nlyr(covariates))), # Default random forest options
      min.node.size = 1,
      splitrule = "gini"
    )
  )
} else {
  method.model <- NULL
  args.model <- NULL
}

```

Finally, we need to run the algorithm. This version of DSMART has implementation allowing the use of the caret package to make models, and that functionality is leveraged here. We will use a parallelized implementation of Random Forest modelling (i.e.: the "ranger" package) in order to predict site series or structural stage at each pixel. Even though I have rewritten this code to be faster using more updated packages, the algorithm still takes approximately 2 hours to run with the following parameters:

- Study area size of 122.507 km^2
- Pixel size of 4 m^2
- Using ~ 100 covariate raster datasets

Unless there are thousands of observations, the number of observations will not likely have an influence on the processing speed here. Beyond that, the way that DSMART works is as follows:

1. At each polygon unit, perform raster data extraction at each pixel. Afterwards, sample that dataset so that there are rate * reals (number) of samples per polygon. Note that when repeated cross validation is used, the number of model realisations drops down to 1 since the model is repeating itself anyway.

2. Once all samples are collected, add the remaining observations to each realisation of the model. Using that entire dataset, create a model (in this case, a repeated cross validation using the ranger and caret packages). That model is then used to generate map predictions using a tiling system implemented through the "stars" package, with tiles being mosaicked together at the end. This is implemented for both probability and raw classification methods: if probabilities are used, the realisation is a multi-layer raster file where each layer is a class probability, and if raw classification is used the realisation is a single layer raster file of predicted classes. Here, class probabilities are used as realisations.

3. With all realisations produced, compute class probabilities. If class probabilites were produced initially, the mean of a class probability is calculated across each realisation (unless there is only one realisation in which case probabilities are written out to single layer files for each class). If raw classes were produced, classes will be tabulated across each realisation and then probabilites are produced from those and written to individual raster files.

4. With the class probabilities produced, raw classifications are subsequently produced. Because certain areas may have overlap with what site series/structural stage it is, you can get a most probable layer, a second most probable layer, etc. At least 2 most probable layers are created (more may be specified, but in our case we just want the best one), and then a confusion layer (where classes disagree in an area) and Shannon index (where there are diverse classes) are created from calculations on the most probable layers. The final files include the most probable classification, a layer describing the probability of the most probable classification being correct, a confusion layer, and Shannon index layer. 

The files used in future modelling include the most probable classification layer, and the probability layers produced as well.

```{r DSMART run}

dsm <- try(dsmart(
  covariates = covariates, 
  polygons = polygons, 
  composition = composition,
  rate = 5, # How many samples to draw per polygon
  reals = 5, # How many models to produce from the modeling run? Gets overridden by number of repeats if caret package is used with repeated cross validation
  observations = observations, 
  method.sample = "by_polygon", 
  method.allocate = "weighted",
  method.model = method.model,
  args.model = args.model,
  outputdir = dsmart_dir, 
  factors = factors,
  type = "prob", 
  nprob = 1))

# Save the file to a better location
out <- rast(list.files(dsm$summarise$locations$mostprobable, 
                       pattern = "mostprob_01_class|mostprob_1_class", 
                       full.names = TRUE)) %>% 
  magrittr::set_names(paste0(run, "_dsmart")) %>% 
  writeRaster(filename = file.path(dsmart_dir, paste0(run, "_dsmart.tif")), 
              overwrite = TRUE, wopt = list(datatype = "INT2S"))

```

Sometimes when the function above is run, it will throw some sort of error. The error is meaningless and in the end he dsm object is still returned. 