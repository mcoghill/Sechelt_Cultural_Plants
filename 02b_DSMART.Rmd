---
title: "02b_DSMART"
author: "Matthew Coghill"
date: "2/19/2020"
output: html_document
---

This document details how to run the DSMART algorithm: Disaggregation and harmonisation of Soil MAp units through Resampled classification Trees. Originally, the algorithm was intended to create a model predicting soil classes; however, it's being used here to predict site series or structural stage classes across a landscape because the process has many parallels. Here, it will be used to convert a TEM to a raster based on the proportions of each polygon.

Additionally, this script assumes that all data manipulation and pre-processing has occurred prior to running (script 02a_PointData_Prep has been ran). The intention of this script was to be a standalone documentation of DSMART since it takes quite a while to run and produce a map.

First, libraries are loaded and data directories are defined. A custom function `dsmsart()` was created based on the existing `rdsmart` package (available on BitBucket). The intention of creating my own function was to improve upon the existing functions using newer, faster packages to accomplish the same tasks.

```{r Load Packages, echo=TRUE, results='hide'}

suppressMessages(suppressWarnings({
  lapply(c("tidyverse", "terra", "sf"), 
         library, character.only = TRUE)[0]}))

# Load custom DSMART files
source("./_functions/dsmart_custom/dsmart.R")

```

Next, file paths are defined. There are two major branches: First, what resolution should this be run at? Second, should this run be for site series or for structural stage? The option `include_bgc` will run stratified virtual sampling using the bec subzone as the stratified layer, and `use_caret` will indicate that the model to be used will be a random forest model using 10 fold cross validation repeated 5 times, instead of C5.0 tree models (base).

```{r Set directories}

AOI <- "Sechelt"
AOI_dir <- file.path(".", paste0(AOI, "_AOI"))
map_res <- 10
res_dir <- ifelse(is.numeric(map_res), paste0(map_res, "m"), map_res)

shapes_dir <- file.path(AOI_dir, "0_raw_inputs", "base_layers", res_dir)
covariate_dir <- file.path(AOI_dir, "1_map_inputs", "covariates", res_dir)

include_bgc <- FALSE
use_caret <- TRUE
run <- "site_ser" # option of "struct_stg" or "site_ser"
dsmart_dir <- file.path(AOI_dir, "1_map_inputs", "dsmart_2019", res_dir, run)

```

Next, important data is loaded. There are a lot of covariate layers created from DEM derivatives, satellite imagery, and climate data layers; however, in order to speed up processing, the amount of covariates are limited to only DEM derivatives, annual satellite imagery indices, and more recent annual climate data. These variables were chosen since it makes sense to model a landscape using more recent data in my mind, and not worry about older factors that may have influenced the landscape previously. Additionally, the climate variables are not much different between the 1961-1990 dataset and the 1981-2010 dataset, so autocorrelation effects may be induced if all of the climate variables were kept here. If a BGC layer is to be used to create the DSMART maps as well, then that is included as well.

```{r Load data}

# Load covariates
terrain_covariates <- grep(pattern = "normal|sentinel2", list.files(
  covariate_dir, full.names = TRUE, pattern = ".tif$"), invert = TRUE, 
  value = TRUE)
sentinel_covariates <- list.files(
  covariate_dir, pattern = "^sentinel2.*.2019.tif$", full.names = TRUE)
climate_covariates <- list.files(
  covariate_dir, pattern = "^normal_1981_2010y", full.names = TRUE)
covariates <- terra::rast(
  c(terrain_covariates, sentinel_covariates, climate_covariates))

# Preliminary check of covariate data to eliminate any outlying rasters
# containing less data than normal
cov_check <- sapply(names(covariates), function(x) {
  cat(paste0("\rCounting NA values in ", x, " [", 
             which(x == names(covariates)), 
             " of ", nlyr(covariates), "]\n"))
  unname(freq(subset(covariates, x) * 0)[, "count"])}) %>% 
  data.frame(data_cells = .) %>% 
  rownames_to_column("layer") %>% 
  dplyr::filter(data_cells >= 0.95 * median(.$data_cells))
covariates <- subset(covariates, cov_check$layer)

# Load the polygons and associated observations
polygons <- terra::vect(file.path(dsmart_dir, "inputs", paste0(
  "dsmart_", run, "_polys.gpkg")))

if(include_bgc) {
  bgc <- st_read(file.path(shapes_dir, "bec.gpkg"), quiet = TRUE) %>% 
    mutate(MAP_LABEL = as.factor(MAP_LABEL), 
           bgc = as.numeric(as.factor(MAP_LABEL))) 
  tem_geom <- st_geometry(bgc)
  
  # Fix geometries that aren't polygon/multipolygon
  for(i in 1:length(tem_geom)) {
    if(!st_geometry_type(tem_geom[[i]]) %in% c("POLYGON", "MULTIPOLYGON")) 
      tem_geom[[i]] <- st_collection_extract(tem_geom[[i]], "POLYGON") %>% 
        st_multipolygon()
  }
  st_geometry(bgc) <- st_cast(tem_geom, "MULTIPOLYGON")
  
  level_table <- unique.data.frame(data.frame(
    label = as.character(bgc$MAP_LABEL), 
    value = as.numeric(bgc$MAP_LABEL)))
  
  bgc_rast <- as.factor(terra::rasterize(vect(bgc), covariates[[1]], field = "bgc"))
  levels(bgc_rast) <- level_table
  
  # File needs to be written to disk and then reloaded back into R for proper 
  # feature assignments
  bgc_rast <- writeRaster(bgc_rast, file.path(dsmart_dir, "inputs", "bgc.tif"), 
                          overwrite = TRUE, wopt = list(datatype = "INT2S"))
  covariates <- c(covariates, bgc_rast)
  factors <- "bgc"
  
} else {
  factors <- NULL
}

# Load TEM composition and additional observations
composition <- read.csv(file.path(dsmart_dir, "inputs", paste0(
  "dsmart_", run, "_composition.csv"))) %>% 
    dplyr::select(-Subzone)
observations <- read.csv(file.path(dsmart_dir, "inputs", paste0(
  "dsmart_", run, "_observations.csv")))

```

During modelling of presence/absence and cover in future scripts, one of the questions that we had was if it was worth it to spend time creating this DSMART layer, or if we should just use the TEM. In order to seek out that questions, rasterized versions of the TEM are created here.

```{r Simple TEM rasterization}

if(run == "site_ser") {
  if(dir.exists(file.path(dsmart_dir, "simple")))
    unlink(file.path(dsmart_dir, "simple"), recursive = TRUE)
  dir.create(file.path(dsmart_dir, "simple"), showWarnings = FALSE)
  
  # Rasterize first calls only
  cov_lyr <- dplyr::slice_max(cov_check, data_cells, with_ties = FALSE) %>% 
    dplyr::pull(layer)
  cov_sub <- subset(covariates, cov_lyr)
  tem <- terra::vect(file.path(dsmart_dir, "inputs", paste0("dsmart_", run, "_polys.gpkg")))
  tem$MapUnit1_num <- as.numeric(as.factor(tem$MapUnit1))
  tem_rast <- terra::rasterize(tem, cov_sub, field = "MapUnit1_num", 
                               filename = file.path(dsmart_dir, "simple", "site_series_simple.tif"), 
                               overwrite = TRUE)
  
  tem_rast_index <- data.frame(name = tem$MapUnit1, code = tem$MapUnit1_num) %>% 
    unique() %>% 
    arrange(code)
  
  write.table(tem_rast_index, file.path(dsmart_dir, "simple", "site_ser_simple_lookup.txt"),
              row.names = FALSE, quote = FALSE, sep = ",")
  
  # Generate probabilities based on composition
  ss_calls <- unique(composition$MapUnit)
  composition_remake <- dplyr::bind_rows(lapply(unique(composition$POLY_NO), function(i) {
    composition %>% dplyr::filter(POLY_NO == i) %>% 
      dplyr::select(POLY_NO, MapUnit, proportion) %>% 
      rbind(data.frame(
        POLY_NO = .$POLY_NO[1], 
        MapUnit = ss_calls[!ss_calls %in% .$MapUnit],
        proportion = 0
      ))
  }))
  
  tem_remake <- values(tem) %>% 
    dplyr::select(POLY_NO) %>% 
    merge(composition_remake) %>% 
    mutate(proportion = proportion / 100) %>% 
    merge(tem, .)
    
  tem_props <- do.call(c, lapply(unique(tem_remake$MapUnit), function(i) {
    tem_remake[tem_remake$MapUnit == i] %>% 
      stats::setNames(c(names(.)[names(.) != "proportion"], paste0("probability_", i))) %>% 
      rasterize(cov_sub, field = paste0("probability_", i), overwrite = TRUE, 
                filename = file.path(dsmart_dir, "simple", paste0("probability_", i, ".tif")))
  }))
}

```

The following section sets up some additional parameters for the DSAMRT algorithm. The `args.model` object specifies some of the modelling intricacies. Modelling will be passed on to the caret package where it will run a parallelized version of random forest analysis to generate class probabilities. 

There was a point in time that I tried to do the modelling via the mlr3 package since it has spatial capabilities; however, this proved to be a very difficult challenge and did not incur any speed benefit in terms of processing since the ranger package (used for the core modelling aspects) is written in C++ as it is. For those reasons, I did not implement the mlr3 modelling techniques into the DSMART algorithm.

```{r DSMART parameters}

# Note: for randomForest model method, you only need to have the mtry defined in 
# the tuneGrid part. Also note that if class probabilities are requested, the 
# value needs to be changed to TRUE and "type" in the disaggregate function 
# needs to be changed to "prob".
if(use_caret) {
  library(caret, warn.conflicts = FALSE, verbose = FALSE, quietly = TRUE)
  method.model <- "ranger"
  args.model <- list(
    trControl = trainControl(
      method = "repeatedcv", 
      number = 10, 
      repeats = 5, 
      classProbs = TRUE,
      returnData = FALSE,
      returnResamp = "final",
      savePredictions = "final",
      summaryFunction = multiClassSummary, # Adds a suite of performance metrics to the output
      allowParallel = TRUE),
    tuneGrid = expand.grid(
      mtry = floor(sqrt(nlyr(covariates))), # Default random forest options
      min.node.size = 1,
      splitrule = "gini"
    )
  )
} else {
  library(C50, warn.conflicts = FALSE, verbose = FALSE, quietly = TRUE)
  method.model <- NULL
  args.model <- NULL
}

```

Finally, we need to run the algorithm. This version of DSMART has implementation allowing the use of the caret package to make models, and that functionality is leveraged here. We will use a parallelized implementation of Random Forest modelling (i.e.: the "ranger" package) in order to predict site series or structural stage at each pixel. Even though I have rewritten this code to be faster using more updated packages, the algorithm still takes approximately 2 hours to run with the following parameters:

- Study area size of 122.507 km^2
- Pixel size of 4 m^2
- Using ~ 100 covariate raster datasets

Unless there are thousands of observations, the number of observations will not likely have an influence on the processing speed here. Beyond that, the way that DSMART works is as follows:

1. At each polygon unit, perform raster data extraction at each pixel. Afterwards, sample that dataset so that there are rate * reals (number) of samples per polygon. Note that when repeated cross validation is used, the number of model realisations drops down to 1 since the model is repeating itself anyway.

2. Once all samples are collected, add the remaining observations to each realisation of the model. Using that entire dataset, create a model (in this case, a repeated cross validation using the ranger and caret packages). That model is then used to generate map predictions using a tiling system implemented through the "stars" package, with tiles being mosaicked together at the end. This is implemented for both probability and raw classification methods: if probabilities are used, the realisation is a multi-layer raster file where each layer is a class probability, and if raw classification is used the realisation is a single layer raster file of predicted classes. Here, class probabilities are used as realisations.

3. With all realisations produced, compute class probabilities. If class probabilites were produced initially, the mean of a class probability is calculated across each realisation (unless there is only one realisation in which case probabilities are written out to single layer files for each class). If raw classes were produced, classes will be tabulated across each realisation and then probabilites are produced from those and written to individual raster files.

4. With the class probabilities produced, raw classifications are subsequently produced. Because certain areas may have overlap with what site series/structural stage it is, you can get a most probable layer, a second most probable layer, etc. At least 2 most probable layers are created (more may be specified, but in our case we just want the best one), and then a confusion layer (where classes disagree in an area) and Shannon index (where there are diverse classes) are created from calculations on the most probable layers. The final files include the most probable classification, a layer describing the probability of the most probable classification being correct, a confusion layer, and Shannon index layer. 

The files used in future modelling include the most probable classification layer, and the probability layers produced as well.

```{r DSMART run}

dsm <- try(dsmart(
  covariates = covariates, 
  polygons = polygons, 
  composition = composition,
  rate = 5, # How many samples to draw per polygon
  reals = 5, # How many models to produce from the modelling run? Gets overridden by number of repeats if caret package is used with repeated cross validation
  observations = observations, 
  method.sample = "by_polygon", 
  method.allocate = "weighted",
  method.model = method.model,
  args.model = args.model,
  outputdir = dsmart_dir, 
  factors = factors,
  type = "prob", 
  nprob = 1, 
  mask = dplyr::slice_max(cov_check, data_cells, with_ties = FALSE) %>% 
    dplyr::pull(layer)))

# Save the file to a better location
out <- rast(list.files(dsm$summarise$locations$mostprobable, 
                       pattern = "mostprob_01_class|mostprob_1_class", 
                       full.names = TRUE)) %>% 
  stats::setNames(paste0(run, "_dsmart")) %>% 
  writeRaster(filename = file.path(dsmart_dir, paste0(run, "_dsmart.tif")), 
              overwrite = TRUE, wopt = list(datatype = "INT2S"))

```

Sometimes when the function above is run, it will throw some sort of error. The error is meaningless and in the end he dsm object is still returned. 
