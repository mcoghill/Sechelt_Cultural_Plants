---
title: "01a_DEM_SpatialLayer_Prep"
author: "Matthew Coghill"
date: "2/5/2020"
output: html_document
---
```{r global_options, include=FALSE }
require(knitr)
#knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
#                      echo=FALSE, warning=FALSE, message=FALSE)
#knitr::opts_knit$set(progress=TRUE, root.dir = 'relative_path_to_root_from_Rmd')
```

## Introduction
This document describes the steps required to prepare spatial layers to be used in machine-learning based predictive ecosystem map method. The code used here is largely pawned from PEM project coding, but with adjustments made to suit this project more accurately.

This method is scripted in R, but uses other open source packages, predominently SAGA :  (https://sourceforge.net/projects/saga-gis/). 


###Preparation steps. 

### Software
Prior to running this script you will need to download SAGA[https://sourceforge.net/projects/saga-gis/] and know its location. SAGA can be downloaded on an external drive and run independent of pemissions. Additionally, a source script may be called in R to install it to your system.

### Data 
To run this script you will need a base digital elevation model (digital terrain model) at 1m resolution. 


### 1) Set up link between Saga and R. 
This is taken care of with the saga_cmd function

### Matt's Notes
The purpose of this document is to derive desired lower resolution DEM's from a 1m DEM, and subsequently create covariate layers for the PEM project. This document uses three coding languages: R, cmd (aka batch), and XML. Command line functions will call individual functions to SAGA, while the XML scripts allow for "chaining" tools together for more efficient processing in SAGA so that SAGA doesn't continuously stop and start. It also prevents the creation of intermediate files. The big benefit of using SAGA is that it automatically uses all cores on the machine you're using. In this manner, SAGA will utilize available RAM on your machine for storing files until the very end when they are written to the disk so in some instances you may be limited in its use depending on the specs of your machine.

Before getting started, the first thing to do is to load the required packages. Most of the processing work will actually be accomplished by SAGA GIS, thus there are only a few packages required here.

```{r Load Packages, include=FALSE}

ls <- c("tidyverse", "raster", "xml2", "sf", "lubridate", "foreach", "rvest", "httr", "tools", "terra")
new_packages <- ls[!(ls %in% installed.packages()[, "Package"])]
if(length(new_packages)) install.packages(new_packages)
lapply(ls, library, character.only = TRUE)[0]  # load the required packages
rm(ls, new_packages)

source(file.path('_functions', 'get_saga.R'))

```

Run the below function to detect and install SAGA GIS if necessary for your system

```{r download SAGA, eval = FALSE}
# NOTE: New function get_saga() will detect your version of SAGA gis and if it's
# too old (< 7.3.0), it will download and install 7.3.0 (latest lts)

saga_cmd <- get_saga()

```

In this next chunk, you enter the only input data required for this sheet to function, the rest is (mostly) automated! Simply input the AOI, the choice of resolutions you want outputted, and a projection code.

```{r R Data Objects, include=FALSE}

AOI <- "Sechelt"
AOI.dir <- file.path(".", paste0(AOI, "_AOI"))
epsg <- 3005

```

In the next chunk, we want to specify where the raw files are located. For the Sechelt project, there were two output LiDAR files: a CHM and a DEM, both at resolutions of (almost) 1m. Once the raw LiDAR files have been located, they are transformed to the larger of the two extents using the provided EPSG code for transformation. After grid transformation, the two files should stack properly. There is a test at the bottom of the chunk to see if they will properly stack, and if they don't then a closer look at the files will need to be made in order to determine why they aren't stacking.

```{r Specify 1m Raster}

covariate_file_path <- file.path(AOI.dir, "1_map_inputs", "covariates")
cov_1m <- file.path(covariate_file_path, "1m")
dir.create(cov_1m, recursive = TRUE, showWarnings = FALSE)

# Load files
dem_raw <- rast(file.path(AOI.dir, "_original_files", "dem", "w001001.adf"))
chm_raw <- rast(file.path(AOI.dir, "_original_files", "chm", "w001001.adf"))

# Want to use BC Albers projection, so transform that. Also, the resolution and
# extent for both the dem and chm are not the same so they will both need to be 
# reset to have the correct resolution, crs, and extents. Want to reproject the
# smaller raster to the larger one so that no data is accidentally lost
# if(ncell(dem_raw) >= ncell(chm_raw)) {
#   
#   system2(saga_cmd, paste(
#     "pj_proj4 4", 
#     "-SOURCE", file.path(AOI.dir, "_original_files", "dem", "w001001.adf"),
#     "-CRS_PROJ4", shQuote(projection(CRS(paste0("+init=epsg:", epsg)))), 
#     "-TARGET_DEFINITION 0", 
#     "-TARGET_USER_SIZE 1", 
#     "-GRID", file.path(AOI.dir, "_original_files", "dem", "dem")
#   ))
#   system2(saga_cmd, paste(
#     "pj_proj4 4", 
#     "-SOURCE", file.path(AOI.dir, "_original_files", "chm", "w001001.adf"), 
#     "-CRS_PROJ4", shQuote(projection(CRS(paste0("+init=epsg:", epsg)))),
#     "-TARGET_DEFINITION 1", 
#     "-TARGET_TEMPLATE",  file.path(AOI.dir, "_original_files", "dem", "dem.sdat"), 
#     "-GRID", file.path(AOI.dir, "_original_files", "chm", "chm")
#   ))
#   
# } else if(ncell(dem_raw) < ncell(chm_raw)) {
#   
#   system2(saga_cmd, paste(
#     "pj_proj4 4", 
#     "-SOURCE", file.path(AOI.dir, "_original_files", "chm", "w001001.adf"),
#     "-CRS_PROJ4", shQuote(projection(CRS(paste0("+init=epsg:", epsg)))), 
#     "-TARGET_DEFINITION 0", 
#     "-TARGET_USER_SIZE 1", 
#     "-GRID", file.path(AOI.dir, "_original_files", "chm", "chm")
#   ))
#   system2(saga_cmd, paste(
#     "pj_proj4 4", 
#     "-SOURCE", file.path(AOI.dir, "_original_files", "dem", "w001001.adf"), 
#     "-CRS_PROJ4", shQuote(projection(CRS(paste0("+init=epsg:", epsg)))),
#     "-TARGET_DEFINITION 1", 
#     "-TARGET_TEMPLATE",  file.path(AOI.dir, "_original_files", "chm", "chm.sdat"), 
#     "-GRID", file.path(AOI.dir, "_original_files", "dem", "dem")
#   ))
#   
# }
# 
# writeRaster(rast(file.path(AOI.dir, "_original_files", "dem", "dem.sdat")), 
#             file.path(covariate_file_path, "1m", "dem.tif"), overwrite = TRUE)
# writeRaster(rast(file.path(AOI.dir, "_original_files", "chm", "chm.sdat")), 
#             file.path(covariate_file_path, "1m", "chm.tif"), overwrite = TRUE)

if(ncell(dem_raw) >= ncell(chm_raw)) {
  in_1 <- "dem"; in_2 <- "chm"
} else {
  in_1 <- "chm"; in_2 <- "dem"
}
saga_ver <- system2(saga_cmd, "-v", stdout = TRUE)
saga_ver <- unlist(regmatches(saga_ver, regexec("Version:\\s*(.*?)\\s*\\(", saga_ver)))[2]
tool_list <- list(
  header = paste0(
    "<?xml version='1.0' encoding='UTF-8'?>
          <toolchain saga-version='", saga_ver, "'>
          <group>toolchains</group>
          <identifier>reproject_original</identifier>
          <name>Reproject and Save Original Grids (one step)</name>
          <description>
            Common DEM derivatives in SAGA GIS
          </description>
        
          <parameters>
            <option varname='GRID_SYSTEM' type='grid_system'>
              <name>Grid System</name>
            </option>
            <option varname='TRANS_SYSTEM' type='grid_system'>
              <name>Grid System</name>
            </option>
            <input varname='dem' type='grid' parent='GRID_SYSTEM'>
                <name>DEM</name>
            </input>
            <input varname='chm' type='grid' parent='TRANS_SYSTEM'>
                <name>CHM</name>
            </input>
      </parameters>
          <tools>", sep = "\n"),
  transform_grid_1 = paste0(
    "<tool library='pj_proj4' tool='4' name='Coordinate Transformation (Grid)'>
        <option id='CRS_PROJ4'>", projection(CRS(paste0("+init=epsg:", epsg))), "</option>
        <input id='SOURCE'>", in_1, "</input>
        <option id='RESAMPLING'>3</option>
        <option id='TARGET_DEFINITION'>0</option>
        <option id='TARGET_USER_SIZE'>1</option>
        <output id='GRID'>", paste0(in_1, "_out"), "</output>
      </tool>"
  ), 
  transform_grid_2 = paste0(
    "<tool library='pj_proj4' tool='4' name='Coordinate Transformation (Grid)'>
        <option id='CRS_PROJ4'>", projection(CRS(paste0("+init=epsg:", epsg))), "</option>
        <input id='SOURCE'>", in_2, "</input>
        <option id='RESAMPLING'>3</option>
        <option id='TARGET_DEFINITION'>1</option>
        <input id='TARGET_TEMPLATE'>", paste0(in_1, "_out"), "</input>
        <output id='GRID'>", paste0(in_2, "_out"), "</output>
      </tool>"
  ),
  export_1 = paste0(
    "<tool library='io_gdal' tool='2' name='Export GeoTIFF'>
        <input id='GRIDS'>dem_out</input>
        <option id='FILE'>", file.path(AOI.dir, "_original_files", "dem", "dem.tif"), "</option>
      </tool>"
  ), 
  export_2 = paste0(
    "<tool library='io_gdal' tool='2' name='Export GeoTIFF'>
        <input id='GRIDS'>chm_out</input>
        <option id='FILE'>", file.path(AOI.dir, "_original_files", "chm", "chm.tif"), "</option>
      </tool>"
  ),
  footer = paste0(
    "</tools>
      </toolchain>"
  )
)
call <- paste0(tool_list, sep = "\n", collapse = " ")

# Determine the toolchain directory based on your system
xml_dir <- ifelse(
  Sys.info()[["sysname"]] == "Windows", 
  file.path(dirname(saga_cmd), "tools", "toolchains", "transform_original.xml"), 
  file.path(dirname(dirname(saga_cmd)), "share", "saga", 
            "toolchains", "transform_original.xml")
)
write_xml(read_xml(call), xml_dir)
sys_cmd <- paste("toolchains reproject_original", 
                 "-dem", file.path(AOI.dir, "_original_files", "dem", "w001001.adf"), 
                 "-chm", file.path(AOI.dir, "_original_files", "chm", "w001001.adf"))
system2(saga_cmd, sys_cmd)

lidar <- rast(list.files(file.path(AOI.dir, "_original_files"), 
                         pattern = "chm.tif|dem.tif", 
                         full.names = TRUE, recursive = TRUE))

crs(lidar) <- projection(CRS(paste0("+init=epsg:", epsg)))

for(i in names(lidar)) {
  writeRaster(subset(lidar, i), 
              file.path(cov_1m, paste0(i, ".tif")), 
              overwrite = TRUE)
}

```


This next chunk is a function which will start the pre-processing of a 1m resolution DEM into your desired DEM resolutions. This is following the examples set out by Lucas Jarron and Nicholas Coops, except this time R will be used to call SAGA GIS functions rather than using ArcGIS or QGIS.

First, focal statistics are computed in a defined area around the focal point (the mean values in a defined area). SAGA calculates these focal statistics based on a radius away from the focal point, thus the resolution of choice is divided by 2 and rounded to the nearest integer. For a radius of 1, it is calculating the mean statistics (elevation) in a 3x3 grid, a radius of 2 will be a 5x5 grid, etc., but the point here is that processing time will increase at coarser resolutions. No weighting is given here since the elevations are given in raw form, so everything is left to the default settings. Source: http://www.saga-gis.org/saga_tool_doc/7.4.0/statistics_grid_1.html

Next, an empty grid is created using the extent of the smoothed DEM and a cell size (resolution) defined by the choices. The grid only contains values of -99999 which are considered "no data" cells in SAGA. Source: http://www.saga-gis.org/saga_tool_doc/7.4.0/grid_tools_23.html

Next, points are placed in the middle of these grid cells using "Grid values to Points". It takes each cell and assigns a point with the same data as the cell (which will be "no data" on the empty grid). This allows for points to be spatially joined later on at the specified cell resolution. This step can take a while depending on hardware and raster cell size. Source: http://www.saga-gis.org/saga_tool_doc/7.4.0/shapes_grid_3.html

Now that we have points where we want them, they can be attributed using the "Add grid values to shapes" tool. Overlaying the points on the smoothed DEM, the elevations are extracted to the point data. Source: http://www.saga-gis.org/saga_tool_doc/7.4.0/shapes_grid_1.html

Resampling was left to the default setting of B-spline interpolation. I'm not familiar with the different resampling techniques but I've seen this used widely for floating point data so I felt it was fine to leave it as it is.

The resulting DEM finally cropped to the data in case there was any shrinkage occurring during the processing. If there was, it saves it as a separate geotiff file.

The XML code is processed and written to the "<SAGA_PATH>/tools/toolchains" folder. Each time it loops through for a different resolution, some of the parameters are recalculated based on the desired resolution and the XML file gets re-written. SAGA is called after the XML file is written to process the toolchain XML, and all output DEM's will be located in the DEM folder.

```{r Derive Lower Resolutions, include=FALSE}

lower_resolutions <- function(
  grid_1m = NULL, 
  out_path = NULL, 
  res_choices = NULL,
  saga_cmd = NULL
) {
  
  # Data input checks
  if(class(grid_1m) %in% "RasterLayer") {
    reference <- grid_1m
    grid_1m <- reference@file@name
  } else if(is.character(grid_1m) && length(grid_1m) == 1) {
    reference <- rast(grid_1m)
  } else {
    stop(paste(
      "\rError: 'grid_1m' must either be a character string to a valid raster 
      \rfile, or a valid raster layer brought in by the 'raster' package"
    ))
  }
  
  if(is.null(out_path))
    out_path <- dirname(dirname(grid_1m))
  
  if(!is.character(out_path) && length(out_path) != 1)
    stop(
      "\r'out_path' must be a character string to a folder on your system"
    )
  
  dir.create(out_path, showWarnings = FALSE)
  
  if(!file.exists(saga_cmd)) 
    stop("\rPlease provide a valid path to 'saga_cmd'")
  
  saga_ver <- system2(saga_cmd, "-v", stdout = TRUE)
  saga_ver <- unlist(regmatches(saga_ver, regexec("Version:\\s*(.*?)\\s*\\(", saga_ver)))[2]
  
  out_grids <- foreach(r = res_choices, .combine = c) %do% {
    covariate_out_path <- file.path(out_path, paste0(r, "m"))
    dir.create(covariate_out_path, showWarnings = FALSE)
    tool_list <- list(
      # Different option names for this tool based on SAGA version
      focal_stats = paste0(
        "<tool library='statistics_grid' tool='1' name='Focal Statistics/Residual Analysis'>
           <input id='GRID'>", basename(file_path_sans_ext(grid_1m)), "</input>
           <output id='MEAN'>grid_smooth</output>
           <option id='BCENTER'>true</option>",
        ifelse(
          compareVersion(saga_ver, "7.3.0") <= 0, 
          paste0("<option id='MODE'>0</option>
       	   <option id='RADIUS'>", round(r / 2), "</option>"), 
          paste0("<option id='KERNEL_TYPE'>0</option>
       	   <option id='KERNEL_RADIUS'>", round(r / 2), "</option>")),
        "</tool>"
      ),
      create_grid = paste0(
        "<tool library='grid_tools' tool='23' name='Create Grid System'>
           <output id='GRID'>grid_empty</output>
           <option id='INIT'>-99999.000000</option>
           <option id='M_EXTENT'>3</option>
           <option id='ADJUST'>0</option>
           <option id='USEOFF'>false</option>
    	     <option id='CELLSIZE'>", r, "</option>
           <input id='GRIDLIST'>grid_smooth</input>
         </tool>"
      ), 
      grid_to_points = paste0(
        "<tool library='shapes_grid' tool='3' name='Grid Values to Points'>
           <output id='SHAPES'>points_empty</output>
           <option id='NODATA'>false</option>
           <option id='TYPE'>0</option>
           <input id='GRIDS'>grid_empty</input>
         </tool>"
      ), 
      smooth_values_to_points = paste0(
        "<tool library='shapes_grid' tool='1' name='Add Grid Values to Shapes'>
          <output id='RESULT'>points_filled</output>
           <option id='RESAMPLING'>3</option>
           <input id='SHAPES'>points_empty</input>
           <input id='GRIDS'>grid_smooth</input>
         </tool>"
      ),
      points_to_grid = paste0(
        "<tool library='grid_gridding' tool='0' name='Shapes to Grid'>
           <output id='GRID'>grid_downscaled</output>
           <option id='FIELD'>Mean Value</option>
           <option id='TARGET_DEFINITION'>1</option>
           <input id='TARGET_TEMPLATE'>grid_empty</input>
           <input id='INPUT'>points_filled</input>
         </tool>"
      ), 
      crop_grid = paste0(
        "<tool library='grid_tools' tool='17' name='Crop to Data'>
           <output id='OUTPUT'>", toupper(basename(file_path_sans_ext(grid_1m))), "</output>
           <input id='INPUT'>grid_downscaled</input>
         </tool>"
      )
    )
    
    # End list of tools, don't adjust code after here
    #############################################################################
    
    tools <- lapply(tool_list, function(x) {
      inputs = gsub(".*>", "", regmatches(
        x, gregexpr("(?<=<input).*?(?=</input>)", x, perl = TRUE))[[1]])
      outputs = unique(gsub(".*>", "", regmatches(
        x, gregexpr("(?<=<output).*?(?=</output>)", x, perl = TRUE))[[1]]))
      tc = sum(length(outputs))
      
      tibble(tool = x, inputs = list(inputs), outputs = list(outputs), tc = tc)
    })
    
    # Dynamically split processing based on the amount of available RAM a PC has.
    # The following code will determine the best way to process the rasters
    # based on the amount of RAM on a users machine. It does this by summing the
    # number of inputs and outputs and comparing them to the maximum number of 
    # files your system can hold based on the size of the input DEM.
    raster_size <- file.info(grid_1m)$size / 1024 ^ 2 
    raster_size_deriv <- raster_size / (r ^ 2)
    raster_memory <- (raster_size * 2) + (raster_size_deriv * 3)
    memory <- memory.limit() - raster_memory - 1024 # Leave ~1GB of RAM available?
    if(memory > (memory.limit() - 1024)) {
      n_files <- floor(memory.limit() - 1024 / raster_size)
    } else n_files <- Inf
    
    # Adjust tools list
    for(i in 1:length(tools)) {
      try({
        if(tools[[i]]$tc < n_files) {
          while(sum(tools[[i]]$tc, tools[[i + 1]]$tc) < n_files) {
            y <- rbind(tools[[i]], tools[[i + 1]])
            y[1, "tc"] <- sum(y$tc)
            y[1, "tool"] <- paste(y$tool, collapse = "\n")
            y[1, "inputs"][[1]] <- list(unlist(y$inputs))
            y[1, "outputs"][[1]] <- list(unlist(y$outputs))
            y <- y[1, ]
            tools[[i]] <- y
            tools[[i + 1]] <- NULL
          }
        }
      }, silent = TRUE)
    }
    
    # Fix inputs and outputs columns, add function to call the whole thing in cmd
    xml_layout <- lapply(tools, function(x) {
      x$inputs <- list(
        unique(unlist(x$inputs)[!unlist(x$inputs) %in% unlist(x$outputs)])
      )
      
      x$input_xml <- foreach(i = unlist(x$inputs), .combine = paste) %do% {
        if(startsWith(i, "points")) {
          paste0("<input varname='", i, "' type='shapes' parent='GRID_SYSTEM'>
          <name>", i, "</name>
      </input>\n", collapse = " ")
        } else {
          paste0("<input varname='", i, "' type='grid' parent='GRID_SYSTEM'>
          <name>", i, "</name>
      </input>\n", collapse = " ")
        }
      }
      
      if(n_files == Inf) {
        x$outputs <- paste0(
          "<tool library='io_gdal' tool='2' name='Export GeoTIFF'>
              <input id='GRIDS'>grid_downscaled</input>
              <option id='FILE'>", file.path(covariate_out_path, "grid_downscaled.tif"), "</option>
            </tool>\n", 
          "<tool library='io_gdal' tool='2' name='Export GeoTIFF'>
              <input id='GRIDS'>", toupper(basename(file_path_sans_ext(grid_1m))), "</input>
              <option id='FILE'>", file.path(covariate_out_path, basename(grid_1m)), "</option>
            </tool>\n",
          collapse = " ")
      } else {
        x$outputs <- foreach(k = unlist(x$outputs), .combine = paste) %do% {
          if(startsWith(k, "points")) {
            paste0(
              "<tool library='io_gdal' tool='4' name='Export Shapes'>
              <input id='SHAPES'>", k, "</input>
              <option id='FILE'>", file.path(covariate_out_path, paste0(k, ".shp")), "</option>
              <option id='FORMAT'>7</option>
            </tool>\n", collapse = " ")
          } else {
            paste0(
              "<tool library='io_gdal' tool='2' name='Export GeoTIFF'>
              <input id='GRIDS'>", k, "</input>
              <option id='FILE'>", file.path(covariate_out_path, paste0(k, ".tif")), "</option>
            </tool>\n", collapse = " ")
          }
        }
      }
      
      x$header <- paste0(
        "<?xml version='1.0' encoding='UTF-8'?>
          <toolchain saga-version='", saga_ver, "'>
          <group>toolchains</group>
          <identifier>Downscale</identifier>
          <name>Derived Layers (one step)</name>
          <description>
            Common DEM derivatives in SAGA GIS
          </description>
        
          <parameters>
            <option varname='GRID_SYSTEM' type='grid_system'>
              <name>Grid System</name>
            </option>
            ", x$input_xml,
          "</parameters>
          <tools>", sep = "\n")
      
      x$footer <- paste0(
        "</tools>
      </toolchain>"
      )
      
      x$call <- paste(x$header, x$tool, x$outputs, x$footer, sep = "\n")
      return(x)
    })
    
    # Define text for cmd input
    cmd_text <- lapply(xml_layout, function(x) {
      foreach(k = unlist(x$inputs), .combine = paste) %do% {
        if(startsWith(k, "points")) {
          paste0("-", k, " ", file.path(covariate_out_path, paste0(k, ".shp")), collapse = " ")
        } else if(startsWith(k, basename(file_path_sans_ext(grid_1m)))) {
          paste0("-", k, " ", grid_1m, collapse = " ")
        } else {
          paste0("-", k, " ", file.path(covariate_out_path, paste0(k, ".tif")), collapse = " ")
        }
      }
    })
    
    # Determine the toolchain directory based on your system
    lower_res_xml <- ifelse(
      Sys.info()[["sysname"]] == "Windows", 
      file.path(dirname(saga_cmd), "tools", "toolchains", "raster_downscale.xml"), 
      file.path(dirname(dirname(saga_cmd)), "share", "saga", 
                "toolchains", "raster_downscale.xml")
    )
    
    # Process SAGA toolchains in the list
    for(p in 1:length(xml_layout)) {
      write_xml(read_xml(xml_layout[[p]]$call), lower_res_xml)
      sys_cmd <- paste("toolchains Downscale", cmd_text[[p]])
      system2(saga_cmd, sys_cmd)
    }
    
    # Remove intermediate files
    if(file.exists(file.path(covariate_out_path, toupper(basename(grid_1m))))) {
      pattern <- "grid_smooth|grid_empty|points_empty|points_filled|grid_downscaled"
    } else pattern <- "grid_smooth|grid_empty|points_empty|points_filled"
    
    unlink(list.files(
      covariate_out_path, 
      pattern = pattern,
      full.names = TRUE)
    )
    
    output <- rast(list.files(
      covariate_out_path, 
      pattern = paste(basename(grid_1m), "grid_downscaled.tif", sep = "|"),
      full.names = TRUE, ignore.case = TRUE))
    
    file.rename(output, file.path(dirname(output), tolower(basename(output))))
    list.files(covariate_out_path, pattern = basename(grid_1m), full.names = TRUE)
  }
  return(out_grids)
}

```

Below is the function which generates all SAGA covariate layers used in the PEM project. The first part of this chunk is the XML code for creating the tool chain. Within the function, this only needs to be ran once since it gets changed based on resolution with a much simpler function below it.

Certain parameters are defined within the toolchain which are important for some of the SAGA layers in order to keep things relatively efficient as well as accurate. These parameters include:

scale_param: This defines the TRI (ruggedness) search area and channel network length. Essentially, the function scales up the resulting value at low resolutions and scales it down at higher resolutions. Since ruggedness looks at an area around the focal cell, it's important to scale this based on resolution since differences will be miniscule at low resolutions, but more dramatic at higher resolutions. This is also used in the "channel network" grid tool to define how long a channel needs to be before it is no longer considered a channel, thus at high resolutions channels need to be continuous for a longer segment before they are considered a channel.

mrvbf_param: This defines the "initial threshold for slope" parameter in the MRVBF algorithm. The math is based on the paper that this originally came from as well as a very helpful infographic retrieved from here: https://www.nrcs.usda.gov/wps/PA_NRCSConsumption/download?cid=stelprdb1258050&ext=pdf 

tpi_param: This defines a search radius around a cell. I figured since it has to do with topographic position, search radius should be the same value in cells at each scale. This will create different results at each resolution as well which might help indicate why certain resolutions are more important than others.

openness_param: This parameter is for using the multi-scaled approach to defining openness. I found through exhaustive testing that this parameter is best defined by half the number of columns, which produced the best looking (least blocky) result out of the plethora of other trials ran.

The workflow for this function is as follows:
1. Script the XML code used for the SAGA toolchain which includes the following tools:

1. Fill Sinks XXL (Wang & Liu): For filling sinks in the DEM and only returning a DEM
2. Slope, Aspect, Curvature: returns Slope, Aspect, General Curvature, and Total Curvature rasters
3. Flow Accumulation (Recursive): Creates the "total catchment area" (TCA) intermediate layer. This layer is only used in the generation of the TWI, and not in the final rasterstack.
4. Topographic wetness index (TWI): Creates the TWI for the study area. I found these to be the best methods based on exhaustive testing of many other methods. Rather than using the specific catchment area, TCA is used and converted all at once within this tool thus it is more efficient. My methods are based on those found here: https://gracilis.carleton.ca/CUOSGwiki/index.php/Enhanced_Wetness_Modelling_in_SAGA_GIS 
5. Channel Network: This creates a channel network grid using the filled DEM and the TCA layer as an initiation grid. I found that using the initiaion value of 1,000,000 worked to create channels consistently at all scales, thus I propose using that here as well. scale_param is used here to define how long a stream needs to be in pixels before it is considered a stream. Source: https://sourceforge.net/projects/saga-gis/files/SAGA%20-%20Documentation/SAGA%20Documents/SagaManual.pdf/download 
Note: This is an intermediate layer and is not used in the final rasterstack
6. Overland Flow Distance to Channel Network: This tool draws the overland flow distance to the channel network created in step 5. In order to create a raster that is used to the borders of the grid, the "boundary" option had to be set to "true" This may not give a realistic representation of this variable at a given study area, but without this it constricts the final grid.
7. Multiresolution Index of Valley Bottom Flatness (MRVBF): Looks at valley bottom flatness and ridge top flatness and uses the mrvbf_param to define the "initial threshold for slope" parameter. The input DEM's used from here all use the original, unfilled DEM.
8. Terrain Ruggedness Index (TRI): Looks at ruggedness at a specified distance away from a focal cell.
9. Convergence index: This tool is unchanged from the parameters defined by Lucas and Nicholas (uses 3x3 gradient to determind convergence)
10. Topographic Openness: Calculates openness across a landscape at multiple scales. The advantage to the multi-scale approach is that it calculates the values at the edges of the map, whereas the line-tracing method does not. Multi-scale keeps things more consistent I would wager. This outputs both negative and positive openness.
11. Diurnal Anisotropic Heat: The settings are unchanged from the default.
12. Topographic Position Index (TPI): This calculates the position of a cell relative to neighboring cells in a defined vicinity. This value could easily change depending on the scale of interest. When running this tool, it pastes the default search parameters; however, it actually runs the ones defined by the tpi_param code. This is a bug from SAGA, but has no effect on the outcome of the TPI, it outputs it properly.
13. Potential Incoming Solar Radiation: This calculates direct and diffuse insolation from a range of days spanning a whole year. I've restricted the time from between 3:30am and 8:30pm which encompasses the earliest sunrise and latest sunset of 2018 in the Kamloops area, rounded to the nearest 30 minutes. Source: https://www.nrc-cnrc.gc.ca/eng/services/sunrise/advanced.html
Note: It would be good to automate that step somehow.
14. Change Grid Value: Changes the grid values of 0 to -99999 (i.e.: no data) for the insolation grids
15. Export Raster: The final output(s)

This function requires that you specify an input file path to a DEM, not an input raster (same for all functions in this document).


```{r DEM Derived Layers, include=FALSE}

dem_derived_layers <- function(
  dem_input = NULL, # Input DEM either as file path or raster layer
  out_path = NULL,  # output file path, will default to dem_input directory
  saga_cmd = NULL # path to saga_cmd
) {
  # Note: All SAGA tools and associated documentation can be found here:
  # http://www.saga-gis.org/saga_tool_doc/7.3.0/index.html
  
  # Data input checks
  if(class(dem_input) %in% "RasterLayer") {
    reference <- dem_input
    dem_input <- reference@file@name
  } else if(is.character(dem_input) && length(dem_input) == 1) {
    reference <- raster(dem_input)
  } else {
    stop(paste(
      "\rError: 'dem_input' must either be a character string to a valid raster 
      \rfile, or a valid raster layer brought in by the 'raster' package"
    ))
  }
  
  if(is.null(out_path))
    out_path <- dirname(dem_input)
  
  if(!is.character(out_path) && length(out_path) != 1)
    stop(
      "\r'out_path' must be a character string to a folder on your system"
    )
  
  dir.create(out_path, showWarnings = FALSE)
  
  if(!file.exists(saga_cmd)) 
    stop("\rPlease provide a valid path to 'saga_cmd'")
  
  # Define inputs used in some of the SAGA tools below
  reference_res <- res(reference)[1]
  covariate_out <- out_path
  dir.create(covariate_out, recursive = TRUE, showWarnings = FALSE)
  
  ## Sunrise/sunset time calculation for solar radiation tool:
  centroid <- data.frame(x = mean(xmax(reference), xmin(reference)),
                         y = mean(ymax(reference), ymin(reference))) %>%
    st_as_sf(coords = c("x", "y"), crs = crs(reference)) %>%
    st_transform(4326) %>% # needs lat/long for calculating sunlight times
    st_coordinates() %>%
    as.data.frame() %>%
    dplyr::rename_at(vars(names(.)), ~c("Longitude", "Latitude"))
    
  # Using rvest to gather sunset/sunrise information
  session <- html_session(
    "https://www.nrc-cnrc.gc.ca/eng/services/sunrise/advanced.html")
  form <- html_form(session)
  form <- form[lapply(form, function(x) x[["name"]]) == "sunrise-sunset"][[1]]
  filled_form <- set_values(
    form, calcSubject = "ss-year-txt", calcMethod = "byLatLong",
    calcDate = "01/01/2019", latitude = as.character(centroid$Latitude), 
    longitude = as.character(centroid$Longitude),
    timezone = "PST|8"
  )
  filled_form$url <- session$url
  
  result <- submit_form(session, filled_form)
  response <- sub(".*Date?", "Date", content(result$response))
  
  timetable <- read_csv(response) %>% 
    na.omit() %>%
    mutate(Month = match(trimws(gsub("[[:digit:]]+|/", "", Date)), month.abb),
           Day = readr::parse_number(Date)) %>%
    mutate(rise = as_datetime(paste0(
      "2019-", Month, "-", Day, " ", hms(`Sun rise`)), tz = "Canada/Pacific"),
      set = as_datetime(paste0(
        "2019-", Month, "-", Day, " ", hms(`Sun set`)), tz = "Canada/Pacific")) %>%
    mutate(rise = floor_date(rise, "30 minutes"),
           set = ceiling_date(set, "30 minutes")) %>%
    mutate(rise = strftime(rise, format = "%H:%M:%S"),
           set = strftime(set, format = "%H:%M:%S"))
  
  min_sunrise <- min(timetable$rise)
  max_sunset <- max(timetable$set)
  min_sunrise_numeric <- as.numeric(hms(min_sunrise)) / 3600
  max_sunset_numeric <- as.numeric(hms(max_sunset)) / 3600
  
  time_res <- (max_sunset_numeric - min_sunrise_numeric) / 4
  
  ## Scaling parameter for a few of the tools:
  scale_param <- ifelse(
    round((reference_res*(25 / reference_res)) / reference_res) == 0, 1, 
    round((reference_res*(25 / reference_res)) / reference_res)
  )
  
  ## Tool specific parameters
  mrvbf_param <- 116.57 * (reference_res ^ -0.62)
  tpi_param <- reference_res * 5
  openness_param <- as.numeric(ncol(reference) / 2)
  
  #############################################################################
  # Build the xml toolchain piece by piece (easier than previously)
  
  # Does this by first creating a list of tools and their associated parameters
  # If a tool is not going to be ran, it can simply be commented out/deleted, and
  # that's it, no mucking around anywhere else! 
  
  # Note: There are multiple ways to fill sinks for a given DEM. The way 
  # currently coded here may not be the most efficient but it produces a 
  # sinkroute layer which is used in multiple tools later on and appears to be 
  # the most accurate.
  tool_list <- list(
    fill_sinks = paste0(
      "<tool library='ta_preprocessor' tool='1' name='Sink Drainage Route Detection'>
        <input  id='ELEVATION'>dem</input>
        <output id='SINKROUTE'>sinkroute</output>
    </tool>
    <tool library='ta_preprocessor' tool='2' name='Sink Removal'>
        <input  id='DEM'>dem</input>
        <input  id='SINKROUTE'>sinkroute</input>
        <output id='DEM_PREPROC'>dem_preproc</output>
        <option id='METHOD'>1</option>
        <option id='THRESHOLD'>0</option>
    </tool>"
    ),
    
    #    Can script more output curvature types, see:
    # http://www.saga-gis.org/saga_tool_doc/7.3.0/ta_morphometry_0.html
    slope = paste0(
      "<tool library='ta_morphometry' tool='0' name='Slope, Aspect, Curvature'>
        <input id='ELEVATION'>dem_preproc</input>
        <output id='SLOPE'>slope</output>
        <output id='ASPECT'>aspect</output>
        <output id='C_GENE'>gencurve</output>
        <output id='C_TOTA'>totcurve</output>
        <option id='METHOD'>6</option>
        <option id='UNIT_SLOPE'>0</option>
        <option id='UNIT_ASPECT'>0</option>
    </tool>"
    ),
    
    #   Following this method for calculating topographic wetness index:
    #    https://gracilis.carleton.ca/CUOSGwiki/index.php/Enhanced_Wetness_Modelling_in_SAGA_GIS
    #    See this paper as well for discussion on different ways to calculate TWI:
    #    https://link.springer.com/article/10.1186/s40965-019-0066-y
    flow_accum = paste0(
      "<tool library='ta_hydrology' tool='1' name='Flow Accumulation (Recursive)'>
         <input id='ELEVATION'>dem_preproc</input>
         <output id='FLOW'>tca</output>
     </tool>"
    ),
    
    
    twi = paste0(
      "<tool library='ta_hydrology' tool='20' name='Topographic Wetness Index (TWI)'>
      <input id='SLOPE'>slope</input>
      <input id='AREA'>tca</input>
      <output id='TWI'>twi</output>
      <option id='CONV'>1</option>
      <option id='METHOD'>1</option>
  </tool>"
    ),
  
    # The threshold value of 1,000,000 on the TCA grid appears to give a good
    # representation of the channel network. See
    # https://sourceforge.net/projects/saga-gis/files/SAGA%20-%20Documentation/SAGA%20Documents/SagaManual.pdf/download
    chan_net = paste0(
      "<tool library='ta_channels' tool='0' name='Channel Network'>
       <input id='ELEVATION'>dem</input>
       <input id='SINKROUTE'>sinkroute</input>
       <output id='CHNLNTWRK'>cnetwork</output>
       <input id='INIT_GRID'>tca</input>
       <option id='INIT_METHOD'>2</option>
       <option id='INIT_VALUE'>1000000</option>
       <option id='DIV_CELLS'>5</option>
       <option id='MINLEN'>", scale_param, "</option>
   </tool>"
    ),
   
   # Since multiple flow direction (MFD) algorithms were used throughout, I
   # thought it would be consistent to keep that trend in this tool here
   # (METHOD = 1). Keeping the "BOUNDARY" attribute at TRUE will model the flow
   # distance to the boundaries of the study area (by making the study area
   # boundary a "channel" in itself).
   
   flow_dist = paste0(
     "<tool library='ta_channels' tool='4' name='Overland Flow Distance to Channel Network'>
            <input id='ELEVATION'>dem_preproc</input>
            <input id='CHANNELS'>cnetwork</input>
            <output id='DISTANCE'>hdist</output>
            <output id='DISTVERT'>vdist</output>
            <option id='METHOD'>1</option>
            <option id='BOUNDARY'>true</option>
        </tool>"
   ),
   
   # See page 2-3 of:
   # https://www.nrcs.usda.gov/wps/PA_NRCSConsumption/download?cid=stelprdb1258050&ext=pdf
   # Which makes reference to the following article:
   # https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2002WR001426
   # So initial slope threshold needs to be changed depending on the input
   # resolution, which is defined by the mrvbf_param variable. All other values
   # are SAGA default
   
   mrvbf = paste0(
     "<tool library='ta_morphometry' tool='8' name='Multiresolution Index of Valley Bottom Flatness (MRVBF)'>
           <input id='DEM'>dem</input>
           <output id='MRVBF'>mrvbf</output>
           <output id='MRRTF'>mrrtf</output>
           <option id='T_SLOPE'>", mrvbf_param, "</option>
           <option id='T_PCTL_V'>0.400000</option>
           <option id='T_PCTL_R'>0.350000</option>
           <option id='P_SLOPE'>4.000000</option>
           <option id='P_PCTL'>3.000000</option>
           <option id='UPDATE'>false</option>
           <option id='CLASSIFY'>false</option>
           <option id='MAX_RES'>100.000000</option>
       </tool>"
   ),
   
   # Terrain Ruggedness Index (TRI): Looks at ruggedness at a specified distance
   # away from a focal cell. The scale_param will adjust the search radius
   # distance in order to get a consistent idea across scales of the terrain
   # ruggedness. Ex: at 2.5m, it should look at a distance of 25m (i.e.: radius
   # of 10 cells), while at 5m if we want to look at a 25m distance away, it
   # should have a radius of 5 cells, etc.
   
   tri = paste0(
     "<tool library='ta_morphometry' tool='16' name='Terrain Ruggedness Index (TRI)'>
           <input id='DEM'>dem</input>
           <output id='TRI'>tri</output>
           <option id='MODE'>0</option>
           <option id='RADIUS'>", scale_param, "</option>
           <option id='DW_WEIGHTING'>0</option>
        </tool>"
   ),
   
   convergence = paste0(
     "<tool library='ta_morphometry' tool='1' name='Convergence Index'>
           <input id='ELEVATION'>dem</input>
           <output id='RESULT'>convergence</output>
           <option id='METHOD'>1</option>
           <option id='NEIGHBOURS'>1</option>
       </tool>"
   ),
   
   # The default openness parameters create a fairly blocky grid. With the Eagle
   # Hills data, changing the radius parameter to be half the amount of rows in
   # a grid helped to alleviate that. I don't have documentation for that, that
   # was just me trying things out and seeing what worked. The original paper for
   # calculating openness is:
   # https://www.asprs.org/wp-content/uploads/pers/2002journal/march/2002_mar_257-265.pdf
  
   openness = paste0(
     "<tool library='ta_lighting' tool='5' name='Topographic Openness'>
           <input id='DEM'>dem</input>
           <output id='POS'>open_pos</output>
           <output id='NEG'>open_neg</output>
           <option id='RADIUS'>", openness_param, "</option>
           <option id='METHOD'>0</option>
           <option id='DLEVEL'>3.000000</option>
           <option id='NDIRS'>8</option>
      </tool>"
   ),
   
   # These are default parameters
   dah = paste0(
     "<tool library='ta_morphometry' tool='12' name='Diurnal Anisotropic Heat'>
          <input id='DEM'>dem</input>
          <output id='DAH'>dah</output>
          <option id='ALPHA_MAX'>202.500000</option>
      </tool>"
   ),
   
   # The tpi_param here will calculate topographic position based on its position
   # relative to the cells in its neighborhood. For a 25m grid, 5 cells away is
   # 125m, while for a 2.5m grid 5 cells away is 12.5m This was scaled this way
   # to promote faster processing at higher resolutions. I also think this is a
   # better representation of topographic position at different scales anyway.
   
   tpi = paste0(
     "<tool library='ta_morphometry' tool='18' name='Topographic Position Index (TPI)'>
          <input id='DEM'>dem</input>
          <output id='TPI'>tpi</output>
          <option id='STANDARD'>false</option>
          <option id='RADIUS'>0.000000;", tpi_param, "</option>
          <option id='DW_WEIGHTING'>0</option>
      </tool>"
   ),
   
   # Using the rvest package, the centroid of the raster is found and sent off to
   # https://www.nrc-cnrc.gc.ca/eng/services/sunrise/advanced.html, where we get
   # official accurate representations of daily sunrise and sunset times
   # throughout a year at that centroid. Minimum sunrise and maximum sunset times
   # are calculated for a given study area in order to limit the amount of
   # calculations that occur while there would be no solar radiation at any given
   # time throughout a year. It also helps to decrease processing time.
   
   solar = paste0(
     "<tool library='ta_lighting' tool='2' name='Potential Incoming Solar Radiation'>
           <input id='GRD_DEM'>dem</input>
           <output id='GRD_DIRECT'>direinso</output>
           <output id='GRD_DIFFUS'>diffinso</output>
           <option id='SOLARCONST'>1367.000000</option>
           <option id='LOCALSVF'>1</option>
           <option id='UNITS'>0</option>
           <option id='SHADOW'>0</option>
           <option id='LOCATION'>1</option>
           <option id='PERIOD'>2</option>
           <option id='DAY'>2019-01-15</option>
           <option id='DAY_STOP'>2019-12-15</option>
           <option id='DAYS_STEP'>30</option>
           <option id='HOUR_RANGE'>", min_sunrise_numeric, "; ", max_sunset_numeric, "</option>
           <option id='HOUR_STEP'>", time_res, "</option>
           <option id='METHOD'>2</option>
           <option id='LUMPED'>70.000000</option>
           <option id='UPDATE'>0</option>
       </tool>
  
       <tool library='grid_tools' tool='12' name='Change Grid Values'>
         <input id='INPUT'>direinso</input>
         <option id='METHOD'>0</option>
         <option id='IDENTITY'>
           <OPTION type='static_table' id='IDENTITY' name='Lookup Table'>
             <FIELDS>
               <FIELD type='DOUBLE'>New Value</FIELD>
               <FIELD type='DOUBLE'>Value</FIELD>
             </FIELDS>
             <RECORDS>
               <RECORD>
                 <FIELD>-99999.000000</FIELD>
                 <FIELD>0.000000</FIELD>
               </RECORD>
             </RECORDS>
           </OPTION>
         </option>
       </tool>
  
     <tool library='grid_tools' tool='12' name='Change Grid Values'>
         <input id='INPUT'>diffinso</input>
         <option id='METHOD'>0</option>
         <option id='IDENTITY'>
           <OPTION type='static_table' id='IDENTITY' name='Lookup Table'>
             <FIELDS>
               <FIELD type='DOUBLE'>New Value</FIELD>
               <FIELD type='DOUBLE'>Value</FIELD>
             </FIELDS>
             <RECORDS>
               <RECORD>
                 <FIELD>-99999.000000</FIELD>
                 <FIELD>0.000000</FIELD>
               </RECORD>
             </RECORDS>
           </OPTION>
         </option>
       </tool>"
   )
  )
  
  # End list of tools, don't adjust code after here
  #############################################################################
  
  tools <- lapply(tool_list, function(x) {
    inputs = gsub(".*>", "", regmatches(
      x, gregexpr("(?<=<input).*?(?=</input>)", x, perl = TRUE))[[1]])
    outputs = unique(gsub(".*>", "", regmatches(
      x, gregexpr("(?<=<output).*?(?=</output>)", x, perl = TRUE))[[1]]))
    
    inputs = unique(inputs[!inputs %in% outputs])
    tc = sum(length(inputs), length(outputs))
    
    tibble(tool = x, inputs = list(inputs), outputs = list(outputs), tc = tc)
  })
  
  # Dynamically split processing based on the amount of available RAM a PC has.
  # The following code will determine the best way to process the rasters
  # based on the amount of RAM on a users machine. It does this by summing the
  # number of inputs and outputs and comparing them to the maximum number of 
  # files your system can hold based on the size of the input DEM.
  raster_size <- file.info(dem_input)$size / 1024 ^ 2 
  memory <- memory.limit() - raster_size - 1024 # Leave ~1GB of RAM available?
  n_files <- floor(memory / raster_size)
  
  # Adjust tools list
  for(i in 1:length(tools)) {
    try({
      if(tools[[i]]$tc < n_files) {
        while(sum(tools[[i]]$tc, tools[[i + 1]]$tc) < n_files) {
          y <- rbind(tools[[i]], tools[[i + 1]])
          y[1, "tc"] <- sum(y$tc)
          y[1, "tool"] <- paste(y$tool, collapse = "\n")
          y[1, "inputs"][[1]] <- list(unlist(y$inputs))
          y[1, "outputs"][[1]] <- list(unlist(y$outputs))
          y <- y[1, ]
          tools[[i]] <- y
          tools[[i + 1]] <- NULL
        }
      }
    }, silent = TRUE)
  }
  
  # Fix inputs and outputs columns, add function to call the whole thing in cmd
  xml_layout <- lapply(tools, function(x) {
    x$inputs <- list(
      unique(unlist(x$inputs)[!unlist(x$inputs) %in% unlist(x$outputs)])
    )
    
    x$input_xml <- foreach(i = x$inputs, .combine = paste) %do% {
      paste0("<input varname='", i, "' type='grid' parent='GRID_SYSTEM'>
          <name>", i, "</name>
      </input>\n", collapse = " ")}
    
    x$outputs <- foreach(k = x$outputs, .combine = paste) %do% {
      paste0(
        "<tool library='io_gdal' tool='2' name='Export GeoTIFF'>
        <input id='GRIDS'>", k, "</input>
        <option id='FILE'>", file.path(covariate_out, paste0(k, ".tif")), "</option>
      </tool>\n", collapse = " ")
    }
    
    x$header <- paste0(
      "<?xml version='1.0' encoding='UTF-8'?>
      <toolchain saga-version='7.3.0'>
      <group>toolchains</group>
      <identifier>Derived</identifier>
      <name>Derived Layers (one step)</name>
      <description>
        Common DEM derivatives in SAGA GIS
      </description>
    
      <parameters>
        <option varname='GRID_SYSTEM' type='grid_system'>
          <name>Grid System</name>
        </option>
        ", x$input_xml,
      "</parameters>
      <tools>", sep = "\n")
    
    x$footer <- paste0(
      "</tools>
  </toolchain>"
    )
    
    x$call <- paste(x$header, x$tool, x$outputs, x$footer, sep = "\n")
    return(x)
  })
  
  # Define text for cmd input
  cmd_text <- lapply(xml_layout, function(x) {
    foreach(k = x$inputs, .combine = paste) %do% {
      paste0("-", k, " ", file.path(covariate_out, paste0(k, ".tif")), collapse = " ")
    }
  })
  
  # Determine the toolchain directory based on your system
  dem_derived_xml <- ifelse(
    Sys.info()[["sysname"]] == "Windows", 
    file.path(dirname(saga_cmd), "tools", "toolchains", "dem_derived_layers.xml"), 
    file.path(dirname(dirname(saga_cmd)), "share", "saga", 
              "toolchains", "dem_derived_layers.xml")
  )
  
  # Process SAGA toolchains in the list
  for(p in 1:length(xml_layout)) {
    write_xml(read_xml(xml_layout[[p]]$call), dem_derived_xml)
    sys_cmd <- paste("toolchains Derived", cmd_text[[p]])
    system2(saga_cmd, sys_cmd)
  }
  
  # Remove intermediate files
  unlink(list.files(
    covariate_out, pattern = "cnetwork|tca|dem_preproc|sinkroute",
    full.names = TRUE)
  )
  
  return(list.files(covariate_out, pattern = ".tif$", full.names = TRUE))
}

```

All that is left to do now is run the functions!

```{r Run Functions }

lidar_layers <- list.files(cov_1m, pattern = "dem.tif|chm.tif", full.names = TRUE)

low_res <- foreach(grid = lidar_layers, .combine = c) %do% {
  lower_resolutions(
    grid_1m = grid, 
    out_path = covariate_file_path, 
    res_choices = c(4, 25), 
    saga_cmd = saga_cmd
  )
}

# One of the things brought up in the beginning was to do a comparison to TRIM
# TRIM is about 25m resolution, so we can clip it using the derived 25m DEM to
# do exact comparisons
# Download TRIM using these links: 
# https://www2.gov.bc.ca/assets/gov/data/geographic/topography/250kgrid.pdf
# https://pub.data.gov.bc.ca/datasets/175624/
dl_path <- file.path(AOI.dir, "_original_files", "TRIM")
trim_path <- file.path(covariate_file_path, "25m_TRIM")

if(!dir.exists(dl_path) || dir.exists(trim_path)) {
  dir.create(dl_path, showWarnings = FALSE)
  dir.create(trim_path, showWarnings = FALSE)
  dem_25 <- rast(file.path(covariate_file_path, "25m", "dem.tif"))
  
  # URL's to download tiles for Sechelt
  urls <- c("https://pub.data.gov.bc.ca/datasets/175624/92f/092f09_e.dem.zip", 
            "https://pub.data.gov.bc.ca/datasets/175624/92g/092g12_w.dem.zip")
  
  trim <- foreach(i = urls, .combine = merge) %do% { #function(x, y) merge(x, y)
    download.file(url = i, destfile = file.path(dl_path, basename(i)))
    unzip(file.path(dl_path, basename(i)), overwrite = TRUE, exdir = file.path(
      dl_path))
    unlink(file.path(dl_path, basename(i)))
    rast(file.path(dl_path, basename(file_path_sans_ext(i))))
  } %>% 
    project(dem_25) %>% 
    mask(dem_25)
  writeRaster(trim, file.path(trim_path, "dem.tif"), overwrite = TRUE)
}

dem_files <- grep("1m", list.files(covariate_file_path, pattern = "dem.tif", 
                                   full.names = TRUE, recursive = TRUE), 
                  value = TRUE, invert = TRUE)

covariate_list <- foreach(i = dem_files, .final = function(x) 
  setNames(x, basename(dirname(dem_files)))) %do% {
  dem_derived_layers(i, saga_cmd = saga_cmd)
}

# Create DEM files for use with ClimateBC program:
clim_bc_path <- file.path(file.path(AOI.dir, "0_raw_inputs", "ClimateBC_Layers"))
dir.create(clim_bc_path, recursive = TRUE, showWarnings = FALSE)

for(i in dem_files) {
  saga_ver <- system2(saga_cmd, "-v", stdout = TRUE)
  saga_ver <- unlist(regmatches(saga_ver, regexec("Version:\\s*(.*?)\\s*\\(", saga_ver)))[2]
  
  tool_list <- list(
    header = paste0(
      "<?xml version='1.0' encoding='UTF-8'?>
          <toolchain saga-version='", saga_ver, "'>
          <group>toolchains</group>
          <identifier>reproject_arc</identifier>
          <name>Reproject and Save ARCInfo Grid (one step)</name>
          <description>
            Common DEM derivatives in SAGA GIS
          </description>
        
          <parameters>
            <option varname='GRID_SYSTEM' type='grid_system'>
              <name>Grid System</name>
            </option>
            <input varname='dem' type='grid' parent='GRID_SYSTEM'>
          <name>DEM</name>
      </input>
      </parameters>
          <tools>", sep = "\n"),
    transform_grid = paste0(
      "<tool library='pj_proj4' tool='4' name='Coordinate Transformation (Grid)'>
        <option id='CRS_PROJ4'>+proj=longlat +datum=WGS84 +no_defs </option>
        <option id='CRS_EPSG'>4326</option>
        <option id='CRS_EPSG_AUTH'>EPSG</option>
        <input id='SOURCE'>dem</input>
        <option id='RESAMPLING'>3</option>
        <option id='TARGET_DEFINITION'>0</option>
        <output id='GRID'>out</output>
      </tool>"
    ), 
    export = paste0(
      "<tool library='io_grid' tool='0' name='Export ESRI Arc/Info Grid'>
        <input id='GRID'>out</input>
        <option id='FILE'>", file.path(clim_bc_path, paste0("dem_", basename(dirname(i)), ".asc")), "</option>
        <option id='FORMAT'>1</option>
        <option id='GEOREF'>0</option>
        <option id='PREC'>4</option>
        <option id='DECSEP'>0</option>
      </tool>"
    ), 
    footer = paste0(
      "</tools>
      </toolchain>"
    )
  )
  call <- paste0(tool_list, sep = "\n", collapse = " ")
  
  # Determine the toolchain directory based on your system
  xml_dir <- ifelse(
    Sys.info()[["sysname"]] == "Windows", 
    file.path(dirname(saga_cmd), "tools", "toolchains", "transform_arc.xml"), 
    file.path(dirname(dirname(saga_cmd)), "share", "saga", 
              "toolchains", "transform_arc.xml")
  )
  write_xml(read_xml(call), xml_dir)
  sys_cmd <- paste("toolchains reproject_arc -dem", i)
  system2(saga_cmd, sys_cmd)
}

# Create climate layers for normal annual & seasonal 1961-1990, same for 1991-2010, 
# for each resolution using the ClimateBC program. When that finishes, come back 
# here for renaming and saving the variables to their final location.

# Use SAGA to bulk transform grids
# Folders indicate resolutions, subfolders are the data attribute
clim_res <- list.dirs(clim_bc_path, full.names = TRUE, recursive = FALSE)

for(i in clim_res) {
  clim_sub <- list.dirs(i, recursive = FALSE)
  for(j in clim_sub) {
    res <- gsub("dem_", "", basename(dirname(j)))
    dem <- rast(file.path(covariate_file_path, res, "dem.tif"))
    clim_layers <- rast(list.files(j, full.names = TRUE, pattern = ".asc$"))
    crs(clim_layers) <- projection(CRS("+init=epsg:4326"))
    
    # Add CRS information and save as tif files
    # terra cannot reproject when CRS is in memory, it has to be in storage
    for(k in names(clim_layers)) {
      writeRaster(subset(clim_layers, k), file.path(
        j, paste0(k, ".tif")), 
        overwrite = TRUE
      )
    }
    
    # Do the reprojection now
    clim_layers <- list.files(j, full.names = TRUE, pattern = ".tif$")
    for(layer in clim_layers) {
      p <- rast(layer) %>% 
        project(dem) %>% 
        mask(dem)
      writeRaster(p, file.path(
        covariate_file_path, res, 
        tolower(paste0(basename(j), "_", basename(layer)))), 
        overwrite = TRUE
      )
    }
  }
}


```