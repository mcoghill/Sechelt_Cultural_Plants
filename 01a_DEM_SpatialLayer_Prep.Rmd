---
title: "01a_DEM_SpatialLayer_Prep"
author: "Matthew Coghill"
date: "2/5/2020"
output: html_document
---

# Introduction
This document describes the steps required to prepare raster spatial layers to be used in machine-learning based predictive ecosystem map method. The code used here is largely pawned from PEM project coding, but with adjustments made to suit this project more accurately.

This method is scripted in R, but uses other open source packages, predominantly [SAGA:](https://sourceforge.net/projects/saga-gis/). 

## Preparation steps

### Software
Prior to running this script you will need to download [SAGA](https://sourceforge.net/projects/saga-gis/) and know its location. SAGA can be downloaded on an external drive and run independent of permissions. Additionally, a source script may be called in R to install it to your system.

### Data 
To run this script you will need a base digital elevation model (digital terrain model) at 1m resolution. In this project, this was provided, though there is no raw LAS files to use, just processed raster grids.

### Matt's Notes
The purpose of this document is to derive desired lower resolution DEM's from a 1m DEM, and subsequently create covariate terrain layers for this project. This document uses three coding languages: R, cmd (aka batch), and XML. Command line functions will call individual functions to SAGA, while the XML scripts allow for "chaining" tools together for more efficient processing in SAGA so that SAGA doesn't continuously stop and start. It also prevents the creation of intermediate files. The big benefit of using SAGA is that it automatically uses all cores on the machine you're using. In this manner, SAGA will utilize available RAM on your machine for storing files until the very end when they are written to the disk so in some instances you may be limited in its use depending on the specs of your machine; however, a built in algorithm should be able to adjust how the processing will continue in the event your machine is limited.

Before getting started, the first thing to do is to load the required packages. Most of the processing work will actually be accomplished by SAGA GIS, thus there are only a few packages required here.

```{r Load Packages, echo=TRUE, results='hide'}

invisible(suppressPackageStartupMessages(
  lapply(c("tidyverse", "terra", "bcmaps"), library, character.only = TRUE)))

source(file.path('_functions', 'get_saga.R'))
source(file.path("_functions", "derive_terrain.R"))

```

### 1) Set up link between Saga and R. 
Run the below saga_cmd function to detect and install SAGA GIS if necessary for your system.

```{r download SAGA}

saga_cmd <- get_saga()
saga_ver <- system2(saga_cmd, "-v", stdout = TRUE)
saga_ver <- unlist(regmatches(saga_ver, regexec("Version:\\s*(.*?)\\s*\\(", saga_ver)))[2]

```

### 2) Define file paths and folders
In this next chunk, you enter the only input data required for this sheet to function, the rest is (mostly) automated! Simply input the name of the AOI folder, and a projection code. Folders will be created for the terrain and climate layer outputs from R.

```{r Set directories}

AOI <- "Sechelt"
AOI_dir <- file.path(".", paste0(AOI, "_AOI"))
epsg <- 3005
cov_res <- c(4, 10, 25)

raw_dir <- file.path(AOI_dir, "_original_files")
covariate_dir <- file.path(AOI_dir, "1_map_inputs", "covariates")
cov_1m <- file.path(covariate_dir, "1m")
dir.create(cov_1m, recursive = TRUE, showWarnings = FALSE)

# Create ClimateBC output directory (for .asc files, not the final location)
clim_bc_dir <- file.path(file.path(AOI_dir, "0_raw_inputs", "ClimateBC_Layers"))
dir.create(clim_bc_dir, recursive = TRUE, showWarnings = FALSE)

```

### 3) Transform raw rasters
Here, we begin raw processing. Likely, we won't be dealing with resolutions less than 2m, but if it does this code needs to be re-evaluated.

The raw DEM and CHM have very slightly different extents and resolutions, so initially they cannot be stacked together. Rather, they are put together in a list item "raw", and their common extents are defined (i.e.: largest bounding area). This area is used as an extent to download raw TRIM data from using the `bcmaps` package. The raw DEM is then Lat/Long transformed to desired resolutions for use in ClimateBC. Those variables are saved as .asc files which then get their line endings (end of line, EOL) defined for Windows CR LF, required by ClimateBC (used to have to do this in Notepad++ before finding solution).

Finally, the common extent of the DEM and CHM are transformed to BC Albers and the resolution is defined. The raw DEM and CHM are projected to that extent, trimmed to remove edges of NA values, and then written to the output folder.

It should be noted that there was a previous method that involved many steps for downscaling a raw DEM involving the following SAGA tools:

* Focal statistics (smoothing a DEM over a given size window)
* Create grid system (creates an empty raster grid for data to be placed into later on)
* Grid values to points (Creates point features at each of the empty raster cells which data can be extracted to from the smoothed DEM)
* Add grid values to shapes (gets values from the smoothed DEM)
* Points to grid (Places values from the points into the empty grid)
* Crop to data (Removes extra NA values from the edges of the raster)

Normal resampling takes into account the neighbouring cells, so I don't see why it can't be used to downscale the DEM and CHM here.

This chunk takes ~25 minutes to run

```{r Downscale raw rasters}

# Load files
raw_files <- c(file.path(raw_dir, "dem", "w001001.adf"),
               file.path(raw_dir, "chm", "w001001.adf"))

# Create rasters of both in list format
raw <- lapply(raw_files, function(x) {
  rast(x) %>% setNames(basename(dirname(x)))}) %>% 
  setNames(basename(dirname(raw_files)))

# Create complete bounding extent
new_ext <- ext(c(xmin = min(sapply(raw, xmin)), xmax = max(sapply(raw, xmax)), 
                 ymin = min(sapply(raw, ymin)), ymax = max(sapply(raw, ymax))))

# Get TRIM elevation as well - first, create shape of AOI to download from
new_ext_sf <- st_bbox(c(
  xmin = xmin(new_ext), xmax = xmax(new_ext),
  ymin = ymin(new_ext), ymax = ymax(new_ext)), crs = crs(raw[[1]])) %>% 
  st_as_sfc()

# Use AOI to download TRIM elevation
trim_raw <- rast(cded_raster(new_ext_sf)) %>% setNames("dem")

# Transform raw DEM to lat/long, matching trim_raw
dem_latlng <- project(raw[["dem"]], trim_raw)

# Mask TRIM download to transformed DEM
trim_latlng <- mask(trim_raw, dem_latlng)

# Reproject TRIM to BC Albers
trim_bca <- project(trim_latlng, paste0("epsg:", epsg))

# Create a BC Albers projected extent of the DEM
dem <- raw[["dem"]]
dem_ext <- rast(ext(dem), crs = crs(dem))
dem_ext_bca <- project(dem_ext, paste0("epsg:", epsg))

# Flow is a little strange here. BC Albers and UTM both use meters, but slightly
# off from each other when projected between each. Want to define the final 
# resolution output as BC Albers, so with the BC Albers extent define the 
# desired resolution, then project that to Lat/Long. Project DEM to Lat/Long 
# extent for ClimateBC layers, change NA values to -9999 for ClimateBC, and 
# write to .asc
for(i in c(cov_res, "TRIM")) {
  r <- suppressWarnings(as.numeric(i))
  if(!is.na(r)) {
    res(dem_ext_bca) <- r
    dem_ext_prj <- project(dem_ext_bca, "epsg:4326")
    dem_prj <- project(dem, dem_ext_prj)
    p <- paste0(r, "m")
  } else {
    dem_prj <- trim_latlng
    p <- i
  }
  dem_prj[is.na(dem_prj)] <- -9999
  out <- file.path(clim_bc_dir, paste0("dem_", p, ".asc"))
  writeRaster(dem_prj, out, overwrite = TRUE, wopt = list(NAflag = -9999))
  
  # Redefine Windows end of line (EOL) options for ClimateBC
  txt <- readLines(out)
  f <- file(out, open = "wb")
  cat(txt, file = f, sep = "\r\n")
  close(f)
  
  # Cleanup temp files
  suppressWarnings(tmpFiles(old = TRUE, remove = TRUE))
}

# Won't use <2m covariates since it is too computationally expensive, so just
# project raw DEM and CHM files to BC Albers extents at defined resolutions rather
# than first creating a 1m transformed raw DEM - not worth it. Also, this makes
# processing quite a bit faster.
for(i in c(cov_res, "TRIM")) {
  r <- suppressWarnings(as.numeric(i))
  if(!is.na(r)) {
    if(r >= 2) {
      dir_out <- file.path(covariate_dir, paste0(r, "m"))
      dir.create(dir_out, showWarnings = FALSE)
      
      lidar_ext <- project(rast(new_ext, crs = crs(raw[[1]])), paste0("epsg:", epsg))
      res(lidar_ext) <- r
      lidar_low <- rast(lapply(raw, terra::project, lidar_ext)) %>% 
        trim() %>% 
        writeRaster(filename = file.path(dir_out, paste0(names(.), ".tif")), 
                    overwrite = TRUE)
    } else {
      message("<2m raster generation not supported")
    }
  } else {
    dir_out <- file.path(covariate_dir, i)
    dir.create(dir_out, showWarnings = FALSE)
    lidar_low <- trim(trim_bca, overwrite = TRUE,
                      filename = file.path(dir_out, paste0(names(trim_bca), ".tif")))
  }
  # Cleanup temp files
  suppressWarnings(tmpFiles(old = TRUE, remove = TRUE))
}

```

### 4) Derive terrain attributes
SAGA GIS has a CLI that can be interfaced with R to create a chain function to derive multiple terrain attributes at once. This chunk harnesses that in a simple function and all that is required are file paths to base DEM's and the path to SAGA GIS.

This chunk should take < 1 hour to run.

```{r Derive terrain attributes}

# Don't use DEM's that are < 1m
all_dem <- list.files(covariate_dir, pattern = "dem.tif$", full.names = TRUE, recursive = TRUE)
dem_files <- unlist(lapply(all_dem, function(x) {
  r <- rast(x)
  if(xres(r) >= 2) sources(r)[, "source"] else NULL }))

covariates <- lapply(dem_files, function(x) 
  dem_derived_layers(x, saga_cmd = saga_cmd)) %>% 
  setNames(basename(dirname(dem_files)))

```

### 5) ClimateBC layers
After generating the layers from ClimateBC, project the .asc files back to BC Albers to match the base DEM's. Certain file checks are involved and this function actually ends up taking quite a while to run. Checks include that the file is valid (i.e.: min/max values can be loaded, and there are > 20 unique values in the raster). Additionally, certain files require a division by 10 to match what they actually represent. This is because all of the rasters output integer values from ClimateBC, so for example MAT (Mean Annual Temperature) will report a cell value of 165 which actually means 16.5. After these checks, the NA values of -9999 are replaced with NaN to match the base DEMs, and are finally written out as .tif files.

This chunk takes ~2.5-3 hours to run.

```{r ClimateBC rasters}

# Folders indicate resolutions, subfolders are the data attribute
clim_res <- grep("dem", list.dirs(clim_bc_dir, full.names = TRUE, recursive = FALSE),
                 value = TRUE)

for(i in clim_res) {
  clim_sub <- list.dirs(i, recursive = FALSE)
  res <- gsub("dem_", "", basename(i))
  dem <- rast(file.path(covariate_dir, res, "dem.tif"))
  for(j in clim_sub) {
    # Invalid/broken layers don't have minmax values set upon reading into R
    clim_layers <- rast(
      unlist(lapply(list.files(j, full.names = TRUE, pattern = ".asc$"), function(x) {
        d <- describe(x, options = "mm")
        if(any(grepl("Min/Max=", d))) x else NULL })))
    
    # Require calculation on certain grids
    recalc <- subset(
      clim_layers, grep("Tave|Tmax|Tmin|Rad|AHM|EMT|EXT|MAR|MAT|MCMT|MWMT|SHM|TD",
                        names(clim_layers))) / 10
    
    # Replace recalculated grids into clim_layers
    clim_layers <- c(
      subset(clim_layers, grep(paste0(names(recalc), collapse = "|"),
                               names(clim_layers), invert = TRUE)),
      recalc)
    
    # Count the unique values in each layer and remove layers with too few
    # unique cell values (set to 20)
    clim_layers_valid <- data.frame(freq(clim_layers, digits = NA)) %>% 
      group_by(layer) %>% 
      summarise(count = n(), .groups = "drop") %>%
      dplyr::mutate(layer = names(clim_layers)[layer]) %>% 
      dplyr::filter(count > 20)
    
    clim_layers <- subset(clim_layers, clim_layers_valid$layer)
    clim_layers[clim_layers == -9999] <- NaN
    
    # Do the reprojection now
    p <- project(clim_layers, dem) %>% 
      mask(dem) %>% 
      setNames(tolower(paste0(basename(j), "_", names(.)))) %>% 
      writeRaster(filename = file.path(covariate_dir, res, 
                              paste0(names(.), ".tif")), overwrite = TRUE)
    
    # Cleanup temp files
    suppressWarnings(tmpFiles(orphan = TRUE, old = TRUE, remove = TRUE))
  }
}

```

The chunk below contains some old code for getting TRIM elevation in case the `bcmaps` function ever stops working.

```{r old code}

#   # URL's to download tiles for Sechelt
#   urls <- c("https://pub.data.gov.bc.ca/datasets/175624/92f/092f09_e.dem.zip", 
#             "https://pub.data.gov.bc.ca/datasets/175624/92g/092g12_w.dem.zip")
#   
#   # Merge the files and convert to a raster object
#   trim <- lapply(urls, function(x) {
#     download.file(url = x, destfile = file.path(dl_dir, basename(x)))
#     z <- unzip(file.path(dl_dir, basename(x)), overwrite = TRUE, 
#                exdir = file.path(dl_dir))
#     unlink(file.path(dl_dir, basename(x)))
#     return(rast(z))
#   }) %>% Reduce(function(x, y) merge(x, y), .) %>% setNames("dem")

```
