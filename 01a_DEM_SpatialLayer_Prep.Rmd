---
title: "01a_DEM_SpatialLayer_Prep"
author: "Matthew Coghill"
date: "2/5/2020"
output: html_document
---

# Introduction
This document describes the steps required to prepare spatial layers to be used in machine-learning based predictive ecosystem map method. The code used here is largely pawned from PEM project coding, but with adjustments made to suit this project more accurately.

This method is scripted in R, but uses other open source packages, predominently SAGA:  (https://sourceforge.net/projects/saga-gis/). 

## Preparation steps

### Software
Prior to running this script you will need to download SAGA[https://sourceforge.net/projects/saga-gis/] and know its location. SAGA can be downloaded on an external drive and run independent of pemissions. Additionally, a source script may be called in R to install it to your system.

### Data 
To run this script you will need a base digital elevation model (digital terrain model) at 1m resolution. In this project, this was provided, though there is no raw LAS files to use, just processed raster grids.

### Matt's Notes
The purpose of this document is to derive desired lower resolution DEM's from a 1m DEM, and subsequently create covariate terrain layers for this project. This document uses three coding languages: R, cmd (aka batch), and XML. Command line functions will call individual functions to SAGA, while the XML scripts allow for "chaining" tools together for more efficient processing in SAGA so that SAGA doesn't continuously stop and start. It also prevents the creation of intermediate files. The big benefit of using SAGA is that it automatically uses all cores on the machine you're using. In this manner, SAGA will utilize available RAM on your machine for storing files until the very end when they are written to the disk so in some instances you may be limited in its use depending on the specs of your machine; however, a built in algorithm should be able to adjust how the processing will continue in the event your machine is limited.

Before getting started, the first thing to do is to load the required packages. Most of the processing work will actually be accomplished by SAGA GIS, thus there are only a few packages required here.

```{r Load Packages}

suppressMessages(suppressWarnings({
  ls <- c("tidyverse", "terra", "xml2", "sf", "lubridate", "foreach", "rvest", 
          "httr", "tools")
  new_packages <- ls[!(ls %in% installed.packages()[, "Package"])]
  if(length(new_packages)) install.packages(new_packages)
  lapply(ls, library, character.only = TRUE)[0]  # load the required packages
  rm(ls, new_packages)
  
  source(file.path('_functions', 'get_saga.R'))
  source(file.path("_functions", "derive_terrain.R"))}))

```

### 1) Set up link between Saga and R. 
Run the below saga_cmd function to detect and install SAGA GIS if necessary for your system

```{r download SAGA}

saga_cmd <- get_saga()
saga_ver <- system2(saga_cmd, "-v", stdout = TRUE)
saga_ver <- unlist(regmatches(saga_ver, regexec("Version:\\s*(.*?)\\s*\\(", saga_ver)))[2]

```

### 2) Define file paths and folders
In this next chunk, you enter the only input data required for this sheet to function, the rest is (mostly) automated! Simply input the name of the AOI folder, and a projection code.

```{r R Data Objects}

AOI <- "Sechelt"
AOI.dir <- file.path(".", paste0(AOI, "_AOI"))
epsg <- 3005

raw_file_path <- file.path(AOI.dir, "_original_files")
covariate_file_path <- file.path(AOI.dir, "1_map_inputs", "covariates")
cov_1m <- file.path(covariate_file_path, "1m")
dir.create(cov_1m, recursive = TRUE, showWarnings = FALSE)

```

### 3) Transform raw rasters
In the next chunk, we want to specify where the raw files are located. For the Sechelt project, there were two output LiDAR files: a CHM and a DEM, both at resolutions of (almost) 1m and using a UTM based CRS. Like most projects in BC, I opted to use the BC Albers (EPSG: 3005) CRS. A few steps were required to carry out the desired functionality using the new and powerful terra package. Certain workarounds have been used in order to make certain functions work, though most of the functions required here are fleshed out by the package creator. The terra package is a replacement for the raster package and the benefits are all to do with processing speed and efficiency by harnessing C++ computing. In order to transform the rasters to the BC Albers projection, the following steps needed to be followed:

1. The extents (bounding boxes) of both of the raw DEM and CHM files were different from each other. A new bounding box containing the extents of both files was first created, and then that bounding box was transformed to the BC Albers CRS. 
2. The raw files were individually transformed to the projection and extent of the BC Albers bounding box. This ensured that they shared the exact same grid.
3. The individually transformed files were then trimmed to remove outside NA values. There is a built in function for this in terra (trim, used later on); however, at the time of writing this, (September 2020), there was an issue with this function when the 1m grids were being used, so instead a workaround was implemented by finding which rows and columns had data in them, then finding the coordinates of those and trimming to those coordinates
4. Now that the transformed rasters were individually trimmed, the extents had to once again be combined to get the largest (smallest) bounding box encompassing both of them. The extents of each file were expanded and they were then able to be stacked.

```{r Specify 1m Raster}

# Load files
raw_files <- c(file.path(raw_file_path, "dem", "w001001.adf"),
               file.path(raw_file_path, "chm", "w001001.adf"))

# I want to use the terra package for this since it has the ability to be 
# functionized over any number of input layers so long as their CRS's are all
# the same. Normally, I would have stacked these in a single "rast" function, 
# however, the extents do not match between the files, hence the list placement of
# each file
raw <- lapply(raw_files, function(x) {
  rast(x) %>% magrittr::set_names(basename(dirname(x)))}) %>% 
  magrittr::set_names(basename(dirname(raw_files)))

# Get the spatial area of each file. From those, the largest possible spatial 
# extent is created. This assumes that the raw files have the same CRS's
new_ext <- ext(c(xmin = min(sapply(raw, xmin)), xmax = max(sapply(raw, xmax)), 
                 ymin = min(sapply(raw, ymin)), ymax = max(sapply(raw, ymax))))

# Give a new extent to project rasters to by projecting the extent and defining
# the resolution we want the raster to be at
new_proj <- project(rast(new_ext, crs = crs(raw[[1]])), 
                    paste0("epsg:", epsg))
res(new_proj) <- 1

# Project rasters to new extent and resolution, and trim NA areas
lidar <- lapply(raw, function(x) {
  proj <- terra::project(x, new_proj)
  
  # Now trim the extra edges. terra package trim function was stalling using
  # the 1m raster, not sure why that was, so I created my own way to do so 
  # below.
  
  # Need to do col and rowsums omitting NA values. If a whole row is NA when 
  # this option is applied, the result is 0. For this reason, all valid raster
  # values need to be changed to values above 0 (i.e.: 1), then it's a simple
  # matter of finding the rows/columns that don't have sums of 0 - those are 
  # the minimum and maximum row and column numbers that will be used to index
  # the minumum and maxiumum x and y coordinates.
  
  # This way creates only one tempfile instead of 2, but col and rowsum 
  # functions need to be carried out in raster package since they are not 
  # currently available in terra
  inter <- raster::raster(terra::lapp(proj, function(x) ((x * 0) + 1)))
  colvals <- raster::colSums(inter, na.rm = TRUE)
  rowvals <- raster::rowSums(inter, na.rm = TRUE)
  
  # We don't care what the sums of the rows are, just that the values > 0 
  # will indicate what rows/cols have data. The first colsum value is used to find
  # the xmin coordinate whereas the last colsum value is used to find xmax. This
  # is reversed for finding y coordinates.
  proj_ext <- terra::ext(c(
    xmin = terra::xFromCol(proj, head(which(colvals > 0) - 1, n = 1)),
    xmax = terra::xFromCol(proj, tail(which(colvals > 0), n = 1)),
    ymin = terra::yFromRow(proj, tail(which(rowvals > 0) + 1, n = 1)),
    ymax = terra::yFromRow(proj, head(which(rowvals > 0), n = 1))))
  
  # Crop original file to new extent, saved as temp file
  crop <- terra::crop(proj, proj_ext)})

# Using the list of rasters, find the smallest extent that all of the rasters
# will fit into
new_ext <- ext(c(xmin = min(sapply(lidar, xmin)), xmax = max(sapply(lidar, xmax)), 
                 ymin = min(sapply(lidar, ymin)), ymax = max(sapply(lidar, ymax))))

# Using the above extent, expand the rasters, write them out, and then load them
# back into R as a single SpatRaster object.
lidar <- foreach(x = names(lidar), .final = rast) %do% {
  expand(lidar[[x]], new_ext) %>% 
    terra::writeRaster(
      filename = file.path(AOI.dir, "_original_files", x, paste0(x, ".tif")), 
      overwrite = TRUE)}

# Cleanup temp files
suppressWarnings(tmpFiles(old = TRUE, remove = TRUE))

```

### 4) Derive lower resolution DEM and CHM files
This next chunk is a function which will start the pre-processing of a 1m resolution DEM into your desired DEM resolutions. Originally, a complicated method of creating these lower resolution files was devised by Lucas Jarron and Nicholas Coops from UBC; however, I believe that it is unnecessary to do that and you will get just as fine information from a simple resampling method. Regardless, the old method involved the following SAGA GIS tools:

* Focal statistics (smoothing a DEM over a given size window)
* Create grid system (creates an empty raster grid for data to be placed into later on)
* Grid values to points (Creates point features at each of the empty raster cells which data can be extracted to from the smoothed DEM)
* Add grid values to shapes (gets values from the smoothed DEM)
* Points to grid (Places values from the points into the empty grid)
* Crop to data (Removes extra NA values from the edges of the raster)

Resampling takes into account the neighboring cells, so I don't see why it can't be used to downscale the DEM and CHM here.

```{r Derive Lower Resolutions}

for(r in c(1, 4, 5, 10, 25)) {
  dir_out <- file.path(covariate_file_path, paste0(r, "m"))
  dir.create(dir_out, showWarnings = FALSE)
  lidar <- rast(list.files(raw_file_path, pattern = ".tif$", 
                           full.names = TRUE, recursive = TRUE))
  if(r != 1) {
    new_ext <- rast(ext(lidar), crs = crs(lidar))
    res(new_ext) <- r
    
    # Note need separate writeraster function to write out multiple files as 
    # opposed to a single multilayered file
    lidar <- terra::resample(lidar, new_ext, method = "bilinear") %>% 
      writeRaster(filename = file.path(dir_out, paste0(names(.), ".tif")), 
                  overwrite = TRUE)
  } else {
    lidar <- terra::writeRaster(
      lidar, overwrite = TRUE,
      filename = file.path(dir_out, paste0(names(lidar), ".tif")))
  }
  
  # Cleanup temp files
  suppressWarnings(tmpFiles(old = TRUE, remove = TRUE))
}

```

### 5) Download TRIM and derive terrain attributes
At one point, the value of TRIM was brought into question - do we even need LiDAR? (yes we do, but this helps drive home a point on comparison methods). We needed 2 TRIM tiles in order to create the DEM from it. Those are downloaded, transformed, and clipped using the shape of the 1m DEM raster.

Finally, a large function (written in the _functions/derive_terrain.R file) is run to create ~19 different covariate layers describing different terrain attributes. This uses SAGA GIS toolchains in order to process everything. The rasters are saved to the same folder as the DEM and CHM.

```{r Derive terrain attributes}

# One of the things brought up in the beginning was to do a comparison to TRIM
# TRIM is about 25m resolution, so we can clip it using the derived 25m DEM to
# do exact comparisons
# Download TRIM using these links: 
# https://www2.gov.bc.ca/assets/gov/data/geographic/topography/250kgrid.pdf
# https://pub.data.gov.bc.ca/datasets/175624/
dl_path <- file.path(raw_file_path, "TRIM")
trim_path <- file.path(covariate_file_path, "TRIM")

if(!dir.exists(dl_path) || !dir.exists(trim_path)) {
  dir.create(dl_path, showWarnings = FALSE)
  dir.create(trim_path, showWarnings = FALSE)
  dem_1 <- rast(file.path(covariate_file_path, "1m", "dem.tif"))
  
  # URL's to download tiles for Sechelt
  urls <- c("https://pub.data.gov.bc.ca/datasets/175624/92f/092f09_e.dem.zip", 
            "https://pub.data.gov.bc.ca/datasets/175624/92g/092g12_w.dem.zip")
  
  # Merge the files and then reproject
  trim <- foreach(i = urls, .final = function(x) 
      do.call(merge, c(x))) %do% { 
    download.file(url = i, destfile = file.path(dl_path, basename(i)))
    z <- unzip(file.path(dl_path, basename(i)), overwrite = TRUE, 
               exdir = file.path(dl_path))
    unlink(file.path(dl_path, basename(i)))
    return(rast(z))
  } %>% project(paste0("epsg:", epsg)) %>% magrittr::set_names("dem")
  
  dem_1_ext <- rast(ext(dem_1), crs = crs(dem_1))
  res(dem_1_ext) <- res(trim)
  trim_mask <- resample(dem_1, dem_1_ext)
    
  trim_bca <- mask(crop(trim, trim_mask), trim_mask, 
                   filename = file.path(trim_path, "dem.tif"), overwrite = TRUE)
  
  # Cleanup temp files
  suppressWarnings(tmpFiles(old = TRUE, remove = TRUE))
  
}

dem_files <- grep("1m", list.files(covariate_file_path, pattern = "dem.tif$", 
                                   full.names = TRUE, recursive = TRUE), 
                  value = TRUE, invert = TRUE)

covariates <- foreach(i = dem_files, .final = function(x) 
  setNames(x, basename(dirname(dem_files)))) %do% {
  dem_derived_layers(i, saga_cmd = saga_cmd)}

```

### 6) ClimateBC layers
Additionally, we need to produce some raster layers from the ClimateBC program: http://climatebc.ca/
The program takes as an input a DEM provided as a .asc file in the 4326 projection. Unfortunately, the program has no CLI and I'm not familiar with how to create one from a given program, so as a user you will need to build those layers yourself in the program and then come back here to the code and run the processing to reproject the layers back to your desired EPSG.

```{r ClimateBC rasters}

# Create DEM files for use with ClimateBC program:
clim_bc_path <- file.path(file.path(AOI.dir, "0_raw_inputs", "ClimateBC_Layers"))
dir.create(clim_bc_path, recursive = TRUE, showWarnings = FALSE)

# Flow is a bit weird - first, DEM's are transformed to EPSG 4326 for ClimateBC
# layers. Any elevation below 0 is changed to NA. Next, the raster package is
# used to change NA values to -9999, again necessary for generating ClimateBC
# layers. Finally, the files are written to .asc files. This type of file is not
# implemented (yet) in the terra package. This is a fairly quick function to run.
for(i in dem_files) {
  dem_bca <- rast(i)
  dem_prj <- project(dem_bca, "epsg:4326")
  dem_prj[dem_prj < 0] <- NA
  dem_prj <- raster::raster(dem_prj)
  raster::NAvalue(dem_prj) <- -9999
  raster::writeRaster(
    dem_prj, overwrite = TRUE, prj = TRUE, NAflag = -9999,
    filename = file.path(clim_bc_path, paste0("dem_", basename(dirname(i)), ".asc")))
  
  # Cleanup temp files
  suppressWarnings(tmpFiles(old = TRUE, remove = TRUE))
}

# Create climate layers for normal annual & seasonal 1961-1990, same for 1991-2010, 
# for each resolution using the ClimateBC program. When that finishes, come back 
# here for renaming and saving the variables to their final location.
# ______________________________________________________________________________
# ______________________________________________________________________________
# ______________________________________________________________________________

# Folders indicate resolutions, subfolders are the data attribute
clim_res <- list.dirs(clim_bc_path, full.names = TRUE, recursive = FALSE)

for(i in clim_res) {
  clim_sub <- list.dirs(i, recursive = FALSE)
  for(j in clim_sub) {
    res <- gsub("dem_", "", basename(dirname(j)))
    dem <- rast(file.path(covariate_file_path, res, "dem.tif$"))
    clim_layers <- rast(list.files(j, full.names = TRUE, pattern = ".asc$"))
    
    # Add CRS information and save as tif files
    # terra cannot reproject when CRS is in memory, it has to be in storage. The
    # clever way is to save them to the same spot as .tif files, but could also
    # be written as temp files elsewhere
    crs(clim_layers) <- "epsg:4326"
    clim_layers <- writeRaster(clim_layers, file.path(j, paste0(names(clim_layers), ".tif")),
                               overwrite = TRUE)
    
    # Do the reprojection now
    p <- project(clim_layers, dem) %>% 
      mask(dem) %>% 
      magrittr::set_names(tolower(paste0(basename(j), "_", names(.)))) %>% 
      writeRaster(filename = file.path(covariate_file_path, res, 
                              paste0(names(.), ".tif")), overwrite = TRUE)
    
    # Cleanup temp files
    suppressWarnings(tmpFiles(old = TRUE, remove = TRUE))
  }
}

```

