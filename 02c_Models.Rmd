---
title: "02c_Models"
author: "Matthew Coghill"
date: "2/19/2020"
output: html_document
---

This document is meant to generate models and predictions of either binary classification of presence / absence, or regression models of plant cover. This script depends on:

1. Covariate layers generated in 01 scripts;
2. Proper datasets generated in the 02a script; and
3. The site series raster generated in the 02b script.

This script uses the mlr3 package (machine learning R) in order to make models. It's very powerful and can evaluate a lot of different models at the same time. I'll attempt to explain things each step of the way. First, we will load the packages used in this script.

```{r Load Packages, echo=TRUE, results='hide'}

suppressMessages(suppressWarnings({
  lapply(c("tidyverse", "sf", "terra", "mlr3verse"), 
         library, character.only = TRUE)[0]}))

# Load custom functions
source("./_functions/model_gen_mlr3.R")
source("./_functions/predict_landscape_mlr3.R")

```

Next, directories and certain files are defined. This is where you would also specify the resolution at which you want to produce your map predictions.

```{r Define directories}

AOI <- "Sechelt"
AOI_dir <- file.path(".", paste0(AOI, "_AOI"))
map_res <- "TRIM"
res_dir <- ifelse(is.numeric(map_res), paste0(map_res, "m"), map_res)

input_dir <- file.path(
  "./Sechelt_AOI/1_map_inputs/field_data/processed_2019", res_dir)
covariate_dir <- file.path(AOI_dir, "1_map_inputs", "covariates", res_dir)
shapes_dir <- file.path(AOI_dir, "0_raw_inputs", "base_layers", res_dir)
dsmart_site_ser_dir <- file.path(AOI_dir, "1_map_inputs", "dsmart_2019", 
                                 res_dir, "site_ser")
dsmart_struct_stg_dir <- file.path(AOI_dir, "1_map_inputs", "dsmart_2019", 
                                   res_dir, "struct_stg")
output_dir <- file.path(AOI_dir, "2_map_predictions", res_dir)
tile_dir <- file.path(output_dir, "tiles")

dir.create(tile_dir, showWarnings = FALSE, recursive = TRUE)
dir.create(file.path(output_dir, "pres_abs_predictions"), showWarnings = FALSE)
dir.create(file.path(output_dir, "cover_predictions"), showWarnings = FALSE)

```

It is highly anticipated that a site series layer will have a high degree of importance in predicting the locations of these culturally sensitive plants. We will let the models decide whether that is true or not, but it is possible to always be kept in the model by defining one of the options (not incorporated here). Cover models will be a regression with outputs of actual plant cover (%) and presence / absence models will output probabilities of presence.

The `mlr3` package creates models using building blocks called "tasks" and "learners". Tasks are essentially the data you are creating your models from, and learners are more specific to building models. For example, the first learner that's created here says "create a ranger regression model, and when creating the model rank importance variables by their impurity values. Also, this is a response model". The next learner is a feature selection filter which wraps around the first learner. It pretty much says that it will filter the variables using the ranger_impurity algorithm, but always will keep the site_series layer. The last learner is a parameter tuning algorithm wrapped around the feature selection filter, and gives further details on the model. It will be a spatial 10-fold cross validation repeated 5 times. Also, it will create models using different amounts of covariate layers and numbers of trees so that it selects the best model from that bunch.

After the best model is fit, that model is extracted and the predictions for presence / absence or cover are created. This can take some time depending on the amount of predictor variables used in the final model and on the resolution of the maps. For example, running 28 different models for 2 plants separately (a total of 56 models) at a 4m resolution took over 2 days, whereas at a 25m pixel took approximately 3 hours. Finally, some zonal statistics are calculated per site series.

The following chunk sets up the filtering parameters to produce the correct amount of models from the master dataset.

```{r Load data}

# List of processing options...
temporal <- c("annual") # either or both of annual and seasonal
ss <- c("simple", "dsmart", NA) # any of simple, dsmart, and/or NA
probabilities <- c(TRUE, FALSE) # Use probabilities or classification
bgc <- c(TRUE, FALSE) # Whether or not to use classified BGC subzone layer
climate_vars <- c(TRUE, FALSE) # TRUE, FALSE
perms <- expand.grid(
  temporal = c(temporal, paste0(temporal, collapse = ", ")), 
  ss = ss, 
  probabilities = c(probabilities, paste0(probabilities, collapse = ", ")),
  bgc = bgc, 
  climate_vars = climate_vars, 
  stringsAsFactors = FALSE) %>% 
  mutate(probabilities = ifelse(is.na(ss), NA, probabilities)) %>% 
  distinct()

# Raster covariates
terrain_covariates <- grep(pattern = "normal|sentinel2", list.files(
  covariate_dir, full.names = TRUE, pattern = ".tif$"), 
  invert = TRUE, value = TRUE)

if("annual" %in% temporal && "seasonal" %in% temporal) {
  s_pattern <- "sentinel2.*..tif$"
  c_pattern <- "normal.*..tif$"
} else if("annual" %in% temporal && !"seasonal" %in% temporal) {
  s_pattern <- "sentinel2.*.2019.tif$"
  c_pattern <- "normal(?!.*(_at.tif|_sm.tif|_sp.tif|_wt.tif))"
} else if(!"annual" %in% temporal && "seasonal" %in% temporal) {
  s_pattern <- "sentinel2.*.(fall.tif|spring.tif|summer.tif|winter.tif)"
  c_pattern <- "normal.*.(_at.tif|_sm.tif|_sp.tif|_wt.tif)"
}

sentinel_covariates <- grep(file.path(covariate_dir, s_pattern), 
                            list.files(covariate_dir, full.names = TRUE), 
                            value = TRUE, perl = TRUE)

climate_covariates <- if(any(climate_vars)) {
  grep(file.path(covariate_dir,c_pattern),
       list.files(covariate_dir, full.names = TRUE), 
       value = TRUE, perl = TRUE)
} else NULL

# Define BEC layer, if used
bgc_layer <- if(any(bgc)) {
  
  # Create BEC layer if it doesn't exist
  if(!file.exists(file.path(dsmart_site_ser_dir, "inputs", "bgc.tif"))) {
    
    bgc_sf <- st_read(file.path(shapes_dir, "bec.gpkg"), quiet = TRUE) %>% 
      mutate(MAP_LABEL = as.factor(MAP_LABEL), bgc = as.numeric(as.factor(MAP_LABEL))) 
    tem_geom <- st_geometry(bgc_sf)
    
    # Fix geometries that aren't polygon/multipolygon
    for(i in 1:length(tem_geom)) {
      if(!st_geometry_type(tem_geom[[i]]) %in% c("POLYGON", "MULTIPOLYGON")) 
        tem_geom[[i]] <- st_collection_extract(tem_geom[[i]], "POLYGON") %>% 
          st_multipolygon()
    }
    st_geometry(bgc_sf) <- st_cast(tem_geom, "MULTIPOLYGON")
    
    level_table <- unique.data.frame(data.frame(
      label = as.character(bgc_sf$MAP_LABEL), 
      value = as.numeric(bgc_sf$MAP_LABEL)))
    
    bgc_rast <- as.factor(terra::rasterize(vect(bgc_sf), rast(terrain_covariates[1]), field = "bgc"))
    levels(bgc_rast) <- level_table
    
    # File needs to be written to disk and then reloaded back into R for proper 
    # feature assignments
    bgc_rast <- writeRaster(bgc_rast, file.path(dsmart_site_ser_dir, "inputs", "bgc.tif"), 
                            overwrite = TRUE, wopt = list(datatype = "INT2S"))
    sources(bgc_rast)[, "source"]
  } else file.path(dsmart_site_ser_dir, "inputs", "bgc.tif")
} else NULL

# There are 2 sources of site series calls: simple and DSMART. under both of those,
# there are class outputs and probabilities. We need to create model permutations
# for each scenario
if(!all(is.na(ss))) {
  site_series <- c(
    rast(file.path(dsmart_site_ser_dir, "simple", "site_series_simple.tif")) %>% 
      magrittr::set_names("simple_site_ser_class"),
    rast(grep(
      "site_series_simple.tif", 
      list.files(file.path(dsmart_site_ser_dir, "simple"), full.names = TRUE),
      invert = TRUE, value = TRUE)) %>% 
      magrittr::set_names(paste0("simple_site_ser_", names(.))),
    rast(file.path(dsmart_site_ser_dir, "site_ser_dsmart.tif")) %>% 
      magrittr::set_names("dsmart_site_ser_class"),
    rast(list.files(
      file.path(dsmart_site_ser_dir, "output", "probabilities"), full.names = TRUE)) %>% 
      magrittr::set_names(paste0("dsmart_site_ser_probability_", names(.))))
  
  # Create filter based on ss and bgc variables
  ss_pattern <- grep(ifelse(length(probabilities) > 1, "probability|class", 
                            ifelse(probabilities, "probability", "class")),
                     grep(paste0(ss, collapse = "|"), names(site_series), value = TRUE), 
                     value = TRUE)
  site_series <- subset(site_series, ss_pattern)
} else site_series <- NULL

# We also need to include the structural stage rasters in this analysis as well
# I debated on whether I should include the associated probabilities. In the end,
# I decided no.
struct_stg <- rast(file.path(dsmart_struct_stg_dir, "struct_stg_dsmart.tif")) %>% 
    magrittr::set_names("dsmart_struct_stg_class")

# Combine all covariates
covariates <- c(rast(c(terrain_covariates, sentinel_covariates, 
                       climate_covariates, bgc_layer)),
                site_series, struct_stg)

# Find the best masking layer to use in the predict_landscape algorithm
mask_layers <- sapply(names(covariates), function(x) {
  cat(paste0("\rCounting NA values in ", x, " [", 
             which(x == names(covariates)), 
             " of ", nlyr(covariates), "]\n"))
    unname(freq(subset(covariates, x) * 0)[, "count"])}) %>% 
    data.frame(data_cells = .) %>% 
    rownames_to_column("layer")

# Attributed point data - limit to only CORNCAN and RUBUSPE
pres_abs_points <- sapply(c("CORNCAN", "RUBUSPE"), function(x) {
  st_read(file.path(input_dir, "pres_abs.gpkg"), layer = x, quiet = TRUE) %>% 
    {if(!is.null(site_series)) {
      cbind(., terra::extract(c(site_series, struct_stg), st_coordinates(.)))
    } else .} %>% 
    {if(!is.null(bgc_layer)) {
      cbind(., terra::extract(rast(bgc_layer), st_coordinates(.)))
    } else .} %>% 
    mutate(across(c("Pres", "simple_site_ser_class", "dsmart_site_ser_class", 
                    "dsmart_struct_stg_class", "bgc"), as.factor)) %>%
    dplyr::select(Pres, any_of(names(covariates)))
  }, simplify = FALSE, USE.NAMES = TRUE)

cover_points <- sapply(c("CORNCAN", "RUBUSPE"), function(x) {
  st_read(file.path(input_dir, "cover.gpkg"), layer = x, quiet = TRUE) %>% 
    {if(!is.null(site_series)) {
      cbind(., terra::extract(site_series, st_coordinates(.)))
    } else .} %>% 
    {if(!is.null(bgc_layer)) {
      cbind(., terra::extract(rast(bgc_layer), st_coordinates(.)))
    } else .} %>% 
    mutate(across(c("simple_site_ser_class", 
                    "dsmart_site_ser_class", "bgc"), as.factor)) %>%
    dplyr::select(Cover, any_of(names(covariates)))
  }, simplify = FALSE, USE.NAMES = TRUE)

# Create list of variable names to be selected 
var_list <- lapply(1:nrow(perms), function(i) {
  
  # Generate output ID
  id <- paste0(
    
    if(perms[i, "bgc"]) {
      "_bec_"
    } else "_",
    
    if(perms[i, "temporal"] == "annual, seasonal") {
      "ann_seas_"
    } else if(perms[i, "temporal"] == "annual") {
      "annual_"
    } else if(perms[i, "temporal"] == "seasonal") "seasonal_",
    
    if(perms[i, "climate_vars"]) {
      "climate_"
    } else NULL,
    
    if(!is.na(perms[i, "ss"])) {
      paste0(perms[i, "ss"], "_", ifelse(
        length(unlist(strsplit(perms[i, "probabilities"], split = "[, ]"))) > 1,
        "probs_class", ifelse(perms[i, "probabilities"], "probs", "class")))
    } else "no_ss")
  
  # Use character matching to select layers used in each model permutation
  filtered_data <- lapply(pres_abs_points[1], function(x) {
    variables <- c(
      basename(tools::file_path_sans_ext(terrain_covariates)), 
      names(struct_stg),
      if("annual" %in% unlist(strsplit(perms[i, "temporal"], split = "[, ]"))) {
        c(grep("sentinel2.*.2019", names(x), value = TRUE),
          grep("normal(?!.*(_at|_sm|_sp|_wt))", names(x), value = TRUE, perl = TRUE))
      },
      if("seasonal" %in% unlist(strsplit(perms[i, "temporal"], split = "[, ]"))) {
        c(grep("sentinel2.*.(fall|spring|summer|winter)", names(x), value = TRUE), 
          grep("normal.*.(_at|_sm|_sp|_wt)", names(x), value = TRUE, perl = TRUE))
      },
      
      if(!is.na(perms[i, "ss"])) {
        grep(ifelse(
          length(unlist(strsplit(perms[i, "probabilities"], split = "[, ]"))) > 1, 
          "probability|class", ifelse(perms[i, "probabilities"], "probability", "class")), 
          grep(perms[i, "ss"], names(x), value = TRUE), 
          value = TRUE)
      },
      
      if(perms[i, "bgc"]) "bgc"
    )
    if(!perms[i, "climate_vars"]) {
      variables <- variables[!startsWith(variables, "normal")]
    }
    return(variables)
  }) %>% magrittr::set_names(id)
}) %>% do.call(c, .)

```

The following chunk builds full datasets to be used in modelling later on. These full datasets include all of the covariate layers created up to this point, including all terrain layers, all climate variables (annual + seasonal), all sentinel variables (annual + seasonal), and a selection of either simple site series calls or DSMART site series calls (or neither). The usefulness of either BEC vs climate variables can be assessed here as well.

```{r mlr3 Presence probability modelling}

# Create the list of input datasets and write them to a single geopackage as
# separate layers
dir_out <- file.path(output_dir, "pres_abs_predictions")
dir.create(dir_out, showWarnings = FALSE, recursive = TRUE)

pres_abs <- lapply(names(var_list), function(i) {
  sapply(names(pres_abs_points), function(x) {
    out <- pres_abs_points[[x]] %>% 
      dplyr::select(any_of(c("Pres", var_list[[i]]))) %>% 
      drop_na()
    st_write(out, file.path(dir_out, paste0("pres_abs_input_data.gpkg")),
             layer = paste0(x, i), delete_layer = TRUE, quiet = TRUE)
    return(out)
  }, simplify = FALSE, USE.NAMES = TRUE) %>% 
    magrittr::set_names(paste0(names(.), i))
}) %>% unlist(recursive = FALSE)

# To save a bit of memory and in order to see results as they are generated, 
# iterate through the list of input data to model and predict
for(i in 1:length(pres_abs)) {
  
  dir_out <- file.path(output_dir, "pres_abs_predictions", names(pres_abs[i]))
  dir.create(dir_out, showWarnings = FALSE)
  
  # Create models for each dataset. Each model is a 10-fold spatial cross validation
  # repeated 5 times.
  pres_models <- model_gen_mlr3(traindat = pres_abs[i], target = "Pres", 
                                     feature_selection = "filter", type = "prob")
  
  # Extract and write performance metrics
  pres_performance <- sapply(names(pres_models$learners), function(x) {
    
    # Some model diagnostics
    folds <- pres_models$learners[[x]]$instance_args$resampling$param_set$values$folds
    reps <- pres_models$learners[[x]]$instance_args$resampling$param_set$values$repeats
    n_iters <- pres_models$learners[[x]]$archive$benchmark_result$n_resample_results
    
    # Classif.ce of each model iteration
    tune_results <- data.frame(
      filter_fraction = pres_models$learners[[x]]$archive$data()$importance.filter.frac, 
      classif.ce = pres_models$learners[[x]]$archive$data()$classif.ce
    ) %>% dplyr::arrange(classif.ce)
    
    # Confusion matrix of the best model
    confusion <- as.data.frame(
      pres_models$learners[[x]]$model$tuning_instance$archive$benchmark_result$
        resample_results$resample_result[[
          pres_models$learners[[x]]$model$tuning_instance$archive$best()$batch_nr]]$
        prediction()$confusion)
    
    # Testing and training datasets for each model run
    model_data <- lapply(1:n_iters, function(j) {
      test_data <- pres_models$learners[[x]]$archive$benchmark_result$resample_result(j)$
        resampling$instance %>% 
        cbind(batch_nr = j)
      train_data <- lapply(1:reps, function(k) {
        lapply(1:folds, function(l) {
          test_rows <- dplyr::filter(test_data, rep == k, fold == l)
          missing <- as.numeric(rownames(pres_abs[[i]])[
            !as.numeric(rownames(pres_abs[[i]])) %in% test_rows$row_id])
          data.frame(row_id = missing, rep = k, fold = l, batch_nr = j)
        }) %>% do.call(rbind, .)
      }) %>% do.call(rbind, .)
      return(list(test_data = test_data, train_data = train_data))
    }) %>% magrittr::set_names(paste0(
      "filter_frac = ", 
      pres_models$learners[[x]]$model$tuning_instance$archive$data()$
        importance.filter.frac))
    
    train_data <- do.call(rbind, lapply(model_data, "[[", "train_data")) %>% 
      magrittr::set_rownames(NULL)
    test_data <- do.call(rbind, lapply(model_data, "[[", "test_data")) %>% 
      magrittr::set_rownames(NULL)
    
    imp <- data.frame(importance = pres_models$learners[[x]]$learner$model$
                        importance$scores) %>% 
      rownames_to_column("layer") %>% 
      dplyr::filter(layer %in% pres_models$learners[[x]]$learner$model$
                      importance$features) %>% 
      arrange(desc(importance))
    
    out <- utils::capture.output(pres_models$learners[[x]]$learner$model$classif.ranger$model)
    best_model <- pres_models$learners[[x]]$learner$model$classif.ranger$model
    
    write.csv(tune_results, file = file.path(dir_out, paste0(x, "_pres_abs_tune_results.csv")),
              row.names = FALSE)
    cat(out, file = file.path(dir_out, paste0(x, "_pres_abs_model.txt")), sep = "\n")
    save(best_model, file = file.path(dir_out, paste0(x, "_pres_abs_model.RData")))
    write.csv(test_data, file = file.path(
      dir_out, paste0(x, "_pres_abs_model_resampling_test_data.csv")), 
      row.names = FALSE)
    write.csv(train_data, file = file.path(
      dir_out, paste0(x, "_pres_abs_model_resampling_train_data.csv")), 
      row.names = FALSE)
    write.csv(confusion, file = file.path(dir_out, paste0(x, "_pres_abs_confusion.csv")), 
              row.names = FALSE)
    write.csv(imp, file.path(dir_out, paste0(x, "_pres_abs_var_importance.csv")), 
              row.names = FALSE)
    write.csv(pres_abs[i][[x]], file.path(dir_out, paste0(x, "_pres_abs_input_data.csv")), 
              row.names = FALSE)
    
    return(list(
      tune_results = tune_results,
      best_model = best_model, 
      resampling = list(test_data = test_data, train_data = train_data),
      confusion = confusion,
      importance = imp
    ))
    
  }, simplify = FALSE, USE.NAMES = TRUE)
  
  # Run the map predictions
  pres_abs_predictions <- sapply(names(pres_models$learners), function(x) {
    
    # Subset raster data used in the model run
    cov_sub <- subset(covariates, pres_models$tasks[[x]]$col_roles$feature)
    
    # Find the best mask layer to use from the covariates left
    pred_mask <- mask_layers %>% 
      dplyr::filter(layer %in% names(cov_sub)) %>% 
      dplyr::arrange(layer) %>% 
      dplyr::slice_max(data_cells, n = 1, with_ties = FALSE) %>% 
      dplyr::pull(layer)
    
    out <- predict_landscape_mlr3(
      learner = pres_models$learners[[x]],
      task = pres_models$tasks[[x]], 
      type = "prob",
      covariates = cov_sub,
      tilesize = 500, 
      outDir = tile_dir, 
      mask_layer = pred_mask
    )
    
    # Write the raster mosaics
    out <- writeRaster(out, file.path(
      dir_out, paste0(x, "_", gsub("_.*", "", names(out)), ".tif")),
      overwrite = TRUE)
    
    return(out)
  }, simplify = FALSE, USE.NAMES = TRUE)
}

# Using the available cover model results, find the best models for each species
# First, collect all of that data together and then use an upper percentile or top
# number of models from the entire collection for a single species
best_model_sets <- lapply(c("CORNCAN", "RUBUSPE"), function(x) {
  
  sp_dirs <- dir(file.path(output_dir, "pres_abs_predictions"), pattern = x, 
                 full.names = TRUE)
  
  result_list <- lapply(sp_dirs, function(i) {
    
    read.csv(file.path(i, list.files(i, pattern = "_tune_results.csv$"))) %>% 
      dplyr::slice(which.min(classif.ce)) %>% 
      mutate(model = basename(i))
  }) %>% do.call(rbind, .) %>% arrange(classif.ce)
}) %>% do.call(rbind, .)

write.csv(best_model_sets, file.path(output_dir, "pres_abs_predictions", "model_performance_index.csv"), 
          row.names = FALSE)

```



```{r mlr3 cover modelling}

# cover <- foreach(i = names(var_list)) %do% {
#   lapply(cover_points, function(x) {
#     x %>% dplyr::select(any_of(c("Cover", var_list[[i]], "X", "Y"))) %>% 
#       drop_na() %>% 
#       dplyr::rename_with(toupper, c("X", "x", "Y", "y")
#                          [c("X", "x", "Y", "y") %in% names(.)])
#   }) %>% magrittr::set_names(paste0(names(.), i))
# } %>% unlist(recursive = FALSE)
# 
# # Write input data
# lapply(names(cover), function(x) {
#   dir_out <- file.path(output_dir, "cover_predictions")
#   dir.create(dir_out, showWarnings = FALSE)
#   y <- st_as_sf(cover[[x]], coords = c("X", "Y"), crs = crs(covariates))
#   st_write(y, file.path(dir_out, paste0("cover_input_data.gpkg")), 
#            layer = x, delete_layer = TRUE)
# })[0]
# 
# # To save a bit of memory and in order to see some results as they are 
# # generated, limit the number of models ran at a time by subsetting into groups of 10
# for(i in 1:length(cover)) {
#   
#   dir_out <- file.path(output_dir, "cover_predictions", names(cover[i]))
#   dir.create(dir_out, showWarnings = FALSE)
#   
#   # Run the models. Each is a 10-fold spatial cross validation repeated 5 times
#   cover_models <- model_gen_mlr3(traindat = cover[i], target = "Cover", 
#                                  feature_selection = "filter", type = "response")
#   
#   # Extract performance metrics for each model
#   cover_performance <- sapply(names(cover_models$learners), function(x) {
#     
#     folds <- cover_models$learners[[x]]$instance_args$resampling$param_set$values$folds
#     reps <- cover_models$learners[[x]]$instance_args$resampling$param_set$values$repeats
#     n_iters <- cover_models$learners[[x]]$archive$benchmark_result$n_resample_results
#     
#     tune_results <- data.frame(
#       filter_fraction = cover_models$learners[[x]]$archive$data()$importance.filter.frac, 
#       regr.mse = cover_models$learners[[x]]$archive$data()$regr.mse
#     ) %>% arrange(regr.mse)
#     
#     model_data <- foreach(j = 1:n_iters) %do% {
#       test_data <- cover_models$learners[[x]]$archive$benchmark_result$resample_result(j)$
#         resampling$instance %>% 
#         cbind(batch_nr = j, 
#               filter_frac = cover_models$learners[[x]]$model$tuning_instance$
#                 archive$data()$importance.filter.frac[j])
#       train_data <- foreach(k = 1:reps, .combine = rbind) %do% { 
#         foreach(l = 1:folds, .combine = rbind) %do% {
#           test_rows <- dplyr::filter(test_data, rep == k, fold == l)
#           missing <- as.numeric(rownames(pres_abs[[i]])[
#             !as.numeric(rownames(pres_abs[[i]])) %in% test_rows$row_id])
#           data.frame(row_id = missing, rep = k, fold = l, batch_nr = j)
#         }
#       } %>% 
#         cbind(filter_frac = cover_models$learners[[x]]$model$tuning_instance$
#                 archive$data()$importance.filter.frac[j])
#       return(list(test_data = test_data, train_data = train_data))
#     } %>% magrittr::set_names(paste0(
#       "filter_frac = ", 
#       cover_models$learners[[x]]$model$tuning_instance$archive$data()$
#         importance.filter.frac))
#     
#     train_data <- do.call(rbind, lapply(model_data, "[[", "train_data")) %>% 
#       magrittr::set_rownames(NULL)
#     test_data <- do.call(rbind, lapply(model_data, "[[", "test_data")) %>% 
#       magrittr::set_rownames(NULL)
#      
#     imp <- data.frame(importance = cover_models$learners[[x]]$learner$model$
#                         importance$scores) %>% 
#       rownames_to_column("layer") %>% 
#       dplyr::filter(layer %in% cover_models$learners[[x]]$learner$model$
#                       importance$features) %>% 
#       arrange(desc(importance))
#     
#     out <- utils::capture.output(cover_models$learners[[x]]$learner$model$regr.ranger$model)
#     best_model <- cover_models$learners[[x]]$learner$model$regr.ranger$model
#     
#     write.csv(tune_results, file = file.path(dir_out, paste0(x, "_cover_tune_results.csv")),
#               row.names = FALSE)
#     cat(out, file = file.path(dir_out, paste0(x, "_cover_model.txt")), sep = "\n")
#     save(best_model, file = file.path(dir_out, paste0(x, "_cover_model.RData")))
#     write.csv(test_data, file = file.path(
#       dir_out, paste0(x, "_cover_model_resampling_test_data.csv")), 
#       row.names = FALSE)
#     write.csv(train_data, file = file.path(
#       dir_out, paste0(x, "_cover_model_resampling_train_data.csv")), 
#       row.names = FALSE)
#     write.csv(imp, file.path(dir_out, paste0(x, "_cover_var_importance.csv")), 
#               row.names = FALSE)
#     write.csv(cover[i][[x]], file.path(dir_out, paste0(x, "_cover_input_data.csv")), 
#               row.names = FALSE)
#     
#     return(list(
#       tune_results = tune_results,
#       best_model = best_model, 
#       resampling = list(test_data = test_data, train_data = train_data),
#       importance = imp
#     ))
#     
#   }, simplify = FALSE, USE.NAMES = TRUE)
#   
#   # Run the map predictions
#   cover_predictions <- sapply(names(cover_models$learners), function(x) {
#     
#     cov_sub <- subset(covariates, cover_models$tasks[[x]]$col_roles$feature)
#     
#     for(j in 1:nrow(mask_layers)) {
#       if(mask_layers[j, "layer"] %in% names(cov_sub)) {
#         pred_mask <- mask_layers[j, "layer"]
#         break
#       }
#     }
#     
#     out <- predict_landscape_mlr3(
#       learner = cover_models$learners[[x]],
#       task = cover_models$tasks[[x]], 
#       type = "response",
#       covariates = cov_sub,
#       tilesize = 500, 
#       outDir = tile_dir, 
#       mask_layer = pred_mask
#     )
#     
#     # with predict_landscape_mlr3 type set to "response", the output is a single raster image
#     writeRaster(
#       out, 
#       file.path(dir_out, paste0(x, "_cover.tif")), 
#       overwrite = TRUE)
#     
#     return(out)
#     
#   }, simplify = FALSE, USE.NAMES = TRUE)
# }
# 
# # Using the available cover model results, find the best models for each species
# # First, collect all of that data together and then use an upper percentile or top
# # number of models from the entire collection for a single species
# best_model_sets <- foreach(x = c("CORNCAN", "RUBUSPE"), .combine = rbind) %do% {
#   
#   sp_dirs <- dir(file.path(output_dir, "cover_predictions"), pattern = x, 
#                  full.names = TRUE)
#   
#   result_list <- foreach(i = sp_dirs, .combine = rbind) %do% {
#     
#     read.csv(file.path(i, list.files(i, pattern = "_tune_results.csv$"))) %>% 
#       dplyr::slice(which.min(regr.mse)) %>% 
#       mutate(model = basename(i))
#   } %>% arrange(regr.mse)
# }
# 
# write.csv(best_model_sets, file.path(output_dir, "cover_predictions", "model_performance_index.csv"), 
#           row.names = FALSE)

```

One of the things that would be interesting to look at is the average probability of presence of a given plant within each site series. This is accomplished by zonal statistics, where the site series layer is the "zone". Mean, min, max, and standard deviation of the probability of presence is carried out here. This information could be directed in a way that we might be able to tell where a given plant has a higher chance of being found based on site series. For cover, this would indicate where the most plant cover would be found based on a given site series.

```{r Zonal statistics}

# Using the dsmart TEM as the zonal layer, run zonal statistics for each map model
site_series_z <- rast(file.path(dsmart_site_ser_dir, "site_ser_dsmart.tif")) %>% 
      magrittr::set_names("dsmart_ss_class")

# Get the numeric codes representing the site series classes
dsmart_lookup <- read.csv(file.path(dsmart_site_ser_dir, "output", "lookup.txt"))

# Collect all model output folder names. Should be the same for both pres/abs
# and cover
model_outs <- list.dirs(
  file.path(output_dir, "pres_abs_predictions"), 
  full.names = FALSE)[-1]

# Load the predicted output rasters from each folder and rename them
pres_abs_outs <- rast(file.path(output_dir, "pres_abs_predictions", model_outs, 
                    paste0(model_outs, "_TRUE.tif"))) %>% 
  magrittr::set_names(model_outs)

# Perform zonal statistics for each of the predicted outputs
pres_stats <- sapply(c("mean", "min", "max", "sd"), function(x) 
  zonal(pres_abs_outs, site_series_z, fun = x, na.rm = TRUE), 
  simplify = FALSE, USE.NAMES = TRUE)

# Reorder the dataframes and merge with the lookup table to define actual
# site series instead of just numeric codes. Save the outputs to the defined
# folders.
pres_reorder <- sapply(model_outs, function(x) {
  z <- lapply(pres_stats, function(y) y[, c("zone", x)]) %>% Reduce(
    function(x, y, ...) merge(x, y, all = TRUE, by = "zone", ...), .) %>% 
    magrittr::set_names(c("zone", "mean", "min", "max", "sd")) %>% 
    merge(dsmart_lookup, ., by.x = "code", by.y = "zone")
  write.csv(z, file.path(output_dir, "pres_abs_predictions", x, 
                         paste0(x, "_pres_abs_zonal_stats.csv")), 
            row.names = FALSE)
  return(z)
}, simplify = FALSE, USE.NAMES = TRUE)

# Below is the same as pres/abs modelling, but save cover outputs
# cover_outs <- rast(file.path(output_dir, "cover_predictions", model_outs, 
#                     paste0(model_outs, "_cover.tif"))) %>% 
#   magrittr::set_names(model_outs)
# 
# cover_stats <- sapply(c("mean", "min", "max", "sd"), function(x) 
#   zonal(cover_outs, site_series_z, fun = x, na.rm = TRUE), 
#   simplify = FALSE, USE.NAMES = TRUE)
# 
# cover_reorder <- sapply(model_outs, function(x) {
#   z <- lapply(cover_stats, function(y) y[, c("zone", x)]) %>% Reduce(
#     function(x, y, ...) merge(x, y, all = TRUE, by = "zone", ...), .) %>% 
#     magrittr::set_names(c("zone", "mean", "min", "max", "sd")) %>% 
#     merge(dsmart_lookup, ., by.x = "code", by.y = "zone")
#   write.csv(z, file.path(output_dir, "cover_predictions", x, 
#                          paste0(x, "_cover_zonal_stats.csv")), 
#             row.names = FALSE)
#   return(z)
# }, simplify = FALSE, USE.NAMES = TRUE)

```
