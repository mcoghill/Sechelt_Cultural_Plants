---
title: "02c_Models"
author: "Matthew Coghill"
date: "2/19/2020"
output: html_document
---

This document is meant to generate models of either binary classification of presence / absence, or regression models of plant cover. This script should be the last script ran in the whole process as it depends on:

1. Covariate layers generated in 01 scripts;
2. Proper datasets generated in the 02a script; and
3. The site series raster generated in the 02b script.

This script uses the mlr package (machine learning R) in order to make models. It's very powerful and can evaluate a lot of different models at the same time. I'll attempt to explain things each step of the way. First, we will load the packages used in this script.

```{r Load Packages}

ls <- c("tidyverse", "tools", "terra", "sf", "ranger", "mlr3verse", "parallel", 
        "parallelMap", "randomForest", "ModelMap", "foreach")
new.packages <- ls[!(ls %in% installed.packages()[, "Package"])]
if(length(new.packages))
  install.packages(new.packages)
lapply(ls, library, character.only = TRUE)[0]
rm(ls, new.packages)

source("./_functions/model_gen_mlr3.R")
source("./_functions/predict_landscape.R")

```

Next, directories and certain files are defined.

```{r}

AOI <- "Sechelt"
AOI_dir <- file.path(".", paste0(AOI, "_AOI"))
map_res <- 4

input_dir <- file.path("./Sechelt_AOI/1_map_inputs/field_data/processed")
covariate_dir <- file.path(AOI_dir, "1_map_inputs", "covariates")
ss_dir <- file.path(AOI_dir, "1_map_inputs", "dsmart", "output")
output_dir <- file.path(AOI_dir, "2_map_predictions")
tile_dir <- file.path(output_dir, "tiles")
dir.create(tile_dir, showWarnings = FALSE)
dir.create(file.path(output_dir, "pres_abs_maps"), showWarnings = FALSE)
dir.create(file.path(output_dir, "cover_maps"), showWarnings = FALSE)

ss <- "dsmart" # The two options are currently simple or dsmart

```

It is highly anticipated that a site series layer will have a high degree of importance in predicting the locations of these culturally sensitive plants. We will let the models decide whether that is true or not, but it will always be kept in the model regardless.

The only step in this script that requires you changing anything is the first line, model_run. Change this parameter to be whatever model you want to generate results for. Cover models will be a regression and presence / absence models will be a classification.

The mlr package creates models using building blocks called "tasks" and "learners". Tasks are essentially the data you are creating your models from, and learners are more specific to building models. For example, the first learner that's created here says "create a ranger regression model, and when creating the model rank importance variables by their impurity values. Also, this is a response model". The next learner is a feature selection filter which wraps around the first learner. It pretty much says that it will filter the variables using the ranger_impurity algorithm, but always will keep the site_series layer. The last learner is a parameter tuning algorithm wrapped around the feature selection filter, and gives further details on the model. It will be a spatial 10-fold cross validation repeated 5 times. Also, it will create models using different amounts of covariate layers and numbers of trees so that it selects the best model from that bunch.

After mlr fits the best model, that model is extracted and the predictions for presence / absence or cover are created. This can take some time depending on the amount of predictor variables used in the final model. Finally, some zonal statistics are calculated per site series.

```{r Load data}

res_folder <- paste0(map_res, "m")

# Raster covariates
terrain_covariates <- grep(pattern = "normal|sentinel2", list.files(
  file.path(covariate_dir, res_folder), full.names = TRUE, pattern = ".tif$"), invert = TRUE, 
  value = TRUE)
sentinel_covariates <- list.files(file.path(covariate_dir, res_folder), 
                                  pattern = "^sentinel2.*.summer.tif$", full.names = TRUE)
climate_covariates <- list.files(file.path(covariate_dir, res_folder), 
                                 pattern = "^normal.*._sm.tif$", full.names = TRUE)
site_series <- list.files(ss_dir, pattern = "site_series_.*..tif", full.names = TRUE)
covariates <- rast(c(terrain_covariates, sentinel_covariates, climate_covariates, site_series))

# Attributed point data
pres_abs_points <- sapply(st_layers(file.path(input_dir, "pres_abs.gpkg"))$name, function(x) {
  st_read(file.path(input_dir, "pres_abs.gpkg"), layer = x, quiet = TRUE) %>% 
    cbind(st_coordinates(.), terra::extract(rast(site_series), st_coordinates(.))) %>% 
    st_drop_geometry() %>% 
    mutate(Pres = as.factor(Pres)) %>% 
    dplyr::select(Pres, names(covariates), X, Y)
  }, simplify = FALSE, USE.NAMES = TRUE)
cover_points <- sapply(st_layers(file.path(input_dir, "cover.gpkg"))$name, function(x) {
  st_read(file.path(input_dir, "cover.gpkg"), layer = x, quiet = TRUE) %>% 
    cbind(st_coordinates(.), terra::extract(rast(site_series), st_coordinates(.))) %>% 
    st_drop_geometry() %>% 
    dplyr::select(Cover, names(covariates), X, Y)
  }, simplify = FALSE, USE.NAMES = TRUE)

```




```{r mlr3 pres/abs probability modelling}

# Use mlr3 to create the best models
# The function does everything needed and outputs a list of the best models
# for each plant species. Function uses mlr3 to do 10 fold cross validaion
# repeated 5 times for each iteration of the model. There are 10 iterations which
# are different combinations of covariate layers (in attempts to find the best 
# feature set). This results in 500 models per species. The function is also aware
# of what the prediction types should be: for pres/abs, we want a probability 
# output from classification modelling, whereas for cover we want the raw 
# predictions created from regression modelling

# To assess how useful the TEM is, I've made different models using a simiple
# rasterized version, the DSMART version, and models that don't use it as well.
pres_abs <- c(
  sapply(paste0(names(pres_abs_points), "_no_tem"), function(x) {
    y <- gsub("_no_tem", "", x)
    dplyr::select(pres_abs_points[[y]], -c(site_series_dsmart, site_series_simple)) %>% 
      drop_na()
    }, simplify = FALSE, USE.NAMES = TRUE),
  sapply(paste0(names(pres_abs_points), "_simple_tem"), function(x) {
    y <- gsub("_simple_tem", "", x)
    dplyr::select(pres_abs_points[[y]], -site_series_dsmart) %>% 
      drop_na()
    }, simplify = FALSE, USE.NAMES = TRUE), 
  sapply(paste0(names(pres_abs_points), "_dsmart_tem"), function(x) {
    y <- gsub("_dsmart_tem", "", x)
    dplyr::select(pres_abs_points[[y]], -site_series_simple) %>% 
      drop_na()
    }, simplify = FALSE, USE.NAMES = TRUE))

pres_models <- model_gen_mlr3(pres_abs, "Pres")

# Save some relevant data from the models
pres_importance <- lapply(seq_along(pres_models), function(x) {
  write.csv(
    pres_models[[x]]$confusion.matrix, 
    file.path(output_dir, paste0(names(pres_models)[x], "_pres_abs_confusion.csv")))
  y <- data.frame(importance = ranger::importance(pres_models[[x]])) %>% 
    rownames_to_column() %>% 
    rename(covariates = rowname) %>% 
    arrange(desc(importance))
  write.csv(
    y, 
    file.path(output_dir, paste0(names(pres_models)[x], "_pres_abs_var_importance.csv")), 
    row.names = FALSE)
  return(y)
  }) %>% magrittr::set_names(names(pres_models))

# Map the predictions using the predict_landscape function
for(i in 1:length(pres_models)) {
  cov_important <- raster::stack(subset(covariates, pres_importance[[i]]$covariates))
  out <- predict_landscape(pres_models[[i]], cov_important, tilesize = 500,
                           outDir = tile_dir, type = "prob")
  
  # with predict_landscape type set to "prob", the output is a list of files
  for(j in out) {
    file.copy(
      from = j, 
      to = file.path(output_dir, "pres_abs_maps", paste0(names(pres_models)[i], "_", basename(j))))
  }
  
}

# After the raster is made, perform some zonal statistics by site series
zonal_stats <- cbind(zonal(pred_raster, site_series, fun = "mean"), 
                     min = zonal(pred_raster, site_series, fun = "min")[, "min"], 
                     max = zonal(pred_raster, site_series, fun = "max")[, "max"], 
                     sd = zonal(pred_raster, site_series, fun = "sd")[, "sd"], 
                     n_cells = zonal(pred_raster, site_series, fun = "count")[, "count"]) %>% 
  merge(dsmart_lookup, by.x = "zone", by.y = "code") %>% 
  dplyr::select(name, mean, min, max, sd, n_cells) %>% 
  dplyr::rename(site_series = name)

# Write final outputs
writeRaster(pred_raster, file.path(output_dir, paste0(model_run, ".tif")), overwrite = TRUE)
write.csv(mod_perform, file.path(output_dir, paste0(model_run, "_performance.csv")), row.names = FALSE)
write.csv(model_importance, file.path(output_dir, paste0(model_run, "_importance.csv")), row.names = FALSE)
write.csv(zonal_stats, file.path(output_dir, paste0(model_run, "_zonal_stats.csv")), row.names = FALSE)

```

```{r mlr3 cover modelling}

# Use mlr3 to create the best models
cover <- c(
  sapply(paste0(names(cover_points), "_no_tem"), function(x) {
    y <- gsub("_no_tem", "", x)
    dplyr::select(cover_points[[y]], -c(site_series_dsmart, site_series_simple)) %>% 
      drop_na()
    }, simplify = FALSE, USE.NAMES = TRUE),
  sapply(paste0(names(cover_points), "_simple_tem"), function(x) {
    y <- gsub("_simple_tem", "", x)
    dplyr::select(cover_points[[y]], -site_series_dsmart) %>% 
      drop_na()
    }, simplify = FALSE, USE.NAMES = TRUE), 
  sapply(paste0(names(cover_points), "_dsmart_tem"), function(x) {
    y <- gsub("_dsmart_tem", "", x)
    dplyr::select(cover_points[[y]], -site_series_simple) %>% 
      drop_na()
    }, simplify = FALSE, USE.NAMES = TRUE))

cover_models <- model_gen_mlr3(cover, "Cover")

# Save some relevant data from the models
cover_importance <- lapply(seq_along(cover_models), function(x) {
  write.csv(
    cover_models[[x]]$confusion.matrix, 
    file.path(output_dir, paste0(names(cover_models)[x], "_cover_confusion.csv")))
  y <- data.frame(importance = ranger::importance(cover_models[[x]])) %>% 
    rownames_to_column() %>% 
    rename(covariates = rowname) %>% 
    arrange(desc(importance))
  write.csv(
    y, 
    file.path(output_dir, paste0(names(cover_models)[x], "_cover_var_importance.csv")), 
    row.names = FALSE)
  return(y)
  }) %>% magrittr::set_names(names(cover_models))

# Map the predictions using the predict_landscape function
for(i in 1:length(cover_models)) {
  cov_important <- raster::stack(subset(covariates, cover_importance[[i]]$covariates))
  out <- predict_landscape(cover_models[[i]], cov_important, tilesize = 500,
                           outDir = tile_dir)
  writeRaster(out, file.path(output_dir, "cover_maps", paste0(names(cover_models)[i], "_cover.tif")))
}

```



```{r ModelMap testing}

important_vars <- names(model_ranger$variable.importance)
mmap_test <- model.build(
  model.type = "RF",
  qdata.trainfn = raw_data[, c("response", important_vars)], 
  folder = tempdir(),
  MODELfn = NULL,
  predList = names(raw_data[, c("response", important_vars)])[-1],
  predFactor = c("Structural.Stage", "Site.Disturbance"),
  response.name = names(raw_data)[1], 
  response.type = "categorical", 
  unique.rowname = FALSE,
  ntree = model_ranger$num.trees, 
  mtry = model_ranger$mtry, 
  proximity = FALSE
)

write.csv(mmap_test$confusion, file.path(output_dir, paste0(model_run, "_MMap_confusion.csv")))

```

```{r ranger package testing}

important_vars <- names(model_ranger$variable.importance)
ranger_mod <- ranger(response ~., 
                     data = raw_data[, c("response", important_vars)], 
                     num.trees = model_ranger$num.trees, 
                     mtry = model_ranger$mtry, 
                     importance = "impurity", 
                     write.forest = TRUE, 
                     probability = FALSE, 
                     min.node.size = model_ranger$min.node.size, 
                     replace = TRUE, 
                     keep.inbag = TRUE, 
                     oob.error = TRUE, 
                     classification = TRUE)
write.csv(ranger_mod$confusion.matrix, file.path(output_dir, paste0(model_run, "_ranger_confusion.csv")))

```


```{r randomForest package testing}

important_vars <- names(model_ranger$variable.importance)
rf_mod <- randomForest(response ~.,
                       data = raw_data[, c("response", important_vars)],
                       ntree = model_ranger$num.trees, 
                       mtry = model_ranger$mtry, 
                       replace = TRUE, 
                       nodesize = model_ranger$min.node.size, 
                       importance = TRUE, 
                       keep.forest = TRUE, 
                       keep.inbag = TRUE)
write.csv(rf_mod$confusion, file.path(output_dir, paste0(model_run, "_randomForest_confusion.csv")))

```

